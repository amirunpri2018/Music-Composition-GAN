{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "        x_train = np.load(r'C:\\Users\\Vee\\Desktop\\python\\answers.npy',allow_pickle=True)\n",
    "        x_train = x_train.reshape(721,4,4,1)\n",
    "        return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 4\n",
    "        self.img_cols = 4\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 16\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        generator = self.generator\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(16 * 4 * 4, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((4, 4, 16)))\n",
    "        model.add(Conv2D(16, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(16, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(16, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(16, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(16, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(1, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 256\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                #self.save_imgs(epoch)\n",
    "                self.generator.save(\"generator.h5\")\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 3, 3\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        gen_imgs = np.array(gen_imgs) * 256\n",
    "        gen_imgs = gen_imgs.astype(int)\n",
    "\n",
    "        fig=plt.figure(figsize=(20, 20))\n",
    "        for i in range(1, c*r+1):\n",
    "            img = gen_imgs[i-1]\n",
    "            fig.add_subplot(r, c, i)\n",
    "            plt.imshow(img)\n",
    "        fig.savefig(r\"C:\\Users\\Vee\\Desktop\\python\\GAN\\epoch_%d.el\n",
    "Interrupt\n",
    "Restart\n",
    "Restart & Clear Output\n",
    "Restart & Run All\n",
    "Reconnect\n",
    "Shutdown\n",
    "Change kernel\n",
    "Widgets\n",
    "Help\n",
    "png\" % epoch)\n",
    "        plt.close()\n",
    "        self.generator.save(\"generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 32)          320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 28,481\n",
      "Trainable params: 28,289\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 1)           65        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 1)           0         \n",
      "=================================================================\n",
      "Total params: 9,937\n",
      "Trainable params: 9,777\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.799277, acc.: 46.00%] [G loss: 0.785889]\n",
      "1 [D loss: 0.773898, acc.: 51.00%] [G loss: 0.794451]\n",
      "2 [D loss: 0.782533, acc.: 51.00%] [G loss: 0.834400]\n",
      "3 [D loss: 0.793022, acc.: 53.00%] [G loss: 0.769421]\n",
      "4 [D loss: 0.812747, acc.: 47.50%] [G loss: 0.647463]\n",
      "5 [D loss: 0.778280, acc.: 48.00%] [G loss: 0.821103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [D loss: 0.773078, acc.: 50.50%] [G loss: 0.788113]\n",
      "7 [D loss: 0.780922, acc.: 49.00%] [G loss: 0.760756]\n",
      "8 [D loss: 0.718276, acc.: 58.00%] [G loss: 0.691405]\n",
      "9 [D loss: 0.771391, acc.: 53.00%] [G loss: 0.698243]\n",
      "10 [D loss: 0.769654, acc.: 50.00%] [G loss: 0.779906]\n",
      "11 [D loss: 0.746527, acc.: 59.50%] [G loss: 0.729495]\n",
      "12 [D loss: 0.788640, acc.: 51.00%] [G loss: 0.756235]\n",
      "13 [D loss: 0.764637, acc.: 54.50%] [G loss: 0.800550]\n",
      "14 [D loss: 0.754639, acc.: 52.00%] [G loss: 0.754415]\n",
      "15 [D loss: 0.795344, acc.: 46.00%] [G loss: 0.800321]\n",
      "16 [D loss: 0.772800, acc.: 52.00%] [G loss: 0.781049]\n",
      "17 [D loss: 0.746596, acc.: 55.00%] [G loss: 0.683899]\n",
      "18 [D loss: 0.708969, acc.: 53.50%] [G loss: 0.716281]\n",
      "19 [D loss: 0.763812, acc.: 52.50%] [G loss: 0.791426]\n",
      "20 [D loss: 0.746365, acc.: 52.50%] [G loss: 0.719689]\n",
      "21 [D loss: 0.791690, acc.: 48.50%] [G loss: 0.751164]\n",
      "22 [D loss: 0.728790, acc.: 56.50%] [G loss: 0.702564]\n",
      "23 [D loss: 0.735795, acc.: 53.00%] [G loss: 0.788743]\n",
      "24 [D loss: 0.724046, acc.: 55.50%] [G loss: 0.708793]\n",
      "25 [D loss: 0.718803, acc.: 50.00%] [G loss: 0.799614]\n",
      "26 [D loss: 0.740214, acc.: 55.50%] [G loss: 0.674312]\n",
      "27 [D loss: 0.734842, acc.: 49.00%] [G loss: 0.720183]\n",
      "28 [D loss: 0.723287, acc.: 57.00%] [G loss: 0.749544]\n",
      "29 [D loss: 0.738066, acc.: 52.00%] [G loss: 0.747908]\n",
      "30 [D loss: 0.708876, acc.: 57.00%] [G loss: 0.700073]\n",
      "31 [D loss: 0.716243, acc.: 55.50%] [G loss: 0.725254]\n",
      "32 [D loss: 0.710391, acc.: 57.00%] [G loss: 0.757426]\n",
      "33 [D loss: 0.706331, acc.: 60.00%] [G loss: 0.739845]\n",
      "34 [D loss: 0.714132, acc.: 59.50%] [G loss: 0.808338]\n",
      "35 [D loss: 0.712106, acc.: 58.00%] [G loss: 0.667099]\n",
      "36 [D loss: 0.695794, acc.: 55.00%] [G loss: 0.805495]\n",
      "37 [D loss: 0.681817, acc.: 65.50%] [G loss: 0.763505]\n",
      "38 [D loss: 0.708699, acc.: 60.00%] [G loss: 0.767590]\n",
      "39 [D loss: 0.693338, acc.: 63.00%] [G loss: 0.812398]\n",
      "40 [D loss: 0.704680, acc.: 61.00%] [G loss: 0.732396]\n",
      "41 [D loss: 0.695423, acc.: 62.50%] [G loss: 0.748017]\n",
      "42 [D loss: 0.662087, acc.: 62.00%] [G loss: 0.718623]\n",
      "43 [D loss: 0.706560, acc.: 61.50%] [G loss: 0.690027]\n",
      "44 [D loss: 0.678065, acc.: 63.00%] [G loss: 0.779891]\n",
      "45 [D loss: 0.654721, acc.: 62.00%] [G loss: 0.713211]\n",
      "46 [D loss: 0.668916, acc.: 64.00%] [G loss: 0.737931]\n",
      "47 [D loss: 0.670919, acc.: 62.00%] [G loss: 0.811238]\n",
      "48 [D loss: 0.700758, acc.: 63.00%] [G loss: 0.746257]\n",
      "49 [D loss: 0.614556, acc.: 69.00%] [G loss: 0.745325]\n",
      "50 [D loss: 0.702710, acc.: 62.00%] [G loss: 0.810841]\n",
      "51 [D loss: 0.671609, acc.: 65.00%] [G loss: 0.783831]\n",
      "52 [D loss: 0.627712, acc.: 68.50%] [G loss: 0.722623]\n",
      "53 [D loss: 0.657904, acc.: 66.00%] [G loss: 0.741533]\n",
      "54 [D loss: 0.684871, acc.: 60.50%] [G loss: 0.764808]\n",
      "55 [D loss: 0.666698, acc.: 61.50%] [G loss: 0.821119]\n",
      "56 [D loss: 0.664888, acc.: 66.50%] [G loss: 0.743132]\n",
      "57 [D loss: 0.600855, acc.: 70.50%] [G loss: 0.768385]\n",
      "58 [D loss: 0.655364, acc.: 61.50%] [G loss: 0.803053]\n",
      "59 [D loss: 0.663307, acc.: 69.00%] [G loss: 0.813825]\n",
      "60 [D loss: 0.650509, acc.: 64.00%] [G loss: 0.831900]\n",
      "61 [D loss: 0.644295, acc.: 67.50%] [G loss: 0.741435]\n",
      "62 [D loss: 0.671284, acc.: 65.00%] [G loss: 0.824257]\n",
      "63 [D loss: 0.632118, acc.: 69.50%] [G loss: 0.787359]\n",
      "64 [D loss: 0.666621, acc.: 67.00%] [G loss: 0.769963]\n",
      "65 [D loss: 0.658328, acc.: 66.00%] [G loss: 0.740344]\n",
      "66 [D loss: 0.644765, acc.: 76.00%] [G loss: 0.756769]\n",
      "67 [D loss: 0.677973, acc.: 64.00%] [G loss: 0.816902]\n",
      "68 [D loss: 0.628034, acc.: 69.00%] [G loss: 0.830594]\n",
      "69 [D loss: 0.664369, acc.: 65.50%] [G loss: 0.806305]\n",
      "70 [D loss: 0.654117, acc.: 68.50%] [G loss: 0.783636]\n",
      "71 [D loss: 0.634407, acc.: 67.50%] [G loss: 0.815608]\n",
      "72 [D loss: 0.661024, acc.: 65.00%] [G loss: 0.839737]\n",
      "73 [D loss: 0.650476, acc.: 62.50%] [G loss: 0.728190]\n",
      "74 [D loss: 0.663586, acc.: 68.00%] [G loss: 0.778429]\n",
      "75 [D loss: 0.653373, acc.: 68.00%] [G loss: 0.755231]\n",
      "76 [D loss: 0.654367, acc.: 67.50%] [G loss: 0.863715]\n",
      "77 [D loss: 0.628929, acc.: 70.00%] [G loss: 0.844956]\n",
      "78 [D loss: 0.622689, acc.: 66.00%] [G loss: 0.793748]\n",
      "79 [D loss: 0.655600, acc.: 67.00%] [G loss: 0.791785]\n",
      "80 [D loss: 0.589022, acc.: 75.00%] [G loss: 0.893772]\n",
      "81 [D loss: 0.644301, acc.: 67.50%] [G loss: 0.742031]\n",
      "82 [D loss: 0.641430, acc.: 68.00%] [G loss: 0.830100]\n",
      "83 [D loss: 0.606672, acc.: 73.00%] [G loss: 0.885416]\n",
      "84 [D loss: 0.611873, acc.: 70.50%] [G loss: 0.800004]\n",
      "85 [D loss: 0.583472, acc.: 77.00%] [G loss: 0.732286]\n",
      "86 [D loss: 0.651922, acc.: 66.00%] [G loss: 0.770868]\n",
      "87 [D loss: 0.627258, acc.: 68.50%] [G loss: 0.780582]\n",
      "88 [D loss: 0.633192, acc.: 70.00%] [G loss: 0.804133]\n",
      "89 [D loss: 0.611570, acc.: 68.00%] [G loss: 0.875922]\n",
      "90 [D loss: 0.619435, acc.: 72.00%] [G loss: 0.805350]\n",
      "91 [D loss: 0.641014, acc.: 70.50%] [G loss: 0.832267]\n",
      "92 [D loss: 0.562514, acc.: 78.00%] [G loss: 0.803705]\n",
      "93 [D loss: 0.628975, acc.: 73.00%] [G loss: 0.867524]\n",
      "94 [D loss: 0.612716, acc.: 72.50%] [G loss: 0.800023]\n",
      "95 [D loss: 0.601013, acc.: 74.00%] [G loss: 0.815652]\n",
      "96 [D loss: 0.635024, acc.: 66.00%] [G loss: 0.758018]\n",
      "97 [D loss: 0.585185, acc.: 77.00%] [G loss: 0.861285]\n",
      "98 [D loss: 0.623799, acc.: 68.00%] [G loss: 0.861368]\n",
      "99 [D loss: 0.612572, acc.: 70.50%] [G loss: 0.776391]\n",
      "100 [D loss: 0.598464, acc.: 73.00%] [G loss: 0.861429]\n",
      "101 [D loss: 0.594315, acc.: 74.00%] [G loss: 0.810663]\n",
      "102 [D loss: 0.601421, acc.: 70.50%] [G loss: 0.846394]\n",
      "103 [D loss: 0.633044, acc.: 74.00%] [G loss: 0.780309]\n",
      "104 [D loss: 0.588910, acc.: 75.00%] [G loss: 0.818278]\n",
      "105 [D loss: 0.598613, acc.: 74.50%] [G loss: 0.811840]\n",
      "106 [D loss: 0.605108, acc.: 72.50%] [G loss: 0.869673]\n",
      "107 [D loss: 0.625795, acc.: 70.00%] [G loss: 0.885192]\n",
      "108 [D loss: 0.599598, acc.: 75.00%] [G loss: 0.857432]\n",
      "109 [D loss: 0.594893, acc.: 73.00%] [G loss: 0.871662]\n",
      "110 [D loss: 0.594594, acc.: 72.00%] [G loss: 0.820438]\n",
      "111 [D loss: 0.584661, acc.: 77.00%] [G loss: 0.869850]\n",
      "112 [D loss: 0.586719, acc.: 73.00%] [G loss: 0.815007]\n",
      "113 [D loss: 0.604004, acc.: 74.00%] [G loss: 0.873128]\n",
      "114 [D loss: 0.567347, acc.: 73.00%] [G loss: 0.866089]\n",
      "115 [D loss: 0.582739, acc.: 70.50%] [G loss: 0.847787]\n",
      "116 [D loss: 0.593357, acc.: 72.00%] [G loss: 0.896600]\n",
      "117 [D loss: 0.580539, acc.: 74.00%] [G loss: 0.859445]\n",
      "118 [D loss: 0.606334, acc.: 74.00%] [G loss: 0.849597]\n",
      "119 [D loss: 0.593850, acc.: 75.50%] [G loss: 0.859982]\n",
      "120 [D loss: 0.586289, acc.: 72.50%] [G loss: 0.833804]\n",
      "121 [D loss: 0.611139, acc.: 77.00%] [G loss: 0.891967]\n",
      "122 [D loss: 0.599924, acc.: 74.00%] [G loss: 0.856439]\n",
      "123 [D loss: 0.572728, acc.: 75.50%] [G loss: 0.818364]\n",
      "124 [D loss: 0.614686, acc.: 74.00%] [G loss: 0.851689]\n",
      "125 [D loss: 0.598562, acc.: 72.00%] [G loss: 0.923831]\n",
      "126 [D loss: 0.585620, acc.: 76.00%] [G loss: 0.795923]\n",
      "127 [D loss: 0.576682, acc.: 76.50%] [G loss: 0.840499]\n",
      "128 [D loss: 0.578988, acc.: 77.50%] [G loss: 1.005363]\n",
      "129 [D loss: 0.574108, acc.: 75.00%] [G loss: 0.823348]\n",
      "130 [D loss: 0.594996, acc.: 70.00%] [G loss: 0.895606]\n",
      "131 [D loss: 0.558242, acc.: 81.00%] [G loss: 0.897924]\n",
      "132 [D loss: 0.584401, acc.: 71.50%] [G loss: 0.859603]\n",
      "133 [D loss: 0.596210, acc.: 72.00%] [G loss: 0.791511]\n",
      "134 [D loss: 0.562724, acc.: 79.50%] [G loss: 0.880281]\n",
      "135 [D loss: 0.590999, acc.: 73.00%] [G loss: 0.866506]\n",
      "136 [D loss: 0.583041, acc.: 73.00%] [G loss: 0.855349]\n",
      "137 [D loss: 0.574137, acc.: 75.50%] [G loss: 0.802799]\n",
      "138 [D loss: 0.594261, acc.: 74.00%] [G loss: 0.931575]\n",
      "139 [D loss: 0.602718, acc.: 73.50%] [G loss: 0.863247]\n",
      "140 [D loss: 0.594232, acc.: 75.50%] [G loss: 0.909500]\n",
      "141 [D loss: 0.573758, acc.: 78.50%] [G loss: 0.955544]\n",
      "142 [D loss: 0.582793, acc.: 74.50%] [G loss: 0.844738]\n",
      "143 [D loss: 0.609588, acc.: 73.00%] [G loss: 0.822859]\n",
      "144 [D loss: 0.576199, acc.: 74.00%] [G loss: 0.994283]\n",
      "145 [D loss: 0.541351, acc.: 78.50%] [G loss: 0.928862]\n",
      "146 [D loss: 0.561954, acc.: 78.00%] [G loss: 0.856059]\n",
      "147 [D loss: 0.569684, acc.: 77.50%] [G loss: 0.860895]\n",
      "148 [D loss: 0.540146, acc.: 79.00%] [G loss: 0.924132]\n",
      "149 [D loss: 0.553059, acc.: 78.00%] [G loss: 0.808719]\n",
      "150 [D loss: 0.606994, acc.: 73.00%] [G loss: 0.941854]\n",
      "151 [D loss: 0.575464, acc.: 77.50%] [G loss: 0.901208]\n",
      "152 [D loss: 0.561723, acc.: 76.00%] [G loss: 0.879967]\n",
      "153 [D loss: 0.550187, acc.: 79.50%] [G loss: 0.856497]\n",
      "154 [D loss: 0.562390, acc.: 82.50%] [G loss: 0.986042]\n",
      "155 [D loss: 0.574224, acc.: 73.50%] [G loss: 0.884882]\n",
      "156 [D loss: 0.561155, acc.: 78.00%] [G loss: 0.913550]\n",
      "157 [D loss: 0.553417, acc.: 77.50%] [G loss: 0.936404]\n",
      "158 [D loss: 0.516769, acc.: 83.00%] [G loss: 0.838083]\n",
      "159 [D loss: 0.552274, acc.: 84.50%] [G loss: 0.958838]\n",
      "160 [D loss: 0.572676, acc.: 78.50%] [G loss: 1.026729]\n",
      "161 [D loss: 0.546482, acc.: 81.50%] [G loss: 0.909342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 [D loss: 0.524338, acc.: 83.00%] [G loss: 0.893834]\n",
      "163 [D loss: 0.522006, acc.: 82.50%] [G loss: 0.869417]\n",
      "164 [D loss: 0.546054, acc.: 80.50%] [G loss: 0.914941]\n",
      "165 [D loss: 0.579609, acc.: 77.50%] [G loss: 0.996397]\n",
      "166 [D loss: 0.534436, acc.: 82.00%] [G loss: 0.958832]\n",
      "167 [D loss: 0.560442, acc.: 78.50%] [G loss: 0.914804]\n",
      "168 [D loss: 0.536077, acc.: 80.00%] [G loss: 0.933393]\n",
      "169 [D loss: 0.537832, acc.: 79.50%] [G loss: 0.982607]\n",
      "170 [D loss: 0.572744, acc.: 74.00%] [G loss: 0.895777]\n",
      "171 [D loss: 0.531732, acc.: 81.00%] [G loss: 0.924567]\n",
      "172 [D loss: 0.553757, acc.: 80.00%] [G loss: 0.990106]\n",
      "173 [D loss: 0.570521, acc.: 75.50%] [G loss: 0.938742]\n",
      "174 [D loss: 0.564937, acc.: 75.00%] [G loss: 0.926115]\n",
      "175 [D loss: 0.532743, acc.: 83.00%] [G loss: 0.920223]\n",
      "176 [D loss: 0.547368, acc.: 76.00%] [G loss: 0.996029]\n",
      "177 [D loss: 0.567295, acc.: 77.50%] [G loss: 0.970741]\n",
      "178 [D loss: 0.536776, acc.: 81.00%] [G loss: 1.015504]\n",
      "179 [D loss: 0.572453, acc.: 74.00%] [G loss: 0.922296]\n",
      "180 [D loss: 0.498479, acc.: 84.00%] [G loss: 0.953645]\n",
      "181 [D loss: 0.568445, acc.: 76.50%] [G loss: 0.921885]\n",
      "182 [D loss: 0.545910, acc.: 79.50%] [G loss: 0.998560]\n",
      "183 [D loss: 0.553633, acc.: 78.00%] [G loss: 0.963354]\n",
      "184 [D loss: 0.553071, acc.: 79.00%] [G loss: 0.973015]\n",
      "185 [D loss: 0.539472, acc.: 80.50%] [G loss: 0.952595]\n",
      "186 [D loss: 0.560653, acc.: 80.00%] [G loss: 0.990053]\n",
      "187 [D loss: 0.548542, acc.: 73.50%] [G loss: 0.911878]\n",
      "188 [D loss: 0.532114, acc.: 78.00%] [G loss: 0.826850]\n",
      "189 [D loss: 0.544634, acc.: 79.50%] [G loss: 0.946402]\n",
      "190 [D loss: 0.544713, acc.: 78.50%] [G loss: 0.877886]\n",
      "191 [D loss: 0.542468, acc.: 77.00%] [G loss: 0.925658]\n",
      "192 [D loss: 0.520854, acc.: 82.50%] [G loss: 0.873314]\n",
      "193 [D loss: 0.518728, acc.: 85.00%] [G loss: 0.924993]\n",
      "194 [D loss: 0.548158, acc.: 81.50%] [G loss: 0.964595]\n",
      "195 [D loss: 0.509843, acc.: 81.00%] [G loss: 0.950875]\n",
      "196 [D loss: 0.519514, acc.: 79.50%] [G loss: 0.926215]\n",
      "197 [D loss: 0.503817, acc.: 83.00%] [G loss: 1.096726]\n",
      "198 [D loss: 0.551383, acc.: 76.50%] [G loss: 0.972830]\n",
      "199 [D loss: 0.496338, acc.: 82.50%] [G loss: 0.973155]\n",
      "200 [D loss: 0.515884, acc.: 83.50%] [G loss: 0.956550]\n",
      "201 [D loss: 0.491016, acc.: 84.50%] [G loss: 0.972755]\n",
      "202 [D loss: 0.518123, acc.: 86.00%] [G loss: 1.011728]\n",
      "203 [D loss: 0.524021, acc.: 82.50%] [G loss: 0.960123]\n",
      "204 [D loss: 0.528911, acc.: 78.00%] [G loss: 1.031551]\n",
      "205 [D loss: 0.537682, acc.: 77.00%] [G loss: 0.897011]\n",
      "206 [D loss: 0.575293, acc.: 78.00%] [G loss: 0.982608]\n",
      "207 [D loss: 0.534609, acc.: 82.00%] [G loss: 0.912737]\n",
      "208 [D loss: 0.523002, acc.: 79.50%] [G loss: 1.074872]\n",
      "209 [D loss: 0.521425, acc.: 83.00%] [G loss: 0.880166]\n",
      "210 [D loss: 0.546855, acc.: 79.00%] [G loss: 0.984814]\n",
      "211 [D loss: 0.550304, acc.: 81.50%] [G loss: 0.999009]\n",
      "212 [D loss: 0.519796, acc.: 83.00%] [G loss: 1.095485]\n",
      "213 [D loss: 0.539961, acc.: 82.50%] [G loss: 1.003030]\n",
      "214 [D loss: 0.549156, acc.: 79.50%] [G loss: 0.967001]\n",
      "215 [D loss: 0.500355, acc.: 85.00%] [G loss: 0.954494]\n",
      "216 [D loss: 0.524900, acc.: 79.00%] [G loss: 1.030929]\n",
      "217 [D loss: 0.495435, acc.: 85.50%] [G loss: 0.926454]\n",
      "218 [D loss: 0.534416, acc.: 81.00%] [G loss: 0.957657]\n",
      "219 [D loss: 0.513057, acc.: 82.50%] [G loss: 1.034636]\n",
      "220 [D loss: 0.474144, acc.: 88.50%] [G loss: 1.023881]\n",
      "221 [D loss: 0.530259, acc.: 77.50%] [G loss: 0.961368]\n",
      "222 [D loss: 0.510679, acc.: 84.50%] [G loss: 0.934980]\n",
      "223 [D loss: 0.486798, acc.: 83.00%] [G loss: 1.007282]\n",
      "224 [D loss: 0.489302, acc.: 84.00%] [G loss: 1.004898]\n",
      "225 [D loss: 0.500296, acc.: 84.50%] [G loss: 1.071279]\n",
      "226 [D loss: 0.537064, acc.: 80.00%] [G loss: 0.995904]\n",
      "227 [D loss: 0.519567, acc.: 82.00%] [G loss: 0.937510]\n",
      "228 [D loss: 0.486784, acc.: 85.50%] [G loss: 1.061255]\n",
      "229 [D loss: 0.499346, acc.: 83.50%] [G loss: 0.981371]\n",
      "230 [D loss: 0.473488, acc.: 90.00%] [G loss: 0.966056]\n",
      "231 [D loss: 0.482122, acc.: 84.50%] [G loss: 1.032820]\n",
      "232 [D loss: 0.517646, acc.: 81.50%] [G loss: 0.982308]\n",
      "233 [D loss: 0.495787, acc.: 87.00%] [G loss: 0.991196]\n",
      "234 [D loss: 0.492339, acc.: 86.00%] [G loss: 1.079974]\n",
      "235 [D loss: 0.490260, acc.: 83.50%] [G loss: 1.033764]\n",
      "236 [D loss: 0.526232, acc.: 84.00%] [G loss: 1.015712]\n",
      "237 [D loss: 0.508813, acc.: 82.50%] [G loss: 1.036023]\n",
      "238 [D loss: 0.476897, acc.: 88.00%] [G loss: 1.120780]\n",
      "239 [D loss: 0.506033, acc.: 86.00%] [G loss: 0.985417]\n",
      "240 [D loss: 0.519423, acc.: 84.50%] [G loss: 0.997756]\n",
      "241 [D loss: 0.502336, acc.: 84.00%] [G loss: 1.089702]\n",
      "242 [D loss: 0.516083, acc.: 82.50%] [G loss: 1.027043]\n",
      "243 [D loss: 0.496168, acc.: 82.00%] [G loss: 1.015663]\n",
      "244 [D loss: 0.520317, acc.: 78.50%] [G loss: 0.998083]\n",
      "245 [D loss: 0.488558, acc.: 84.50%] [G loss: 0.961744]\n",
      "246 [D loss: 0.509229, acc.: 84.50%] [G loss: 0.937988]\n",
      "247 [D loss: 0.522240, acc.: 83.00%] [G loss: 1.040451]\n",
      "248 [D loss: 0.499583, acc.: 85.00%] [G loss: 1.051936]\n",
      "249 [D loss: 0.469384, acc.: 85.50%] [G loss: 1.054301]\n",
      "250 [D loss: 0.531172, acc.: 83.00%] [G loss: 1.010867]\n",
      "251 [D loss: 0.478510, acc.: 85.50%] [G loss: 1.115921]\n",
      "252 [D loss: 0.484065, acc.: 87.50%] [G loss: 1.020446]\n",
      "253 [D loss: 0.511139, acc.: 82.00%] [G loss: 0.956591]\n",
      "254 [D loss: 0.475808, acc.: 86.50%] [G loss: 1.068296]\n",
      "255 [D loss: 0.521271, acc.: 80.50%] [G loss: 1.103166]\n",
      "256 [D loss: 0.499356, acc.: 83.00%] [G loss: 0.963211]\n",
      "257 [D loss: 0.504013, acc.: 82.00%] [G loss: 1.051300]\n",
      "258 [D loss: 0.460024, acc.: 87.50%] [G loss: 0.934632]\n",
      "259 [D loss: 0.503082, acc.: 82.00%] [G loss: 1.029115]\n",
      "260 [D loss: 0.510533, acc.: 82.00%] [G loss: 1.036656]\n",
      "261 [D loss: 0.465537, acc.: 85.50%] [G loss: 1.024276]\n",
      "262 [D loss: 0.481011, acc.: 87.50%] [G loss: 1.041276]\n",
      "263 [D loss: 0.485280, acc.: 87.50%] [G loss: 1.058782]\n",
      "264 [D loss: 0.469620, acc.: 85.50%] [G loss: 1.052729]\n",
      "265 [D loss: 0.494152, acc.: 82.00%] [G loss: 1.013349]\n",
      "266 [D loss: 0.512543, acc.: 84.00%] [G loss: 1.091727]\n",
      "267 [D loss: 0.485439, acc.: 86.50%] [G loss: 1.063911]\n",
      "268 [D loss: 0.469391, acc.: 87.50%] [G loss: 1.124666]\n",
      "269 [D loss: 0.462267, acc.: 89.00%] [G loss: 1.145671]\n",
      "270 [D loss: 0.431218, acc.: 92.00%] [G loss: 1.064824]\n",
      "271 [D loss: 0.497330, acc.: 87.00%] [G loss: 1.032177]\n",
      "272 [D loss: 0.477711, acc.: 86.00%] [G loss: 1.022106]\n",
      "273 [D loss: 0.479381, acc.: 87.00%] [G loss: 1.026758]\n",
      "274 [D loss: 0.495239, acc.: 82.50%] [G loss: 1.110312]\n",
      "275 [D loss: 0.494558, acc.: 85.00%] [G loss: 1.076695]\n",
      "276 [D loss: 0.419714, acc.: 91.00%] [G loss: 1.035431]\n",
      "277 [D loss: 0.487946, acc.: 84.00%] [G loss: 1.061934]\n",
      "278 [D loss: 0.441608, acc.: 90.50%] [G loss: 1.074660]\n",
      "279 [D loss: 0.485004, acc.: 86.00%] [G loss: 1.027049]\n",
      "280 [D loss: 0.471817, acc.: 86.00%] [G loss: 1.112822]\n",
      "281 [D loss: 0.466884, acc.: 86.50%] [G loss: 1.108000]\n",
      "282 [D loss: 0.449271, acc.: 90.50%] [G loss: 1.095771]\n",
      "283 [D loss: 0.472373, acc.: 83.00%] [G loss: 0.926846]\n",
      "284 [D loss: 0.486769, acc.: 82.50%] [G loss: 1.120294]\n",
      "285 [D loss: 0.455274, acc.: 89.00%] [G loss: 1.057497]\n",
      "286 [D loss: 0.464571, acc.: 89.50%] [G loss: 1.059162]\n",
      "287 [D loss: 0.487589, acc.: 83.50%] [G loss: 1.145608]\n",
      "288 [D loss: 0.487563, acc.: 84.00%] [G loss: 1.045525]\n",
      "289 [D loss: 0.465180, acc.: 87.50%] [G loss: 1.089194]\n",
      "290 [D loss: 0.469709, acc.: 88.50%] [G loss: 1.080256]\n",
      "291 [D loss: 0.488926, acc.: 84.50%] [G loss: 1.069901]\n",
      "292 [D loss: 0.474296, acc.: 87.50%] [G loss: 1.148304]\n",
      "293 [D loss: 0.482522, acc.: 87.00%] [G loss: 1.170053]\n",
      "294 [D loss: 0.450589, acc.: 87.50%] [G loss: 1.161878]\n",
      "295 [D loss: 0.494918, acc.: 83.00%] [G loss: 1.101585]\n",
      "296 [D loss: 0.461744, acc.: 84.50%] [G loss: 1.119037]\n",
      "297 [D loss: 0.429453, acc.: 90.50%] [G loss: 1.097275]\n",
      "298 [D loss: 0.453781, acc.: 90.50%] [G loss: 0.986278]\n",
      "299 [D loss: 0.476888, acc.: 87.00%] [G loss: 1.057049]\n",
      "300 [D loss: 0.461030, acc.: 87.00%] [G loss: 1.132460]\n",
      "301 [D loss: 0.476131, acc.: 84.50%] [G loss: 1.077460]\n",
      "302 [D loss: 0.498106, acc.: 84.00%] [G loss: 1.118662]\n",
      "303 [D loss: 0.476237, acc.: 86.00%] [G loss: 1.096032]\n",
      "304 [D loss: 0.484910, acc.: 86.50%] [G loss: 1.185467]\n",
      "305 [D loss: 0.446872, acc.: 91.00%] [G loss: 1.133541]\n",
      "306 [D loss: 0.469149, acc.: 84.00%] [G loss: 1.178007]\n",
      "307 [D loss: 0.456071, acc.: 86.50%] [G loss: 1.105839]\n",
      "308 [D loss: 0.465696, acc.: 89.50%] [G loss: 1.162566]\n",
      "309 [D loss: 0.486749, acc.: 85.50%] [G loss: 1.035438]\n",
      "310 [D loss: 0.448745, acc.: 90.50%] [G loss: 1.135430]\n",
      "311 [D loss: 0.465056, acc.: 84.50%] [G loss: 1.143944]\n",
      "312 [D loss: 0.433654, acc.: 90.50%] [G loss: 1.144183]\n",
      "313 [D loss: 0.483884, acc.: 87.00%] [G loss: 1.159799]\n",
      "314 [D loss: 0.447884, acc.: 89.00%] [G loss: 1.106384]\n",
      "315 [D loss: 0.444709, acc.: 86.50%] [G loss: 1.101673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 [D loss: 0.460730, acc.: 87.00%] [G loss: 1.066247]\n",
      "317 [D loss: 0.458567, acc.: 87.50%] [G loss: 1.130818]\n",
      "318 [D loss: 0.446724, acc.: 86.00%] [G loss: 1.102477]\n",
      "319 [D loss: 0.430714, acc.: 87.50%] [G loss: 1.118303]\n",
      "320 [D loss: 0.440261, acc.: 88.00%] [G loss: 1.138975]\n",
      "321 [D loss: 0.455680, acc.: 87.50%] [G loss: 1.114068]\n",
      "322 [D loss: 0.452599, acc.: 87.50%] [G loss: 1.104244]\n",
      "323 [D loss: 0.447211, acc.: 88.50%] [G loss: 1.118085]\n",
      "324 [D loss: 0.450036, acc.: 88.00%] [G loss: 1.084921]\n",
      "325 [D loss: 0.426285, acc.: 92.00%] [G loss: 1.205724]\n",
      "326 [D loss: 0.433250, acc.: 89.00%] [G loss: 1.205963]\n",
      "327 [D loss: 0.460213, acc.: 87.50%] [G loss: 1.123146]\n",
      "328 [D loss: 0.466546, acc.: 86.00%] [G loss: 1.156289]\n",
      "329 [D loss: 0.451644, acc.: 88.00%] [G loss: 1.137854]\n",
      "330 [D loss: 0.467229, acc.: 85.00%] [G loss: 1.160600]\n",
      "331 [D loss: 0.424506, acc.: 92.50%] [G loss: 1.188892]\n",
      "332 [D loss: 0.440074, acc.: 86.00%] [G loss: 1.144649]\n",
      "333 [D loss: 0.427669, acc.: 88.50%] [G loss: 1.136608]\n",
      "334 [D loss: 0.456202, acc.: 88.00%] [G loss: 1.210728]\n",
      "335 [D loss: 0.450868, acc.: 87.00%] [G loss: 1.221134]\n",
      "336 [D loss: 0.434772, acc.: 90.50%] [G loss: 1.109822]\n",
      "337 [D loss: 0.443512, acc.: 91.00%] [G loss: 1.115332]\n",
      "338 [D loss: 0.435522, acc.: 89.00%] [G loss: 1.224708]\n",
      "339 [D loss: 0.460317, acc.: 85.50%] [G loss: 1.166245]\n",
      "340 [D loss: 0.433716, acc.: 92.00%] [G loss: 1.148259]\n",
      "341 [D loss: 0.419408, acc.: 93.50%] [G loss: 1.147369]\n",
      "342 [D loss: 0.467637, acc.: 85.50%] [G loss: 1.131337]\n",
      "343 [D loss: 0.437319, acc.: 90.50%] [G loss: 1.149470]\n",
      "344 [D loss: 0.447263, acc.: 88.50%] [G loss: 1.257666]\n",
      "345 [D loss: 0.458906, acc.: 87.50%] [G loss: 1.215284]\n",
      "346 [D loss: 0.452139, acc.: 87.00%] [G loss: 1.203915]\n",
      "347 [D loss: 0.413952, acc.: 91.50%] [G loss: 1.108814]\n",
      "348 [D loss: 0.434408, acc.: 89.50%] [G loss: 1.207229]\n",
      "349 [D loss: 0.434785, acc.: 90.00%] [G loss: 1.254445]\n",
      "350 [D loss: 0.437415, acc.: 90.00%] [G loss: 1.209169]\n",
      "351 [D loss: 0.464809, acc.: 86.50%] [G loss: 1.321325]\n",
      "352 [D loss: 0.438955, acc.: 92.50%] [G loss: 1.241627]\n",
      "353 [D loss: 0.430622, acc.: 90.00%] [G loss: 1.210106]\n",
      "354 [D loss: 0.467934, acc.: 88.00%] [G loss: 1.228173]\n",
      "355 [D loss: 0.449079, acc.: 90.00%] [G loss: 1.236523]\n",
      "356 [D loss: 0.437684, acc.: 88.50%] [G loss: 1.244227]\n",
      "357 [D loss: 0.419061, acc.: 90.50%] [G loss: 1.111112]\n",
      "358 [D loss: 0.425329, acc.: 92.00%] [G loss: 1.271891]\n",
      "359 [D loss: 0.436506, acc.: 92.00%] [G loss: 1.191742]\n",
      "360 [D loss: 0.423840, acc.: 93.00%] [G loss: 1.185505]\n",
      "361 [D loss: 0.423720, acc.: 91.00%] [G loss: 1.158480]\n",
      "362 [D loss: 0.451896, acc.: 87.00%] [G loss: 1.200662]\n",
      "363 [D loss: 0.435120, acc.: 90.50%] [G loss: 1.065664]\n",
      "364 [D loss: 0.473729, acc.: 84.50%] [G loss: 1.248778]\n",
      "365 [D loss: 0.422660, acc.: 92.50%] [G loss: 1.202228]\n",
      "366 [D loss: 0.435686, acc.: 88.00%] [G loss: 1.310076]\n",
      "367 [D loss: 0.410124, acc.: 91.00%] [G loss: 1.236565]\n",
      "368 [D loss: 0.413134, acc.: 90.50%] [G loss: 1.224389]\n",
      "369 [D loss: 0.433538, acc.: 90.50%] [G loss: 1.194723]\n",
      "370 [D loss: 0.422357, acc.: 89.50%] [G loss: 1.227865]\n",
      "371 [D loss: 0.437683, acc.: 91.50%] [G loss: 1.170723]\n",
      "372 [D loss: 0.421661, acc.: 90.00%] [G loss: 1.264907]\n",
      "373 [D loss: 0.430585, acc.: 91.00%] [G loss: 1.236438]\n",
      "374 [D loss: 0.425471, acc.: 91.00%] [G loss: 1.177534]\n",
      "375 [D loss: 0.420262, acc.: 90.50%] [G loss: 1.227288]\n",
      "376 [D loss: 0.416698, acc.: 89.50%] [G loss: 1.211676]\n",
      "377 [D loss: 0.431348, acc.: 91.00%] [G loss: 1.296565]\n",
      "378 [D loss: 0.420714, acc.: 90.00%] [G loss: 1.256712]\n",
      "379 [D loss: 0.412839, acc.: 92.50%] [G loss: 1.183727]\n",
      "380 [D loss: 0.390796, acc.: 94.50%] [G loss: 1.201603]\n",
      "381 [D loss: 0.432830, acc.: 91.00%] [G loss: 1.140834]\n",
      "382 [D loss: 0.440936, acc.: 89.50%] [G loss: 1.177857]\n",
      "383 [D loss: 0.437262, acc.: 91.00%] [G loss: 1.196003]\n",
      "384 [D loss: 0.398460, acc.: 92.50%] [G loss: 1.190710]\n",
      "385 [D loss: 0.390259, acc.: 93.50%] [G loss: 1.280127]\n",
      "386 [D loss: 0.413342, acc.: 91.50%] [G loss: 1.201022]\n",
      "387 [D loss: 0.401109, acc.: 92.50%] [G loss: 1.225952]\n",
      "388 [D loss: 0.438635, acc.: 92.00%] [G loss: 1.287033]\n",
      "389 [D loss: 0.418496, acc.: 92.00%] [G loss: 1.245067]\n",
      "390 [D loss: 0.446942, acc.: 89.50%] [G loss: 1.273828]\n",
      "391 [D loss: 0.452735, acc.: 89.00%] [G loss: 1.318825]\n",
      "392 [D loss: 0.434056, acc.: 90.00%] [G loss: 1.189025]\n",
      "393 [D loss: 0.400292, acc.: 93.00%] [G loss: 1.296678]\n",
      "394 [D loss: 0.423577, acc.: 89.00%] [G loss: 1.108973]\n",
      "395 [D loss: 0.414071, acc.: 90.50%] [G loss: 1.292125]\n",
      "396 [D loss: 0.419351, acc.: 94.00%] [G loss: 1.291631]\n",
      "397 [D loss: 0.436947, acc.: 89.00%] [G loss: 1.236680]\n",
      "398 [D loss: 0.405809, acc.: 89.50%] [G loss: 1.203411]\n",
      "399 [D loss: 0.427196, acc.: 91.50%] [G loss: 1.277176]\n",
      "400 [D loss: 0.440610, acc.: 86.50%] [G loss: 1.146183]\n",
      "401 [D loss: 0.425528, acc.: 89.50%] [G loss: 1.316786]\n",
      "402 [D loss: 0.413469, acc.: 92.50%] [G loss: 1.257065]\n",
      "403 [D loss: 0.427585, acc.: 91.00%] [G loss: 1.214301]\n",
      "404 [D loss: 0.443081, acc.: 90.00%] [G loss: 1.333431]\n",
      "405 [D loss: 0.424706, acc.: 92.00%] [G loss: 1.355332]\n",
      "406 [D loss: 0.435656, acc.: 91.50%] [G loss: 1.252980]\n",
      "407 [D loss: 0.412423, acc.: 92.50%] [G loss: 1.272835]\n",
      "408 [D loss: 0.440384, acc.: 90.00%] [G loss: 1.215505]\n",
      "409 [D loss: 0.421355, acc.: 89.50%] [G loss: 1.274225]\n",
      "410 [D loss: 0.414705, acc.: 91.50%] [G loss: 1.314246]\n",
      "411 [D loss: 0.419832, acc.: 89.50%] [G loss: 1.227273]\n",
      "412 [D loss: 0.407131, acc.: 91.50%] [G loss: 1.289408]\n",
      "413 [D loss: 0.398699, acc.: 94.50%] [G loss: 1.331983]\n",
      "414 [D loss: 0.410859, acc.: 89.50%] [G loss: 1.239538]\n",
      "415 [D loss: 0.383785, acc.: 92.50%] [G loss: 1.251967]\n",
      "416 [D loss: 0.396526, acc.: 92.00%] [G loss: 1.322549]\n",
      "417 [D loss: 0.420302, acc.: 91.00%] [G loss: 1.402544]\n",
      "418 [D loss: 0.407448, acc.: 89.00%] [G loss: 1.262083]\n",
      "419 [D loss: 0.422364, acc.: 88.50%] [G loss: 1.323761]\n",
      "420 [D loss: 0.411422, acc.: 92.00%] [G loss: 1.208725]\n",
      "421 [D loss: 0.380698, acc.: 93.50%] [G loss: 1.370355]\n",
      "422 [D loss: 0.401107, acc.: 91.50%] [G loss: 1.249317]\n",
      "423 [D loss: 0.438500, acc.: 87.50%] [G loss: 1.234610]\n",
      "424 [D loss: 0.399325, acc.: 93.50%] [G loss: 1.287781]\n",
      "425 [D loss: 0.406082, acc.: 90.00%] [G loss: 1.286388]\n",
      "426 [D loss: 0.394931, acc.: 89.50%] [G loss: 1.383600]\n",
      "427 [D loss: 0.392667, acc.: 93.00%] [G loss: 1.330605]\n",
      "428 [D loss: 0.396820, acc.: 94.50%] [G loss: 1.202336]\n",
      "429 [D loss: 0.413649, acc.: 89.50%] [G loss: 1.382302]\n",
      "430 [D loss: 0.383154, acc.: 93.00%] [G loss: 1.318793]\n",
      "431 [D loss: 0.393161, acc.: 94.00%] [G loss: 1.339911]\n",
      "432 [D loss: 0.412562, acc.: 90.50%] [G loss: 1.344324]\n",
      "433 [D loss: 0.377141, acc.: 93.50%] [G loss: 1.262792]\n",
      "434 [D loss: 0.395288, acc.: 90.00%] [G loss: 1.309343]\n",
      "435 [D loss: 0.390238, acc.: 94.00%] [G loss: 1.332555]\n",
      "436 [D loss: 0.375435, acc.: 92.00%] [G loss: 1.219252]\n",
      "437 [D loss: 0.385354, acc.: 95.00%] [G loss: 1.346777]\n",
      "438 [D loss: 0.386976, acc.: 94.00%] [G loss: 1.364810]\n",
      "439 [D loss: 0.401395, acc.: 92.50%] [G loss: 1.303708]\n",
      "440 [D loss: 0.401901, acc.: 92.50%] [G loss: 1.314657]\n",
      "441 [D loss: 0.386132, acc.: 92.50%] [G loss: 1.319137]\n",
      "442 [D loss: 0.410157, acc.: 89.50%] [G loss: 1.386594]\n",
      "443 [D loss: 0.403642, acc.: 90.00%] [G loss: 1.220137]\n",
      "444 [D loss: 0.393700, acc.: 91.00%] [G loss: 1.369846]\n",
      "445 [D loss: 0.395568, acc.: 92.50%] [G loss: 1.320272]\n",
      "446 [D loss: 0.425157, acc.: 89.50%] [G loss: 1.384100]\n",
      "447 [D loss: 0.399525, acc.: 92.00%] [G loss: 1.354451]\n",
      "448 [D loss: 0.409126, acc.: 92.00%] [G loss: 1.362811]\n",
      "449 [D loss: 0.365452, acc.: 93.50%] [G loss: 1.398489]\n",
      "450 [D loss: 0.403805, acc.: 91.50%] [G loss: 1.268524]\n",
      "451 [D loss: 0.425576, acc.: 90.50%] [G loss: 1.227651]\n",
      "452 [D loss: 0.391854, acc.: 94.00%] [G loss: 1.266088]\n",
      "453 [D loss: 0.384960, acc.: 91.50%] [G loss: 1.416534]\n",
      "454 [D loss: 0.378752, acc.: 93.50%] [G loss: 1.346648]\n",
      "455 [D loss: 0.385492, acc.: 93.50%] [G loss: 1.335653]\n",
      "456 [D loss: 0.413579, acc.: 91.50%] [G loss: 1.402333]\n",
      "457 [D loss: 0.393697, acc.: 93.00%] [G loss: 1.344146]\n",
      "458 [D loss: 0.402441, acc.: 91.00%] [G loss: 1.293030]\n",
      "459 [D loss: 0.396954, acc.: 93.00%] [G loss: 1.300094]\n",
      "460 [D loss: 0.393151, acc.: 94.00%] [G loss: 1.338868]\n",
      "461 [D loss: 0.400898, acc.: 93.00%] [G loss: 1.383453]\n",
      "462 [D loss: 0.399891, acc.: 94.00%] [G loss: 1.439308]\n",
      "463 [D loss: 0.399405, acc.: 91.00%] [G loss: 1.361555]\n",
      "464 [D loss: 0.363456, acc.: 94.50%] [G loss: 1.379409]\n",
      "465 [D loss: 0.364411, acc.: 96.00%] [G loss: 1.244384]\n",
      "466 [D loss: 0.406755, acc.: 91.50%] [G loss: 1.426036]\n",
      "467 [D loss: 0.383141, acc.: 92.50%] [G loss: 1.261846]\n",
      "468 [D loss: 0.373367, acc.: 94.50%] [G loss: 1.364073]\n",
      "469 [D loss: 0.371751, acc.: 95.00%] [G loss: 1.303225]\n",
      "470 [D loss: 0.407172, acc.: 92.00%] [G loss: 1.353529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 [D loss: 0.391028, acc.: 94.00%] [G loss: 1.399555]\n",
      "472 [D loss: 0.386853, acc.: 92.50%] [G loss: 1.426423]\n",
      "473 [D loss: 0.378753, acc.: 94.50%] [G loss: 1.378647]\n",
      "474 [D loss: 0.373134, acc.: 94.00%] [G loss: 1.490147]\n",
      "475 [D loss: 0.406595, acc.: 92.00%] [G loss: 1.363448]\n",
      "476 [D loss: 0.361595, acc.: 95.50%] [G loss: 1.315188]\n",
      "477 [D loss: 0.376493, acc.: 91.50%] [G loss: 1.404727]\n",
      "478 [D loss: 0.382540, acc.: 93.00%] [G loss: 1.422559]\n",
      "479 [D loss: 0.364750, acc.: 95.00%] [G loss: 1.272986]\n",
      "480 [D loss: 0.382828, acc.: 93.00%] [G loss: 1.420775]\n",
      "481 [D loss: 0.391095, acc.: 91.00%] [G loss: 1.293686]\n",
      "482 [D loss: 0.391579, acc.: 92.50%] [G loss: 1.412315]\n",
      "483 [D loss: 0.390518, acc.: 92.00%] [G loss: 1.451744]\n",
      "484 [D loss: 0.388096, acc.: 95.00%] [G loss: 1.409824]\n",
      "485 [D loss: 0.359868, acc.: 93.50%] [G loss: 1.442867]\n",
      "486 [D loss: 0.395302, acc.: 92.00%] [G loss: 1.404374]\n",
      "487 [D loss: 0.423665, acc.: 90.00%] [G loss: 1.366713]\n",
      "488 [D loss: 0.369571, acc.: 95.00%] [G loss: 1.446787]\n",
      "489 [D loss: 0.378122, acc.: 94.00%] [G loss: 1.409015]\n",
      "490 [D loss: 0.379081, acc.: 91.50%] [G loss: 1.412638]\n",
      "491 [D loss: 0.350932, acc.: 97.00%] [G loss: 1.464766]\n",
      "492 [D loss: 0.407687, acc.: 90.50%] [G loss: 1.407574]\n",
      "493 [D loss: 0.378172, acc.: 94.00%] [G loss: 1.438459]\n",
      "494 [D loss: 0.378195, acc.: 93.00%] [G loss: 1.367193]\n",
      "495 [D loss: 0.393405, acc.: 92.00%] [G loss: 1.360323]\n",
      "496 [D loss: 0.392519, acc.: 92.50%] [G loss: 1.332636]\n",
      "497 [D loss: 0.368093, acc.: 95.00%] [G loss: 1.416857]\n",
      "498 [D loss: 0.355553, acc.: 97.00%] [G loss: 1.345063]\n",
      "499 [D loss: 0.365805, acc.: 94.50%] [G loss: 1.310401]\n",
      "500 [D loss: 0.366634, acc.: 94.50%] [G loss: 1.374471]\n",
      "501 [D loss: 0.395477, acc.: 90.00%] [G loss: 1.416812]\n",
      "502 [D loss: 0.363269, acc.: 96.50%] [G loss: 1.378367]\n",
      "503 [D loss: 0.394394, acc.: 92.00%] [G loss: 1.373007]\n",
      "504 [D loss: 0.388203, acc.: 90.50%] [G loss: 1.380911]\n",
      "505 [D loss: 0.384328, acc.: 92.00%] [G loss: 1.446201]\n",
      "506 [D loss: 0.373696, acc.: 93.50%] [G loss: 1.335201]\n",
      "507 [D loss: 0.370602, acc.: 95.00%] [G loss: 1.477019]\n",
      "508 [D loss: 0.386940, acc.: 95.50%] [G loss: 1.445205]\n",
      "509 [D loss: 0.359393, acc.: 93.50%] [G loss: 1.374801]\n",
      "510 [D loss: 0.367007, acc.: 97.50%] [G loss: 1.426590]\n",
      "511 [D loss: 0.354865, acc.: 94.00%] [G loss: 1.418486]\n",
      "512 [D loss: 0.343783, acc.: 95.50%] [G loss: 1.426945]\n",
      "513 [D loss: 0.369105, acc.: 92.50%] [G loss: 1.504468]\n",
      "514 [D loss: 0.403580, acc.: 93.50%] [G loss: 1.437450]\n",
      "515 [D loss: 0.379846, acc.: 93.50%] [G loss: 1.473210]\n",
      "516 [D loss: 0.373877, acc.: 95.00%] [G loss: 1.467103]\n",
      "517 [D loss: 0.374267, acc.: 92.00%] [G loss: 1.420928]\n",
      "518 [D loss: 0.399916, acc.: 90.50%] [G loss: 1.307611]\n",
      "519 [D loss: 0.356116, acc.: 93.50%] [G loss: 1.447940]\n",
      "520 [D loss: 0.388408, acc.: 94.00%] [G loss: 1.496953]\n",
      "521 [D loss: 0.364794, acc.: 94.00%] [G loss: 1.484684]\n",
      "522 [D loss: 0.375546, acc.: 92.00%] [G loss: 1.405362]\n",
      "523 [D loss: 0.380934, acc.: 94.00%] [G loss: 1.451114]\n",
      "524 [D loss: 0.375934, acc.: 92.50%] [G loss: 1.529484]\n",
      "525 [D loss: 0.366049, acc.: 92.00%] [G loss: 1.461681]\n",
      "526 [D loss: 0.401754, acc.: 91.00%] [G loss: 1.571119]\n",
      "527 [D loss: 0.365459, acc.: 94.00%] [G loss: 1.445274]\n",
      "528 [D loss: 0.378703, acc.: 95.00%] [G loss: 1.472165]\n",
      "529 [D loss: 0.355360, acc.: 95.50%] [G loss: 1.564309]\n",
      "530 [D loss: 0.365262, acc.: 93.00%] [G loss: 1.467451]\n",
      "531 [D loss: 0.366611, acc.: 94.00%] [G loss: 1.439532]\n",
      "532 [D loss: 0.357943, acc.: 94.50%] [G loss: 1.425807]\n",
      "533 [D loss: 0.343660, acc.: 96.00%] [G loss: 1.441959]\n",
      "534 [D loss: 0.348625, acc.: 95.50%] [G loss: 1.418150]\n",
      "535 [D loss: 0.355412, acc.: 95.50%] [G loss: 1.500473]\n",
      "536 [D loss: 0.379496, acc.: 93.50%] [G loss: 1.393258]\n",
      "537 [D loss: 0.410837, acc.: 91.00%] [G loss: 1.501093]\n",
      "538 [D loss: 0.358926, acc.: 94.00%] [G loss: 1.462060]\n",
      "539 [D loss: 0.390898, acc.: 93.50%] [G loss: 1.358113]\n",
      "540 [D loss: 0.371217, acc.: 93.50%] [G loss: 1.511888]\n",
      "541 [D loss: 0.363942, acc.: 92.00%] [G loss: 1.534218]\n",
      "542 [D loss: 0.358249, acc.: 93.00%] [G loss: 1.433100]\n",
      "543 [D loss: 0.384484, acc.: 91.00%] [G loss: 1.521914]\n",
      "544 [D loss: 0.359823, acc.: 94.00%] [G loss: 1.405872]\n",
      "545 [D loss: 0.366164, acc.: 94.50%] [G loss: 1.405563]\n",
      "546 [D loss: 0.346813, acc.: 94.50%] [G loss: 1.361337]\n",
      "547 [D loss: 0.353677, acc.: 93.50%] [G loss: 1.529947]\n",
      "548 [D loss: 0.344088, acc.: 94.00%] [G loss: 1.384515]\n",
      "549 [D loss: 0.350502, acc.: 95.50%] [G loss: 1.368811]\n",
      "550 [D loss: 0.369862, acc.: 93.50%] [G loss: 1.472582]\n",
      "551 [D loss: 0.362392, acc.: 94.00%] [G loss: 1.491238]\n",
      "552 [D loss: 0.371121, acc.: 96.50%] [G loss: 1.443796]\n",
      "553 [D loss: 0.374574, acc.: 94.00%] [G loss: 1.475551]\n",
      "554 [D loss: 0.353326, acc.: 95.00%] [G loss: 1.435735]\n",
      "555 [D loss: 0.346290, acc.: 96.00%] [G loss: 1.515429]\n",
      "556 [D loss: 0.365858, acc.: 94.50%] [G loss: 1.554018]\n",
      "557 [D loss: 0.356675, acc.: 94.00%] [G loss: 1.478224]\n",
      "558 [D loss: 0.363007, acc.: 94.00%] [G loss: 1.484170]\n",
      "559 [D loss: 0.375675, acc.: 92.50%] [G loss: 1.386314]\n",
      "560 [D loss: 0.369477, acc.: 95.00%] [G loss: 1.527699]\n",
      "561 [D loss: 0.340326, acc.: 94.50%] [G loss: 1.493536]\n",
      "562 [D loss: 0.343069, acc.: 94.00%] [G loss: 1.455946]\n",
      "563 [D loss: 0.348465, acc.: 96.00%] [G loss: 1.420936]\n",
      "564 [D loss: 0.333263, acc.: 96.50%] [G loss: 1.469701]\n",
      "565 [D loss: 0.343376, acc.: 96.00%] [G loss: 1.468212]\n",
      "566 [D loss: 0.353412, acc.: 95.00%] [G loss: 1.593247]\n",
      "567 [D loss: 0.361764, acc.: 95.50%] [G loss: 1.439876]\n",
      "568 [D loss: 0.342551, acc.: 95.50%] [G loss: 1.490298]\n",
      "569 [D loss: 0.358486, acc.: 95.00%] [G loss: 1.463787]\n",
      "570 [D loss: 0.333737, acc.: 97.00%] [G loss: 1.575830]\n",
      "571 [D loss: 0.365285, acc.: 93.50%] [G loss: 1.451135]\n",
      "572 [D loss: 0.349655, acc.: 93.50%] [G loss: 1.514312]\n",
      "573 [D loss: 0.365748, acc.: 92.50%] [G loss: 1.500934]\n",
      "574 [D loss: 0.344546, acc.: 96.50%] [G loss: 1.468028]\n",
      "575 [D loss: 0.338702, acc.: 97.00%] [G loss: 1.497816]\n",
      "576 [D loss: 0.352604, acc.: 96.00%] [G loss: 1.480543]\n",
      "577 [D loss: 0.333519, acc.: 95.50%] [G loss: 1.455549]\n",
      "578 [D loss: 0.365381, acc.: 93.00%] [G loss: 1.596582]\n",
      "579 [D loss: 0.354874, acc.: 97.50%] [G loss: 1.470780]\n",
      "580 [D loss: 0.344339, acc.: 95.50%] [G loss: 1.552946]\n",
      "581 [D loss: 0.380144, acc.: 92.50%] [G loss: 1.434988]\n",
      "582 [D loss: 0.335294, acc.: 96.00%] [G loss: 1.512654]\n",
      "583 [D loss: 0.361174, acc.: 94.50%] [G loss: 1.594887]\n",
      "584 [D loss: 0.347123, acc.: 93.50%] [G loss: 1.459381]\n",
      "585 [D loss: 0.353680, acc.: 94.00%] [G loss: 1.495727]\n",
      "586 [D loss: 0.337331, acc.: 95.50%] [G loss: 1.459874]\n",
      "587 [D loss: 0.346314, acc.: 95.50%] [G loss: 1.476561]\n",
      "588 [D loss: 0.378035, acc.: 92.50%] [G loss: 1.488165]\n",
      "589 [D loss: 0.373472, acc.: 94.00%] [G loss: 1.552884]\n",
      "590 [D loss: 0.363521, acc.: 92.50%] [G loss: 1.416373]\n",
      "591 [D loss: 0.329163, acc.: 95.50%] [G loss: 1.411416]\n",
      "592 [D loss: 0.363108, acc.: 96.00%] [G loss: 1.505934]\n",
      "593 [D loss: 0.369802, acc.: 94.00%] [G loss: 1.559949]\n",
      "594 [D loss: 0.353811, acc.: 94.00%] [G loss: 1.532922]\n",
      "595 [D loss: 0.353638, acc.: 94.00%] [G loss: 1.460961]\n",
      "596 [D loss: 0.339447, acc.: 95.50%] [G loss: 1.533867]\n",
      "597 [D loss: 0.329769, acc.: 96.00%] [G loss: 1.538112]\n",
      "598 [D loss: 0.352062, acc.: 94.00%] [G loss: 1.587056]\n",
      "599 [D loss: 0.365650, acc.: 93.50%] [G loss: 1.541481]\n",
      "600 [D loss: 0.348208, acc.: 95.00%] [G loss: 1.487118]\n",
      "601 [D loss: 0.336343, acc.: 96.00%] [G loss: 1.601104]\n",
      "602 [D loss: 0.367281, acc.: 91.50%] [G loss: 1.444652]\n",
      "603 [D loss: 0.333279, acc.: 93.50%] [G loss: 1.581373]\n",
      "604 [D loss: 0.356204, acc.: 93.00%] [G loss: 1.639716]\n",
      "605 [D loss: 0.368758, acc.: 94.00%] [G loss: 1.602764]\n",
      "606 [D loss: 0.358258, acc.: 93.00%] [G loss: 1.602444]\n",
      "607 [D loss: 0.360807, acc.: 94.50%] [G loss: 1.616379]\n",
      "608 [D loss: 0.359992, acc.: 94.00%] [G loss: 1.534371]\n",
      "609 [D loss: 0.335440, acc.: 93.50%] [G loss: 1.585453]\n",
      "610 [D loss: 0.347176, acc.: 96.00%] [G loss: 1.523840]\n",
      "611 [D loss: 0.330625, acc.: 94.50%] [G loss: 1.467071]\n",
      "612 [D loss: 0.353471, acc.: 94.00%] [G loss: 1.624903]\n",
      "613 [D loss: 0.354557, acc.: 95.50%] [G loss: 1.570907]\n",
      "614 [D loss: 0.372688, acc.: 93.00%] [G loss: 1.543495]\n",
      "615 [D loss: 0.356346, acc.: 93.50%] [G loss: 1.549751]\n",
      "616 [D loss: 0.333282, acc.: 94.50%] [G loss: 1.535317]\n",
      "617 [D loss: 0.344000, acc.: 96.00%] [G loss: 1.643081]\n",
      "618 [D loss: 0.343365, acc.: 96.00%] [G loss: 1.581772]\n",
      "619 [D loss: 0.366722, acc.: 92.00%] [G loss: 1.448926]\n",
      "620 [D loss: 0.325728, acc.: 96.00%] [G loss: 1.603100]\n",
      "621 [D loss: 0.351580, acc.: 94.00%] [G loss: 1.644323]\n",
      "622 [D loss: 0.341033, acc.: 95.00%] [G loss: 1.616478]\n",
      "623 [D loss: 0.327825, acc.: 97.00%] [G loss: 1.501050]\n",
      "624 [D loss: 0.332216, acc.: 96.00%] [G loss: 1.634491]\n",
      "625 [D loss: 0.345381, acc.: 94.00%] [G loss: 1.530882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626 [D loss: 0.371527, acc.: 92.50%] [G loss: 1.520867]\n",
      "627 [D loss: 0.354194, acc.: 92.00%] [G loss: 1.559188]\n",
      "628 [D loss: 0.345354, acc.: 94.50%] [G loss: 1.616777]\n",
      "629 [D loss: 0.335955, acc.: 95.50%] [G loss: 1.491235]\n",
      "630 [D loss: 0.345091, acc.: 94.00%] [G loss: 1.541338]\n",
      "631 [D loss: 0.346676, acc.: 94.50%] [G loss: 1.624848]\n",
      "632 [D loss: 0.341213, acc.: 97.50%] [G loss: 1.542271]\n",
      "633 [D loss: 0.308920, acc.: 97.00%] [G loss: 1.648833]\n",
      "634 [D loss: 0.331995, acc.: 95.00%] [G loss: 1.552272]\n",
      "635 [D loss: 0.330055, acc.: 97.00%] [G loss: 1.568515]\n",
      "636 [D loss: 0.331194, acc.: 96.00%] [G loss: 1.517568]\n",
      "637 [D loss: 0.328959, acc.: 95.00%] [G loss: 1.453885]\n",
      "638 [D loss: 0.332639, acc.: 95.00%] [G loss: 1.642295]\n",
      "639 [D loss: 0.351562, acc.: 94.00%] [G loss: 1.732192]\n",
      "640 [D loss: 0.318280, acc.: 96.50%] [G loss: 1.454768]\n",
      "641 [D loss: 0.337274, acc.: 96.00%] [G loss: 1.634834]\n",
      "642 [D loss: 0.340509, acc.: 95.50%] [G loss: 1.659031]\n",
      "643 [D loss: 0.346308, acc.: 94.50%] [G loss: 1.521322]\n",
      "644 [D loss: 0.327562, acc.: 95.50%] [G loss: 1.668073]\n",
      "645 [D loss: 0.338546, acc.: 95.00%] [G loss: 1.519587]\n",
      "646 [D loss: 0.353651, acc.: 92.50%] [G loss: 1.511800]\n",
      "647 [D loss: 0.322598, acc.: 97.00%] [G loss: 1.594385]\n",
      "648 [D loss: 0.327292, acc.: 96.50%] [G loss: 1.515701]\n",
      "649 [D loss: 0.356948, acc.: 95.00%] [G loss: 1.628515]\n",
      "650 [D loss: 0.326736, acc.: 95.50%] [G loss: 1.479787]\n",
      "651 [D loss: 0.328530, acc.: 98.00%] [G loss: 1.528580]\n",
      "652 [D loss: 0.328727, acc.: 96.50%] [G loss: 1.565289]\n",
      "653 [D loss: 0.332737, acc.: 96.50%] [G loss: 1.642269]\n",
      "654 [D loss: 0.341932, acc.: 96.00%] [G loss: 1.609841]\n",
      "655 [D loss: 0.327610, acc.: 96.00%] [G loss: 1.536492]\n",
      "656 [D loss: 0.316361, acc.: 97.00%] [G loss: 1.684039]\n",
      "657 [D loss: 0.337073, acc.: 95.00%] [G loss: 1.643406]\n",
      "658 [D loss: 0.312262, acc.: 94.50%] [G loss: 1.598513]\n",
      "659 [D loss: 0.349145, acc.: 94.50%] [G loss: 1.628441]\n",
      "660 [D loss: 0.304804, acc.: 97.00%] [G loss: 1.756965]\n",
      "661 [D loss: 0.333212, acc.: 97.50%] [G loss: 1.575067]\n",
      "662 [D loss: 0.332582, acc.: 97.50%] [G loss: 1.528530]\n",
      "663 [D loss: 0.306050, acc.: 97.50%] [G loss: 1.649420]\n",
      "664 [D loss: 0.329891, acc.: 95.50%] [G loss: 1.644422]\n",
      "665 [D loss: 0.327275, acc.: 95.00%] [G loss: 1.783765]\n",
      "666 [D loss: 0.363658, acc.: 94.00%] [G loss: 1.539553]\n",
      "667 [D loss: 0.313447, acc.: 97.00%] [G loss: 1.607294]\n",
      "668 [D loss: 0.336281, acc.: 95.00%] [G loss: 1.756657]\n",
      "669 [D loss: 0.324139, acc.: 93.50%] [G loss: 1.645570]\n",
      "670 [D loss: 0.319877, acc.: 96.00%] [G loss: 1.676241]\n",
      "671 [D loss: 0.339233, acc.: 94.50%] [G loss: 1.663775]\n",
      "672 [D loss: 0.335439, acc.: 95.00%] [G loss: 1.582725]\n",
      "673 [D loss: 0.331996, acc.: 94.00%] [G loss: 1.778188]\n",
      "674 [D loss: 0.325320, acc.: 93.50%] [G loss: 1.575552]\n",
      "675 [D loss: 0.330646, acc.: 94.50%] [G loss: 1.588422]\n",
      "676 [D loss: 0.338668, acc.: 95.00%] [G loss: 1.671293]\n",
      "677 [D loss: 0.318087, acc.: 97.50%] [G loss: 1.596304]\n",
      "678 [D loss: 0.339319, acc.: 96.00%] [G loss: 1.658370]\n",
      "679 [D loss: 0.317912, acc.: 95.00%] [G loss: 1.564054]\n",
      "680 [D loss: 0.313990, acc.: 96.50%] [G loss: 1.554387]\n",
      "681 [D loss: 0.337074, acc.: 96.50%] [G loss: 1.657166]\n",
      "682 [D loss: 0.344036, acc.: 94.50%] [G loss: 1.623974]\n",
      "683 [D loss: 0.317674, acc.: 96.00%] [G loss: 1.587413]\n",
      "684 [D loss: 0.328740, acc.: 96.00%] [G loss: 1.604881]\n",
      "685 [D loss: 0.313659, acc.: 97.00%] [G loss: 1.620903]\n",
      "686 [D loss: 0.308447, acc.: 95.00%] [G loss: 1.611584]\n",
      "687 [D loss: 0.316985, acc.: 96.50%] [G loss: 1.616341]\n",
      "688 [D loss: 0.319423, acc.: 97.00%] [G loss: 1.596004]\n",
      "689 [D loss: 0.350344, acc.: 94.50%] [G loss: 1.525887]\n",
      "690 [D loss: 0.338264, acc.: 95.50%] [G loss: 1.557003]\n",
      "691 [D loss: 0.327589, acc.: 96.50%] [G loss: 1.722641]\n",
      "692 [D loss: 0.326347, acc.: 95.00%] [G loss: 1.747688]\n",
      "693 [D loss: 0.323199, acc.: 95.50%] [G loss: 1.716919]\n",
      "694 [D loss: 0.344655, acc.: 94.00%] [G loss: 1.664194]\n",
      "695 [D loss: 0.341537, acc.: 95.00%] [G loss: 1.585207]\n",
      "696 [D loss: 0.339635, acc.: 94.50%] [G loss: 1.664999]\n",
      "697 [D loss: 0.312049, acc.: 95.50%] [G loss: 1.612285]\n",
      "698 [D loss: 0.316970, acc.: 95.50%] [G loss: 1.828032]\n",
      "699 [D loss: 0.326242, acc.: 94.50%] [G loss: 1.683954]\n",
      "700 [D loss: 0.329100, acc.: 95.00%] [G loss: 1.610509]\n",
      "701 [D loss: 0.350428, acc.: 93.50%] [G loss: 1.607495]\n",
      "702 [D loss: 0.316710, acc.: 96.00%] [G loss: 1.601545]\n",
      "703 [D loss: 0.306208, acc.: 97.00%] [G loss: 1.530716]\n",
      "704 [D loss: 0.309831, acc.: 96.00%] [G loss: 1.736347]\n",
      "705 [D loss: 0.301516, acc.: 97.50%] [G loss: 1.671735]\n",
      "706 [D loss: 0.302995, acc.: 97.00%] [G loss: 1.679168]\n",
      "707 [D loss: 0.343207, acc.: 93.00%] [G loss: 1.661653]\n",
      "708 [D loss: 0.330097, acc.: 95.50%] [G loss: 1.832632]\n",
      "709 [D loss: 0.327673, acc.: 95.00%] [G loss: 1.612636]\n",
      "710 [D loss: 0.321578, acc.: 94.50%] [G loss: 1.512158]\n",
      "711 [D loss: 0.292726, acc.: 98.00%] [G loss: 1.669522]\n",
      "712 [D loss: 0.325036, acc.: 96.00%] [G loss: 1.667013]\n",
      "713 [D loss: 0.304945, acc.: 95.00%] [G loss: 1.545591]\n",
      "714 [D loss: 0.329022, acc.: 96.00%] [G loss: 1.563937]\n",
      "715 [D loss: 0.328942, acc.: 96.50%] [G loss: 1.759314]\n",
      "716 [D loss: 0.320138, acc.: 95.00%] [G loss: 1.741881]\n",
      "717 [D loss: 0.326149, acc.: 95.00%] [G loss: 1.617345]\n",
      "718 [D loss: 0.306788, acc.: 97.00%] [G loss: 1.778345]\n",
      "719 [D loss: 0.316548, acc.: 97.00%] [G loss: 1.580415]\n",
      "720 [D loss: 0.298904, acc.: 97.50%] [G loss: 1.784564]\n",
      "721 [D loss: 0.311585, acc.: 97.00%] [G loss: 1.715188]\n",
      "722 [D loss: 0.311634, acc.: 97.00%] [G loss: 1.716041]\n",
      "723 [D loss: 0.305031, acc.: 97.00%] [G loss: 1.602815]\n",
      "724 [D loss: 0.316928, acc.: 95.50%] [G loss: 1.693520]\n",
      "725 [D loss: 0.338884, acc.: 96.00%] [G loss: 1.766048]\n",
      "726 [D loss: 0.335617, acc.: 94.00%] [G loss: 1.736406]\n",
      "727 [D loss: 0.315916, acc.: 95.00%] [G loss: 1.689297]\n",
      "728 [D loss: 0.302745, acc.: 97.50%] [G loss: 1.524441]\n",
      "729 [D loss: 0.306581, acc.: 97.50%] [G loss: 1.759930]\n",
      "730 [D loss: 0.306371, acc.: 97.00%] [G loss: 1.645060]\n",
      "731 [D loss: 0.305967, acc.: 97.50%] [G loss: 1.650772]\n",
      "732 [D loss: 0.336654, acc.: 95.50%] [G loss: 1.635552]\n",
      "733 [D loss: 0.340233, acc.: 93.00%] [G loss: 1.723531]\n",
      "734 [D loss: 0.309488, acc.: 97.00%] [G loss: 1.752034]\n",
      "735 [D loss: 0.337755, acc.: 94.50%] [G loss: 1.723392]\n",
      "736 [D loss: 0.306585, acc.: 97.00%] [G loss: 1.626027]\n",
      "737 [D loss: 0.319302, acc.: 95.00%] [G loss: 1.672270]\n",
      "738 [D loss: 0.312313, acc.: 94.50%] [G loss: 1.717992]\n",
      "739 [D loss: 0.308264, acc.: 97.00%] [G loss: 1.747012]\n",
      "740 [D loss: 0.286254, acc.: 98.00%] [G loss: 1.594706]\n",
      "741 [D loss: 0.326645, acc.: 95.00%] [G loss: 1.700419]\n",
      "742 [D loss: 0.300474, acc.: 95.50%] [G loss: 1.776673]\n",
      "743 [D loss: 0.291802, acc.: 96.00%] [G loss: 1.751091]\n",
      "744 [D loss: 0.338864, acc.: 95.00%] [G loss: 1.777534]\n",
      "745 [D loss: 0.321159, acc.: 96.00%] [G loss: 1.782464]\n",
      "746 [D loss: 0.312966, acc.: 96.00%] [G loss: 1.691981]\n",
      "747 [D loss: 0.333011, acc.: 95.50%] [G loss: 1.759400]\n",
      "748 [D loss: 0.302017, acc.: 97.00%] [G loss: 1.544581]\n",
      "749 [D loss: 0.313463, acc.: 95.50%] [G loss: 1.720103]\n",
      "750 [D loss: 0.303776, acc.: 95.00%] [G loss: 1.711594]\n",
      "751 [D loss: 0.307976, acc.: 96.50%] [G loss: 1.689514]\n",
      "752 [D loss: 0.340152, acc.: 93.50%] [G loss: 1.773196]\n",
      "753 [D loss: 0.332577, acc.: 96.00%] [G loss: 1.804386]\n",
      "754 [D loss: 0.278780, acc.: 98.00%] [G loss: 1.709326]\n",
      "755 [D loss: 0.310860, acc.: 95.00%] [G loss: 1.783026]\n",
      "756 [D loss: 0.303077, acc.: 97.00%] [G loss: 1.617473]\n",
      "757 [D loss: 0.317735, acc.: 97.00%] [G loss: 1.687695]\n",
      "758 [D loss: 0.313194, acc.: 96.00%] [G loss: 1.692320]\n",
      "759 [D loss: 0.286798, acc.: 96.50%] [G loss: 1.681792]\n",
      "760 [D loss: 0.322278, acc.: 95.50%] [G loss: 1.726248]\n",
      "761 [D loss: 0.312169, acc.: 96.00%] [G loss: 1.685302]\n",
      "762 [D loss: 0.323588, acc.: 95.50%] [G loss: 1.712055]\n",
      "763 [D loss: 0.319955, acc.: 96.00%] [G loss: 1.841649]\n",
      "764 [D loss: 0.310849, acc.: 96.50%] [G loss: 1.858611]\n",
      "765 [D loss: 0.320495, acc.: 95.00%] [G loss: 1.755190]\n",
      "766 [D loss: 0.296751, acc.: 98.00%] [G loss: 1.845681]\n",
      "767 [D loss: 0.308038, acc.: 97.00%] [G loss: 1.751527]\n",
      "768 [D loss: 0.312062, acc.: 97.50%] [G loss: 1.606439]\n",
      "769 [D loss: 0.292195, acc.: 98.00%] [G loss: 1.757947]\n",
      "770 [D loss: 0.294992, acc.: 98.00%] [G loss: 1.819904]\n",
      "771 [D loss: 0.317731, acc.: 96.50%] [G loss: 1.873736]\n",
      "772 [D loss: 0.317548, acc.: 96.00%] [G loss: 1.797876]\n",
      "773 [D loss: 0.302244, acc.: 95.50%] [G loss: 1.788243]\n",
      "774 [D loss: 0.308726, acc.: 95.50%] [G loss: 1.749278]\n",
      "775 [D loss: 0.283309, acc.: 97.00%] [G loss: 1.749135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776 [D loss: 0.313524, acc.: 94.00%] [G loss: 1.678859]\n",
      "777 [D loss: 0.316440, acc.: 96.00%] [G loss: 1.819476]\n",
      "778 [D loss: 0.299952, acc.: 95.50%] [G loss: 1.669501]\n",
      "779 [D loss: 0.296311, acc.: 95.50%] [G loss: 1.848910]\n",
      "780 [D loss: 0.298529, acc.: 97.00%] [G loss: 1.818185]\n",
      "781 [D loss: 0.294680, acc.: 95.50%] [G loss: 1.922544]\n",
      "782 [D loss: 0.316153, acc.: 94.50%] [G loss: 1.687421]\n",
      "783 [D loss: 0.301421, acc.: 96.00%] [G loss: 1.767333]\n",
      "784 [D loss: 0.301846, acc.: 95.50%] [G loss: 1.675862]\n",
      "785 [D loss: 0.304968, acc.: 95.50%] [G loss: 1.664461]\n",
      "786 [D loss: 0.315857, acc.: 96.50%] [G loss: 1.774170]\n",
      "787 [D loss: 0.285105, acc.: 98.50%] [G loss: 1.812345]\n",
      "788 [D loss: 0.300172, acc.: 97.50%] [G loss: 1.736761]\n",
      "789 [D loss: 0.300893, acc.: 96.00%] [G loss: 1.670276]\n",
      "790 [D loss: 0.284032, acc.: 96.50%] [G loss: 1.709967]\n",
      "791 [D loss: 0.309904, acc.: 98.00%] [G loss: 1.801261]\n",
      "792 [D loss: 0.279286, acc.: 100.00%] [G loss: 1.778568]\n",
      "793 [D loss: 0.308928, acc.: 95.00%] [G loss: 1.852569]\n",
      "794 [D loss: 0.296274, acc.: 96.00%] [G loss: 1.616865]\n",
      "795 [D loss: 0.295243, acc.: 97.00%] [G loss: 1.817440]\n",
      "796 [D loss: 0.304156, acc.: 96.00%] [G loss: 1.731387]\n",
      "797 [D loss: 0.303032, acc.: 96.00%] [G loss: 1.940874]\n",
      "798 [D loss: 0.317506, acc.: 96.50%] [G loss: 1.825897]\n",
      "799 [D loss: 0.305777, acc.: 95.50%] [G loss: 1.871253]\n",
      "800 [D loss: 0.313936, acc.: 94.50%] [G loss: 1.720588]\n",
      "801 [D loss: 0.284533, acc.: 98.00%] [G loss: 1.792092]\n",
      "802 [D loss: 0.296576, acc.: 97.00%] [G loss: 1.763788]\n",
      "803 [D loss: 0.283771, acc.: 98.00%] [G loss: 1.748684]\n",
      "804 [D loss: 0.295331, acc.: 96.00%] [G loss: 1.678710]\n",
      "805 [D loss: 0.301506, acc.: 97.50%] [G loss: 1.724252]\n",
      "806 [D loss: 0.290131, acc.: 97.50%] [G loss: 1.922306]\n",
      "807 [D loss: 0.293917, acc.: 97.50%] [G loss: 1.793620]\n",
      "808 [D loss: 0.291936, acc.: 95.00%] [G loss: 1.867691]\n",
      "809 [D loss: 0.327052, acc.: 93.50%] [G loss: 1.763729]\n",
      "810 [D loss: 0.283700, acc.: 96.00%] [G loss: 1.779183]\n",
      "811 [D loss: 0.283869, acc.: 98.50%] [G loss: 1.745003]\n",
      "812 [D loss: 0.331306, acc.: 96.50%] [G loss: 1.815454]\n",
      "813 [D loss: 0.309547, acc.: 95.00%] [G loss: 1.844675]\n",
      "814 [D loss: 0.286304, acc.: 96.50%] [G loss: 1.867623]\n",
      "815 [D loss: 0.295506, acc.: 97.50%] [G loss: 1.774073]\n",
      "816 [D loss: 0.298223, acc.: 97.00%] [G loss: 1.801739]\n",
      "817 [D loss: 0.316598, acc.: 93.50%] [G loss: 1.765679]\n",
      "818 [D loss: 0.295625, acc.: 97.50%] [G loss: 1.846425]\n",
      "819 [D loss: 0.286541, acc.: 99.00%] [G loss: 1.825940]\n",
      "820 [D loss: 0.316484, acc.: 96.00%] [G loss: 1.789004]\n",
      "821 [D loss: 0.298950, acc.: 97.00%] [G loss: 1.859311]\n",
      "822 [D loss: 0.303578, acc.: 97.00%] [G loss: 1.897043]\n",
      "823 [D loss: 0.334656, acc.: 93.50%] [G loss: 1.781463]\n",
      "824 [D loss: 0.292554, acc.: 96.00%] [G loss: 1.742464]\n",
      "825 [D loss: 0.319551, acc.: 97.00%] [G loss: 1.824833]\n",
      "826 [D loss: 0.278295, acc.: 98.00%] [G loss: 1.801259]\n",
      "827 [D loss: 0.278942, acc.: 96.50%] [G loss: 1.683180]\n",
      "828 [D loss: 0.305007, acc.: 95.00%] [G loss: 1.732391]\n",
      "829 [D loss: 0.301165, acc.: 94.50%] [G loss: 1.955092]\n",
      "830 [D loss: 0.315016, acc.: 95.00%] [G loss: 2.000971]\n",
      "831 [D loss: 0.298865, acc.: 95.50%] [G loss: 1.762016]\n",
      "832 [D loss: 0.286529, acc.: 98.50%] [G loss: 1.733215]\n",
      "833 [D loss: 0.280016, acc.: 97.00%] [G loss: 1.827754]\n",
      "834 [D loss: 0.296890, acc.: 94.00%] [G loss: 1.794431]\n",
      "835 [D loss: 0.300321, acc.: 97.00%] [G loss: 1.992120]\n",
      "836 [D loss: 0.299200, acc.: 96.00%] [G loss: 1.925942]\n",
      "837 [D loss: 0.280050, acc.: 98.00%] [G loss: 1.891203]\n",
      "838 [D loss: 0.274961, acc.: 97.50%] [G loss: 1.839419]\n",
      "839 [D loss: 0.287310, acc.: 98.00%] [G loss: 1.651465]\n",
      "840 [D loss: 0.272645, acc.: 98.50%] [G loss: 1.820381]\n",
      "841 [D loss: 0.307511, acc.: 97.00%] [G loss: 1.814762]\n",
      "842 [D loss: 0.298447, acc.: 96.50%] [G loss: 1.821831]\n",
      "843 [D loss: 0.287133, acc.: 98.00%] [G loss: 1.789649]\n",
      "844 [D loss: 0.301382, acc.: 97.00%] [G loss: 1.773667]\n",
      "845 [D loss: 0.291887, acc.: 94.50%] [G loss: 1.843946]\n",
      "846 [D loss: 0.299995, acc.: 96.50%] [G loss: 1.860578]\n",
      "847 [D loss: 0.269573, acc.: 98.00%] [G loss: 1.791508]\n",
      "848 [D loss: 0.297055, acc.: 96.50%] [G loss: 1.946627]\n",
      "849 [D loss: 0.271620, acc.: 97.00%] [G loss: 1.720474]\n",
      "850 [D loss: 0.284929, acc.: 98.50%] [G loss: 1.886552]\n",
      "851 [D loss: 0.305999, acc.: 96.00%] [G loss: 1.743280]\n",
      "852 [D loss: 0.285111, acc.: 98.00%] [G loss: 1.929150]\n",
      "853 [D loss: 0.276127, acc.: 98.50%] [G loss: 1.912492]\n",
      "854 [D loss: 0.286020, acc.: 98.00%] [G loss: 1.927855]\n",
      "855 [D loss: 0.276801, acc.: 97.50%] [G loss: 1.910172]\n",
      "856 [D loss: 0.286226, acc.: 99.00%] [G loss: 1.734340]\n",
      "857 [D loss: 0.302165, acc.: 96.00%] [G loss: 1.952849]\n",
      "858 [D loss: 0.276722, acc.: 97.50%] [G loss: 1.843048]\n",
      "859 [D loss: 0.289598, acc.: 97.50%] [G loss: 1.838201]\n",
      "860 [D loss: 0.275270, acc.: 98.00%] [G loss: 1.779541]\n",
      "861 [D loss: 0.276389, acc.: 98.00%] [G loss: 1.771334]\n",
      "862 [D loss: 0.309579, acc.: 95.50%] [G loss: 1.922881]\n",
      "863 [D loss: 0.278843, acc.: 97.00%] [G loss: 1.919429]\n",
      "864 [D loss: 0.289099, acc.: 97.50%] [G loss: 1.878547]\n",
      "865 [D loss: 0.306395, acc.: 94.50%] [G loss: 1.901418]\n",
      "866 [D loss: 0.300370, acc.: 96.50%] [G loss: 1.803512]\n",
      "867 [D loss: 0.287715, acc.: 98.00%] [G loss: 2.019439]\n",
      "868 [D loss: 0.283412, acc.: 96.50%] [G loss: 1.874940]\n",
      "869 [D loss: 0.295968, acc.: 97.50%] [G loss: 1.820916]\n",
      "870 [D loss: 0.267858, acc.: 99.00%] [G loss: 1.723696]\n",
      "871 [D loss: 0.314080, acc.: 94.50%] [G loss: 1.745213]\n",
      "872 [D loss: 0.269170, acc.: 97.00%] [G loss: 1.912139]\n",
      "873 [D loss: 0.272541, acc.: 98.00%] [G loss: 1.954028]\n",
      "874 [D loss: 0.285829, acc.: 96.50%] [G loss: 1.943797]\n",
      "875 [D loss: 0.279852, acc.: 98.00%] [G loss: 1.883049]\n",
      "876 [D loss: 0.291882, acc.: 95.00%] [G loss: 1.839810]\n",
      "877 [D loss: 0.285258, acc.: 99.00%] [G loss: 1.855462]\n",
      "878 [D loss: 0.287099, acc.: 97.50%] [G loss: 1.876486]\n",
      "879 [D loss: 0.288239, acc.: 96.50%] [G loss: 1.925636]\n",
      "880 [D loss: 0.299633, acc.: 96.50%] [G loss: 1.837429]\n",
      "881 [D loss: 0.266745, acc.: 98.00%] [G loss: 1.867905]\n",
      "882 [D loss: 0.283689, acc.: 96.50%] [G loss: 1.819202]\n",
      "883 [D loss: 0.282152, acc.: 97.00%] [G loss: 1.813862]\n",
      "884 [D loss: 0.256750, acc.: 99.00%] [G loss: 1.911891]\n",
      "885 [D loss: 0.280603, acc.: 97.00%] [G loss: 2.011806]\n",
      "886 [D loss: 0.286362, acc.: 96.00%] [G loss: 1.900938]\n",
      "887 [D loss: 0.301913, acc.: 97.00%] [G loss: 1.892024]\n",
      "888 [D loss: 0.298906, acc.: 98.00%] [G loss: 1.870679]\n",
      "889 [D loss: 0.300594, acc.: 96.00%] [G loss: 1.838883]\n",
      "890 [D loss: 0.277230, acc.: 96.50%] [G loss: 1.880877]\n",
      "891 [D loss: 0.294943, acc.: 96.00%] [G loss: 1.982577]\n",
      "892 [D loss: 0.267617, acc.: 98.00%] [G loss: 2.005383]\n",
      "893 [D loss: 0.276785, acc.: 96.50%] [G loss: 2.053894]\n",
      "894 [D loss: 0.289026, acc.: 95.50%] [G loss: 1.831438]\n",
      "895 [D loss: 0.271162, acc.: 99.00%] [G loss: 1.886321]\n",
      "896 [D loss: 0.279378, acc.: 96.00%] [G loss: 1.952276]\n",
      "897 [D loss: 0.307253, acc.: 94.50%] [G loss: 1.869262]\n",
      "898 [D loss: 0.275908, acc.: 98.50%] [G loss: 1.843357]\n",
      "899 [D loss: 0.291220, acc.: 95.50%] [G loss: 1.932303]\n",
      "900 [D loss: 0.278040, acc.: 97.50%] [G loss: 1.740073]\n",
      "901 [D loss: 0.301010, acc.: 96.50%] [G loss: 1.903082]\n",
      "902 [D loss: 0.264722, acc.: 99.00%] [G loss: 1.980601]\n",
      "903 [D loss: 0.274531, acc.: 98.00%] [G loss: 1.889010]\n",
      "904 [D loss: 0.283989, acc.: 97.50%] [G loss: 1.884533]\n",
      "905 [D loss: 0.271930, acc.: 98.50%] [G loss: 1.808369]\n",
      "906 [D loss: 0.255391, acc.: 99.00%] [G loss: 1.845525]\n",
      "907 [D loss: 0.271059, acc.: 99.00%] [G loss: 1.993544]\n",
      "908 [D loss: 0.251664, acc.: 99.00%] [G loss: 1.843503]\n",
      "909 [D loss: 0.253106, acc.: 98.50%] [G loss: 1.788956]\n",
      "910 [D loss: 0.282996, acc.: 96.50%] [G loss: 1.857943]\n",
      "911 [D loss: 0.285546, acc.: 96.50%] [G loss: 1.879080]\n",
      "912 [D loss: 0.273405, acc.: 97.00%] [G loss: 1.878160]\n",
      "913 [D loss: 0.276462, acc.: 97.50%] [G loss: 1.971950]\n",
      "914 [D loss: 0.266954, acc.: 98.00%] [G loss: 1.914562]\n",
      "915 [D loss: 0.267037, acc.: 97.00%] [G loss: 1.860817]\n",
      "916 [D loss: 0.283946, acc.: 96.00%] [G loss: 2.027749]\n",
      "917 [D loss: 0.260065, acc.: 99.00%] [G loss: 1.992014]\n",
      "918 [D loss: 0.267196, acc.: 98.50%] [G loss: 1.808852]\n",
      "919 [D loss: 0.291927, acc.: 96.50%] [G loss: 1.845289]\n",
      "920 [D loss: 0.263864, acc.: 98.50%] [G loss: 1.826724]\n",
      "921 [D loss: 0.276087, acc.: 97.50%] [G loss: 1.843353]\n",
      "922 [D loss: 0.282328, acc.: 98.00%] [G loss: 1.939199]\n",
      "923 [D loss: 0.290213, acc.: 97.50%] [G loss: 1.927512]\n",
      "924 [D loss: 0.264654, acc.: 99.50%] [G loss: 1.893380]\n",
      "925 [D loss: 0.287291, acc.: 96.50%] [G loss: 1.925399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926 [D loss: 0.278107, acc.: 98.00%] [G loss: 1.815347]\n",
      "927 [D loss: 0.288669, acc.: 97.50%] [G loss: 1.978900]\n",
      "928 [D loss: 0.291803, acc.: 97.00%] [G loss: 1.884210]\n",
      "929 [D loss: 0.274264, acc.: 98.00%] [G loss: 1.900158]\n",
      "930 [D loss: 0.274342, acc.: 98.50%] [G loss: 2.009720]\n",
      "931 [D loss: 0.274824, acc.: 98.00%] [G loss: 1.908729]\n",
      "932 [D loss: 0.285546, acc.: 97.50%] [G loss: 2.006296]\n",
      "933 [D loss: 0.263477, acc.: 99.00%] [G loss: 2.008579]\n",
      "934 [D loss: 0.286687, acc.: 95.50%] [G loss: 2.002862]\n",
      "935 [D loss: 0.286750, acc.: 98.00%] [G loss: 1.915098]\n",
      "936 [D loss: 0.293218, acc.: 96.50%] [G loss: 1.935696]\n",
      "937 [D loss: 0.262511, acc.: 98.50%] [G loss: 1.959272]\n",
      "938 [D loss: 0.301820, acc.: 98.00%] [G loss: 1.940667]\n",
      "939 [D loss: 0.272114, acc.: 98.50%] [G loss: 1.999994]\n",
      "940 [D loss: 0.262743, acc.: 98.50%] [G loss: 1.914833]\n",
      "941 [D loss: 0.274169, acc.: 99.00%] [G loss: 1.781168]\n",
      "942 [D loss: 0.258038, acc.: 98.00%] [G loss: 2.017285]\n",
      "943 [D loss: 0.267941, acc.: 98.50%] [G loss: 2.060219]\n",
      "944 [D loss: 0.272321, acc.: 98.50%] [G loss: 1.957407]\n",
      "945 [D loss: 0.270599, acc.: 99.00%] [G loss: 1.928507]\n",
      "946 [D loss: 0.263628, acc.: 98.00%] [G loss: 1.949086]\n",
      "947 [D loss: 0.272119, acc.: 97.50%] [G loss: 1.971874]\n",
      "948 [D loss: 0.267550, acc.: 98.00%] [G loss: 1.895996]\n",
      "949 [D loss: 0.282033, acc.: 97.00%] [G loss: 1.892510]\n",
      "950 [D loss: 0.273153, acc.: 97.00%] [G loss: 1.894810]\n",
      "951 [D loss: 0.264208, acc.: 98.50%] [G loss: 1.958756]\n",
      "952 [D loss: 0.279900, acc.: 98.50%] [G loss: 1.862618]\n",
      "953 [D loss: 0.280013, acc.: 97.50%] [G loss: 2.063673]\n",
      "954 [D loss: 0.274916, acc.: 98.00%] [G loss: 1.948538]\n",
      "955 [D loss: 0.272607, acc.: 96.50%] [G loss: 2.002426]\n",
      "956 [D loss: 0.271800, acc.: 98.50%] [G loss: 2.021469]\n",
      "957 [D loss: 0.274289, acc.: 96.50%] [G loss: 2.006867]\n",
      "958 [D loss: 0.293363, acc.: 96.00%] [G loss: 1.818725]\n",
      "959 [D loss: 0.248927, acc.: 97.50%] [G loss: 1.880648]\n",
      "960 [D loss: 0.286359, acc.: 98.50%] [G loss: 1.868483]\n",
      "961 [D loss: 0.263066, acc.: 98.50%] [G loss: 1.870702]\n",
      "962 [D loss: 0.263682, acc.: 97.50%] [G loss: 1.929103]\n",
      "963 [D loss: 0.266610, acc.: 98.50%] [G loss: 1.953719]\n",
      "964 [D loss: 0.247404, acc.: 99.00%] [G loss: 1.881156]\n",
      "965 [D loss: 0.264029, acc.: 98.00%] [G loss: 2.110092]\n",
      "966 [D loss: 0.262683, acc.: 98.50%] [G loss: 2.058327]\n",
      "967 [D loss: 0.265051, acc.: 98.50%] [G loss: 1.937983]\n",
      "968 [D loss: 0.264073, acc.: 98.50%] [G loss: 2.073583]\n",
      "969 [D loss: 0.281658, acc.: 96.50%] [G loss: 2.108098]\n",
      "970 [D loss: 0.250062, acc.: 98.50%] [G loss: 2.007742]\n",
      "971 [D loss: 0.270815, acc.: 97.50%] [G loss: 2.050780]\n",
      "972 [D loss: 0.265228, acc.: 97.50%] [G loss: 1.949073]\n",
      "973 [D loss: 0.259175, acc.: 100.00%] [G loss: 1.887855]\n",
      "974 [D loss: 0.274244, acc.: 99.00%] [G loss: 1.849554]\n",
      "975 [D loss: 0.274683, acc.: 96.00%] [G loss: 1.963455]\n",
      "976 [D loss: 0.237950, acc.: 98.00%] [G loss: 1.958580]\n",
      "977 [D loss: 0.255232, acc.: 99.00%] [G loss: 2.115258]\n",
      "978 [D loss: 0.264964, acc.: 98.50%] [G loss: 2.086267]\n",
      "979 [D loss: 0.260996, acc.: 97.50%] [G loss: 1.968038]\n",
      "980 [D loss: 0.261658, acc.: 99.00%] [G loss: 1.919443]\n",
      "981 [D loss: 0.258035, acc.: 98.50%] [G loss: 1.957530]\n",
      "982 [D loss: 0.245609, acc.: 99.00%] [G loss: 1.929780]\n",
      "983 [D loss: 0.264677, acc.: 99.50%] [G loss: 2.036240]\n",
      "984 [D loss: 0.250090, acc.: 97.50%] [G loss: 1.843014]\n",
      "985 [D loss: 0.250816, acc.: 98.00%] [G loss: 1.992349]\n",
      "986 [D loss: 0.264504, acc.: 96.50%] [G loss: 2.055186]\n",
      "987 [D loss: 0.262037, acc.: 97.50%] [G loss: 1.782281]\n",
      "988 [D loss: 0.274687, acc.: 97.00%] [G loss: 2.164891]\n",
      "989 [D loss: 0.263867, acc.: 99.00%] [G loss: 2.058046]\n",
      "990 [D loss: 0.245409, acc.: 99.50%] [G loss: 1.959676]\n",
      "991 [D loss: 0.297978, acc.: 97.00%] [G loss: 2.008748]\n",
      "992 [D loss: 0.254595, acc.: 98.00%] [G loss: 1.922878]\n",
      "993 [D loss: 0.250224, acc.: 100.00%] [G loss: 2.001624]\n",
      "994 [D loss: 0.239561, acc.: 99.00%] [G loss: 2.045769]\n",
      "995 [D loss: 0.269181, acc.: 98.00%] [G loss: 1.963956]\n",
      "996 [D loss: 0.256851, acc.: 98.50%] [G loss: 1.922010]\n",
      "997 [D loss: 0.260560, acc.: 98.50%] [G loss: 1.795319]\n",
      "998 [D loss: 0.256001, acc.: 98.50%] [G loss: 1.986807]\n",
      "999 [D loss: 0.267285, acc.: 99.00%] [G loss: 1.983507]\n",
      "1000 [D loss: 0.243917, acc.: 99.00%] [G loss: 2.074859]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN()\n",
    "dcgan.train(epochs=1001, batch_size=100, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
