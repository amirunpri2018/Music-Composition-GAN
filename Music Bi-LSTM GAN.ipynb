{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "        x_train = np.load(r'C:\\Users\\Vee\\Desktop\\python\\GAN\\Music GAN\\songs.npy',allow_pickle=True)\n",
    "        x_train = x_train.reshape(len(x_train),4,4)\n",
    "        return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 4\n",
    "        self.img_cols = 4\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 16\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(4,4))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(160, return_sequences=True), input_shape=(4, 4)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(160, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(160)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(16))\n",
    "        #specifying 1 feature as the output\n",
    "        model.add(Bidirectional(LSTM(160, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(160, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(160, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(160)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(160)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(4,4))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Bidirectional(LSTM(160, return_sequences=True), input_shape=(16, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(160)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(TimeDistributed(Dense(160)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(160)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=(16,1))\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),16,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size,4,4))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.generator.save(\"LSTM_generator1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 16, 320)           207360    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 320)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 320)               615680    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 320)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 160)            51360     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 160)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 160)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 160)            25760     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 160)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 160)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 1)              161       \n",
      "=================================================================\n",
      "Total params: 900,321\n",
      "Trainable params: 900,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 4, 320)            211200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 4, 320)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 4, 320)            615680    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4, 320)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 320)               615680    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 16, 320)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 16, 320)           615680    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 320)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 16, 320)           615680    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 16, 320)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 16, 320)           615680    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 16, 320)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 320)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 16, 160)           51360     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 16, 160)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 160)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 16, 160)           25760     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 16, 160)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 160)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 16, 1)             161       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 16, 1)             0         \n",
      "=================================================================\n",
      "Total params: 3,366,881\n",
      "Trainable params: 3,366,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 7.350955, acc.: 50.00%] [G loss: 6.289051]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 3.129720, acc.: 50.00%] [G loss: 5.826455]\n",
      "2 [D loss: 2.928718, acc.: 50.00%] [G loss: 5.594081]\n",
      "3 [D loss: 2.825413, acc.: 50.00%] [G loss: 5.406894]\n",
      "4 [D loss: 2.731702, acc.: 50.00%] [G loss: 5.294282]\n",
      "5 [D loss: 2.655751, acc.: 50.00%] [G loss: 5.144709]\n",
      "6 [D loss: 2.597709, acc.: 50.00%] [G loss: 5.103446]\n",
      "7 [D loss: 2.553933, acc.: 50.00%] [G loss: 4.971944]\n",
      "8 [D loss: 2.477453, acc.: 50.00%] [G loss: 4.849258]\n",
      "9 [D loss: 2.405779, acc.: 50.00%] [G loss: 4.753980]\n",
      "10 [D loss: 2.367964, acc.: 50.00%] [G loss: 4.682136]\n",
      "11 [D loss: 2.347654, acc.: 50.00%] [G loss: 4.627163]\n",
      "12 [D loss: 2.309519, acc.: 50.00%] [G loss: 4.481177]\n",
      "13 [D loss: 2.272539, acc.: 50.00%] [G loss: 4.441154]\n",
      "14 [D loss: 2.249113, acc.: 50.00%] [G loss: 4.394472]\n",
      "15 [D loss: 2.180459, acc.: 50.00%] [G loss: 4.335694]\n",
      "16 [D loss: 2.141166, acc.: 50.00%] [G loss: 4.205614]\n",
      "17 [D loss: 2.120231, acc.: 50.00%] [G loss: 4.174664]\n",
      "18 [D loss: 2.084530, acc.: 50.00%] [G loss: 4.055471]\n",
      "19 [D loss: 2.062155, acc.: 50.00%] [G loss: 4.013101]\n",
      "20 [D loss: 2.016353, acc.: 50.00%] [G loss: 3.983254]\n",
      "21 [D loss: 2.001565, acc.: 50.00%] [G loss: 3.888276]\n",
      "22 [D loss: 1.958513, acc.: 50.00%] [G loss: 3.711612]\n",
      "23 [D loss: 1.907787, acc.: 50.00%] [G loss: 3.705261]\n",
      "24 [D loss: 1.863317, acc.: 50.00%] [G loss: 3.630821]\n",
      "25 [D loss: 1.799610, acc.: 50.00%] [G loss: 3.488740]\n",
      "26 [D loss: 1.764347, acc.: 50.00%] [G loss: 3.397844]\n",
      "27 [D loss: 1.719065, acc.: 50.00%] [G loss: 3.358603]\n",
      "28 [D loss: 1.672307, acc.: 50.00%] [G loss: 3.234288]\n",
      "29 [D loss: 1.610186, acc.: 50.00%] [G loss: 3.118359]\n",
      "30 [D loss: 1.568200, acc.: 50.00%] [G loss: 3.012728]\n",
      "31 [D loss: 1.538907, acc.: 50.00%] [G loss: 2.918706]\n",
      "32 [D loss: 1.482592, acc.: 50.00%] [G loss: 2.793933]\n",
      "33 [D loss: 1.437852, acc.: 50.00%] [G loss: 2.675149]\n",
      "34 [D loss: 1.378882, acc.: 50.00%] [G loss: 2.574826]\n",
      "35 [D loss: 1.305573, acc.: 50.00%] [G loss: 2.387484]\n",
      "36 [D loss: 1.252710, acc.: 50.00%] [G loss: 2.234017]\n",
      "37 [D loss: 1.215030, acc.: 50.00%] [G loss: 2.185633]\n",
      "38 [D loss: 1.171623, acc.: 50.00%] [G loss: 2.003948]\n",
      "39 [D loss: 1.157098, acc.: 50.00%] [G loss: 1.874193]\n",
      "40 [D loss: 1.062553, acc.: 50.00%] [G loss: 1.886788]\n",
      "41 [D loss: 0.982916, acc.: 50.00%] [G loss: 3.078240]\n",
      "42 [D loss: 0.872682, acc.: 50.00%] [G loss: 3.091050]\n",
      "43 [D loss: 0.842741, acc.: 50.00%] [G loss: 2.112411]\n",
      "44 [D loss: 0.871661, acc.: 50.00%] [G loss: 1.483150]\n",
      "45 [D loss: 0.813520, acc.: 48.75%] [G loss: 1.253786]\n",
      "46 [D loss: 0.830663, acc.: 48.75%] [G loss: 1.037497]\n",
      "47 [D loss: 0.797647, acc.: 47.50%] [G loss: 0.914312]\n",
      "48 [D loss: 0.790453, acc.: 40.00%] [G loss: 0.921911]\n",
      "49 [D loss: 0.784298, acc.: 42.50%] [G loss: 0.927088]\n",
      "50 [D loss: 0.812343, acc.: 35.00%] [G loss: 0.917544]\n",
      "51 [D loss: 0.782894, acc.: 42.50%] [G loss: 0.922779]\n",
      "52 [D loss: 0.751483, acc.: 42.50%] [G loss: 0.765469]\n",
      "53 [D loss: 0.777879, acc.: 37.50%] [G loss: 0.838366]\n",
      "54 [D loss: 0.800409, acc.: 41.25%] [G loss: 0.800113]\n",
      "55 [D loss: 0.791869, acc.: 37.50%] [G loss: 0.842418]\n",
      "56 [D loss: 0.710582, acc.: 52.50%] [G loss: 0.769543]\n",
      "57 [D loss: 0.790511, acc.: 36.25%] [G loss: 0.810097]\n",
      "58 [D loss: 0.785815, acc.: 35.00%] [G loss: 0.837336]\n",
      "59 [D loss: 0.778269, acc.: 37.50%] [G loss: 0.871346]\n",
      "60 [D loss: 0.755302, acc.: 46.25%] [G loss: 0.850879]\n",
      "61 [D loss: 0.753412, acc.: 46.25%] [G loss: 0.809393]\n",
      "62 [D loss: 0.778099, acc.: 41.25%] [G loss: 0.824087]\n",
      "63 [D loss: 0.758374, acc.: 40.00%] [G loss: 0.825081]\n",
      "64 [D loss: 0.738458, acc.: 42.50%] [G loss: 0.833611]\n",
      "65 [D loss: 0.726585, acc.: 45.00%] [G loss: 0.910694]\n",
      "66 [D loss: 0.754201, acc.: 37.50%] [G loss: 0.861397]\n",
      "67 [D loss: 0.735982, acc.: 50.00%] [G loss: 0.805818]\n",
      "68 [D loss: 0.740563, acc.: 43.75%] [G loss: 0.871060]\n",
      "69 [D loss: 0.687054, acc.: 51.25%] [G loss: 0.815750]\n",
      "70 [D loss: 0.760261, acc.: 35.00%] [G loss: 0.826441]\n",
      "71 [D loss: 0.695445, acc.: 52.50%] [G loss: 0.824588]\n",
      "72 [D loss: 0.710755, acc.: 48.75%] [G loss: 0.843235]\n",
      "73 [D loss: 0.721919, acc.: 52.50%] [G loss: 0.788285]\n",
      "74 [D loss: 0.753668, acc.: 43.75%] [G loss: 0.861257]\n",
      "75 [D loss: 0.718292, acc.: 48.75%] [G loss: 0.815998]\n",
      "76 [D loss: 0.722784, acc.: 51.25%] [G loss: 0.758724]\n",
      "77 [D loss: 0.711361, acc.: 51.25%] [G loss: 0.802306]\n",
      "78 [D loss: 0.779745, acc.: 33.75%] [G loss: 0.784269]\n",
      "79 [D loss: 0.751875, acc.: 40.00%] [G loss: 0.787004]\n",
      "80 [D loss: 0.726069, acc.: 51.25%] [G loss: 0.809397]\n",
      "81 [D loss: 0.744839, acc.: 38.75%] [G loss: 0.836909]\n",
      "82 [D loss: 0.712051, acc.: 51.25%] [G loss: 0.779775]\n",
      "83 [D loss: 0.738808, acc.: 42.50%] [G loss: 0.837127]\n",
      "84 [D loss: 0.720655, acc.: 48.75%] [G loss: 0.902342]\n",
      "85 [D loss: 0.710045, acc.: 48.75%] [G loss: 0.821857]\n",
      "86 [D loss: 0.752937, acc.: 42.50%] [G loss: 0.865383]\n",
      "87 [D loss: 0.726599, acc.: 50.00%] [G loss: 0.787811]\n",
      "88 [D loss: 0.760251, acc.: 46.25%] [G loss: 0.762405]\n",
      "89 [D loss: 0.732359, acc.: 46.25%] [G loss: 0.748171]\n",
      "90 [D loss: 0.768602, acc.: 37.50%] [G loss: 0.759430]\n",
      "91 [D loss: 0.742083, acc.: 46.25%] [G loss: 0.719239]\n",
      "92 [D loss: 0.785282, acc.: 33.75%] [G loss: 0.759864]\n",
      "93 [D loss: 0.721193, acc.: 42.50%] [G loss: 0.805766]\n",
      "94 [D loss: 0.751415, acc.: 46.25%] [G loss: 0.882946]\n",
      "95 [D loss: 0.771566, acc.: 38.75%] [G loss: 0.856225]\n",
      "96 [D loss: 0.659107, acc.: 58.75%] [G loss: 0.863490]\n",
      "97 [D loss: 0.683290, acc.: 56.25%] [G loss: 0.863800]\n",
      "98 [D loss: 0.692470, acc.: 52.50%] [G loss: 0.884925]\n",
      "99 [D loss: 0.741557, acc.: 45.00%] [G loss: 0.872722]\n",
      "100 [D loss: 0.658519, acc.: 65.00%] [G loss: 0.868853]\n",
      "101 [D loss: 0.688276, acc.: 43.75%] [G loss: 0.835377]\n",
      "102 [D loss: 0.736538, acc.: 37.50%] [G loss: 0.812478]\n",
      "103 [D loss: 0.743711, acc.: 41.25%] [G loss: 0.845414]\n",
      "104 [D loss: 0.745428, acc.: 42.50%] [G loss: 0.740787]\n",
      "105 [D loss: 0.717729, acc.: 48.75%] [G loss: 0.780396]\n",
      "106 [D loss: 0.744575, acc.: 48.75%] [G loss: 0.783695]\n",
      "107 [D loss: 0.719056, acc.: 47.50%] [G loss: 0.746281]\n",
      "108 [D loss: 0.732205, acc.: 50.00%] [G loss: 0.806908]\n",
      "109 [D loss: 0.758822, acc.: 40.00%] [G loss: 0.835123]\n",
      "110 [D loss: 0.713411, acc.: 50.00%] [G loss: 0.743578]\n",
      "111 [D loss: 0.739331, acc.: 40.00%] [G loss: 0.828176]\n",
      "112 [D loss: 0.705926, acc.: 52.50%] [G loss: 0.744546]\n",
      "113 [D loss: 0.699637, acc.: 51.25%] [G loss: 0.789689]\n",
      "114 [D loss: 0.753236, acc.: 42.50%] [G loss: 0.715096]\n",
      "115 [D loss: 0.736274, acc.: 45.00%] [G loss: 0.687344]\n",
      "116 [D loss: 0.748136, acc.: 43.75%] [G loss: 0.762217]\n",
      "117 [D loss: 0.745335, acc.: 47.50%] [G loss: 0.738328]\n",
      "118 [D loss: 0.723719, acc.: 42.50%] [G loss: 0.764005]\n",
      "119 [D loss: 0.714667, acc.: 48.75%] [G loss: 0.720059]\n",
      "120 [D loss: 0.701359, acc.: 51.25%] [G loss: 0.836400]\n",
      "121 [D loss: 0.737327, acc.: 46.25%] [G loss: 0.816294]\n",
      "122 [D loss: 0.736466, acc.: 48.75%] [G loss: 0.797357]\n",
      "123 [D loss: 0.730366, acc.: 46.25%] [G loss: 0.813259]\n",
      "124 [D loss: 0.780428, acc.: 41.25%] [G loss: 0.758313]\n",
      "125 [D loss: 0.727695, acc.: 46.25%] [G loss: 0.818278]\n",
      "126 [D loss: 0.736472, acc.: 42.50%] [G loss: 0.818346]\n",
      "127 [D loss: 0.749194, acc.: 48.75%] [G loss: 0.814603]\n",
      "128 [D loss: 0.775210, acc.: 38.75%] [G loss: 0.786750]\n",
      "129 [D loss: 0.740201, acc.: 48.75%] [G loss: 0.790624]\n",
      "130 [D loss: 0.725205, acc.: 45.00%] [G loss: 0.727065]\n",
      "131 [D loss: 0.753692, acc.: 40.00%] [G loss: 0.839256]\n",
      "132 [D loss: 0.745259, acc.: 37.50%] [G loss: 0.833139]\n",
      "133 [D loss: 0.703191, acc.: 50.00%] [G loss: 0.744726]\n",
      "134 [D loss: 0.727381, acc.: 48.75%] [G loss: 0.774972]\n",
      "135 [D loss: 0.800526, acc.: 33.75%] [G loss: 0.777238]\n",
      "136 [D loss: 0.737197, acc.: 45.00%] [G loss: 0.778536]\n",
      "137 [D loss: 0.750171, acc.: 38.75%] [G loss: 0.773977]\n",
      "138 [D loss: 0.726368, acc.: 51.25%] [G loss: 0.770292]\n",
      "139 [D loss: 0.753819, acc.: 43.75%] [G loss: 0.778945]\n",
      "140 [D loss: 0.736075, acc.: 45.00%] [G loss: 0.817062]\n",
      "141 [D loss: 0.740634, acc.: 40.00%] [G loss: 0.743358]\n",
      "142 [D loss: 0.718170, acc.: 50.00%] [G loss: 0.813337]\n",
      "143 [D loss: 0.732147, acc.: 48.75%] [G loss: 0.803620]\n",
      "144 [D loss: 0.753002, acc.: 40.00%] [G loss: 0.778394]\n",
      "145 [D loss: 0.680021, acc.: 56.25%] [G loss: 0.777542]\n",
      "146 [D loss: 0.723756, acc.: 46.25%] [G loss: 0.796653]\n",
      "147 [D loss: 0.706489, acc.: 48.75%] [G loss: 0.761472]\n",
      "148 [D loss: 0.779772, acc.: 43.75%] [G loss: 0.774049]\n",
      "149 [D loss: 0.718119, acc.: 50.00%] [G loss: 0.784884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.749167, acc.: 38.75%] [G loss: 0.790064]\n",
      "151 [D loss: 0.746174, acc.: 42.50%] [G loss: 0.766850]\n",
      "152 [D loss: 0.708260, acc.: 52.50%] [G loss: 0.799428]\n",
      "153 [D loss: 0.770970, acc.: 41.25%] [G loss: 0.855220]\n",
      "154 [D loss: 0.744627, acc.: 41.25%] [G loss: 0.809979]\n",
      "155 [D loss: 0.714084, acc.: 51.25%] [G loss: 0.780241]\n",
      "156 [D loss: 0.746546, acc.: 46.25%] [G loss: 0.802130]\n",
      "157 [D loss: 0.795172, acc.: 36.25%] [G loss: 0.787575]\n",
      "158 [D loss: 0.730653, acc.: 43.75%] [G loss: 0.829278]\n",
      "159 [D loss: 0.708655, acc.: 51.25%] [G loss: 0.819801]\n",
      "160 [D loss: 0.741019, acc.: 43.75%] [G loss: 0.777609]\n",
      "161 [D loss: 0.781034, acc.: 38.75%] [G loss: 0.820308]\n",
      "162 [D loss: 0.733763, acc.: 48.75%] [G loss: 0.858295]\n",
      "163 [D loss: 0.743423, acc.: 46.25%] [G loss: 0.787223]\n",
      "164 [D loss: 0.737612, acc.: 43.75%] [G loss: 0.802024]\n",
      "165 [D loss: 0.741312, acc.: 36.25%] [G loss: 0.805255]\n",
      "166 [D loss: 0.745240, acc.: 38.75%] [G loss: 0.805600]\n",
      "167 [D loss: 0.778876, acc.: 38.75%] [G loss: 0.813769]\n",
      "168 [D loss: 0.713028, acc.: 52.50%] [G loss: 0.748366]\n",
      "169 [D loss: 0.751841, acc.: 45.00%] [G loss: 0.849172]\n",
      "170 [D loss: 0.738138, acc.: 46.25%] [G loss: 0.794357]\n",
      "171 [D loss: 0.766429, acc.: 37.50%] [G loss: 0.811147]\n",
      "172 [D loss: 0.733022, acc.: 43.75%] [G loss: 0.770921]\n",
      "173 [D loss: 0.744346, acc.: 47.50%] [G loss: 0.790033]\n",
      "174 [D loss: 0.736056, acc.: 50.00%] [G loss: 0.763257]\n",
      "175 [D loss: 0.733055, acc.: 42.50%] [G loss: 0.778864]\n",
      "176 [D loss: 0.775734, acc.: 36.25%] [G loss: 0.827638]\n",
      "177 [D loss: 0.714656, acc.: 51.25%] [G loss: 0.735276]\n",
      "178 [D loss: 0.712671, acc.: 45.00%] [G loss: 0.815483]\n",
      "179 [D loss: 0.718312, acc.: 48.75%] [G loss: 0.830944]\n",
      "180 [D loss: 0.721449, acc.: 46.25%] [G loss: 0.774698]\n",
      "181 [D loss: 0.749696, acc.: 43.75%] [G loss: 0.783908]\n",
      "182 [D loss: 0.748379, acc.: 38.75%] [G loss: 0.801604]\n",
      "183 [D loss: 0.743455, acc.: 41.25%] [G loss: 0.789516]\n",
      "184 [D loss: 0.750407, acc.: 43.75%] [G loss: 0.765502]\n",
      "185 [D loss: 0.750307, acc.: 45.00%] [G loss: 0.807405]\n",
      "186 [D loss: 0.767271, acc.: 47.50%] [G loss: 0.809442]\n",
      "187 [D loss: 0.757879, acc.: 41.25%] [G loss: 0.890925]\n",
      "188 [D loss: 0.747233, acc.: 41.25%] [G loss: 0.838903]\n",
      "189 [D loss: 0.707053, acc.: 51.25%] [G loss: 0.869002]\n",
      "190 [D loss: 0.734695, acc.: 47.50%] [G loss: 0.770244]\n",
      "191 [D loss: 0.763090, acc.: 42.50%] [G loss: 0.802663]\n",
      "192 [D loss: 0.757262, acc.: 40.00%] [G loss: 0.804310]\n",
      "193 [D loss: 0.717370, acc.: 48.75%] [G loss: 0.790899]\n",
      "194 [D loss: 0.739227, acc.: 48.75%] [G loss: 0.768630]\n",
      "195 [D loss: 0.720406, acc.: 46.25%] [G loss: 0.729630]\n",
      "196 [D loss: 0.750556, acc.: 43.75%] [G loss: 0.785501]\n",
      "197 [D loss: 0.711971, acc.: 51.25%] [G loss: 0.786599]\n",
      "198 [D loss: 0.687917, acc.: 58.75%] [G loss: 0.745242]\n",
      "199 [D loss: 0.743474, acc.: 38.75%] [G loss: 0.773013]\n",
      "200 [D loss: 0.748659, acc.: 38.75%] [G loss: 0.729303]\n",
      "201 [D loss: 0.756618, acc.: 41.25%] [G loss: 0.797705]\n",
      "202 [D loss: 0.697621, acc.: 56.25%] [G loss: 0.741494]\n",
      "203 [D loss: 0.768378, acc.: 37.50%] [G loss: 0.755965]\n",
      "204 [D loss: 0.702113, acc.: 53.75%] [G loss: 0.788252]\n",
      "205 [D loss: 0.764801, acc.: 35.00%] [G loss: 0.830040]\n",
      "206 [D loss: 0.751018, acc.: 42.50%] [G loss: 0.802318]\n",
      "207 [D loss: 0.757071, acc.: 32.50%] [G loss: 0.795068]\n",
      "208 [D loss: 0.717191, acc.: 42.50%] [G loss: 0.821349]\n",
      "209 [D loss: 0.760339, acc.: 35.00%] [G loss: 0.814715]\n",
      "210 [D loss: 0.731688, acc.: 51.25%] [G loss: 0.804084]\n",
      "211 [D loss: 0.743935, acc.: 41.25%] [G loss: 0.785045]\n",
      "212 [D loss: 0.754052, acc.: 41.25%] [G loss: 0.810996]\n",
      "213 [D loss: 0.744959, acc.: 46.25%] [G loss: 0.770942]\n",
      "214 [D loss: 0.698582, acc.: 53.75%] [G loss: 0.766271]\n",
      "215 [D loss: 0.753332, acc.: 45.00%] [G loss: 0.781821]\n",
      "216 [D loss: 0.731190, acc.: 50.00%] [G loss: 0.761781]\n",
      "217 [D loss: 0.710117, acc.: 51.25%] [G loss: 0.748762]\n",
      "218 [D loss: 0.697147, acc.: 52.50%] [G loss: 0.752070]\n",
      "219 [D loss: 0.768966, acc.: 32.50%] [G loss: 0.826471]\n",
      "220 [D loss: 0.726916, acc.: 51.25%] [G loss: 0.778068]\n",
      "221 [D loss: 0.769065, acc.: 31.25%] [G loss: 0.793834]\n",
      "222 [D loss: 0.725395, acc.: 51.25%] [G loss: 0.741779]\n",
      "223 [D loss: 0.763444, acc.: 36.25%] [G loss: 0.829524]\n",
      "224 [D loss: 0.761382, acc.: 37.50%] [G loss: 0.777838]\n",
      "225 [D loss: 0.743163, acc.: 46.25%] [G loss: 0.771449]\n",
      "226 [D loss: 0.719425, acc.: 52.50%] [G loss: 0.790857]\n",
      "227 [D loss: 0.710689, acc.: 42.50%] [G loss: 0.815628]\n",
      "228 [D loss: 0.750618, acc.: 42.50%] [G loss: 0.839635]\n",
      "229 [D loss: 0.741210, acc.: 50.00%] [G loss: 0.793530]\n",
      "230 [D loss: 0.722600, acc.: 47.50%] [G loss: 0.834847]\n",
      "231 [D loss: 0.734962, acc.: 41.25%] [G loss: 0.773414]\n",
      "232 [D loss: 0.721042, acc.: 47.50%] [G loss: 0.802964]\n",
      "233 [D loss: 0.759441, acc.: 42.50%] [G loss: 0.792305]\n",
      "234 [D loss: 0.789709, acc.: 36.25%] [G loss: 0.851594]\n",
      "235 [D loss: 0.746632, acc.: 46.25%] [G loss: 0.820158]\n",
      "236 [D loss: 0.747760, acc.: 42.50%] [G loss: 0.831651]\n",
      "237 [D loss: 0.742153, acc.: 42.50%] [G loss: 0.804045]\n",
      "238 [D loss: 0.751129, acc.: 41.25%] [G loss: 0.814285]\n",
      "239 [D loss: 0.718321, acc.: 48.75%] [G loss: 0.772362]\n",
      "240 [D loss: 0.697182, acc.: 58.75%] [G loss: 0.799020]\n",
      "241 [D loss: 0.762211, acc.: 37.50%] [G loss: 0.881393]\n",
      "242 [D loss: 0.742080, acc.: 45.00%] [G loss: 0.756065]\n",
      "243 [D loss: 0.691702, acc.: 48.75%] [G loss: 0.855590]\n",
      "244 [D loss: 0.744989, acc.: 41.25%] [G loss: 0.809193]\n",
      "245 [D loss: 0.738547, acc.: 46.25%] [G loss: 0.840083]\n",
      "246 [D loss: 0.759979, acc.: 42.50%] [G loss: 0.806421]\n",
      "247 [D loss: 0.773451, acc.: 37.50%] [G loss: 0.796699]\n",
      "248 [D loss: 0.724408, acc.: 47.50%] [G loss: 0.846487]\n",
      "249 [D loss: 0.703299, acc.: 52.50%] [G loss: 0.730197]\n",
      "250 [D loss: 0.751222, acc.: 41.25%] [G loss: 0.825347]\n",
      "251 [D loss: 0.683582, acc.: 53.75%] [G loss: 0.807964]\n",
      "252 [D loss: 0.696360, acc.: 56.25%] [G loss: 0.820972]\n",
      "253 [D loss: 0.712651, acc.: 50.00%] [G loss: 0.801073]\n",
      "254 [D loss: 0.740644, acc.: 42.50%] [G loss: 0.760237]\n",
      "255 [D loss: 0.716357, acc.: 53.75%] [G loss: 0.777614]\n",
      "256 [D loss: 0.743551, acc.: 43.75%] [G loss: 0.777160]\n",
      "257 [D loss: 0.745698, acc.: 46.25%] [G loss: 0.756008]\n",
      "258 [D loss: 0.718768, acc.: 48.75%] [G loss: 0.804348]\n",
      "259 [D loss: 0.765806, acc.: 41.25%] [G loss: 0.863432]\n",
      "260 [D loss: 0.766797, acc.: 36.25%] [G loss: 0.802145]\n",
      "261 [D loss: 0.743295, acc.: 47.50%] [G loss: 0.814089]\n",
      "262 [D loss: 0.734269, acc.: 43.75%] [G loss: 0.783787]\n",
      "263 [D loss: 0.766266, acc.: 38.75%] [G loss: 0.822362]\n",
      "264 [D loss: 0.774256, acc.: 40.00%] [G loss: 0.810142]\n",
      "265 [D loss: 0.748585, acc.: 41.25%] [G loss: 0.819287]\n",
      "266 [D loss: 0.739782, acc.: 43.75%] [G loss: 0.756949]\n",
      "267 [D loss: 0.711267, acc.: 52.50%] [G loss: 0.792728]\n",
      "268 [D loss: 0.734621, acc.: 43.75%] [G loss: 0.782271]\n",
      "269 [D loss: 0.717886, acc.: 51.25%] [G loss: 0.797970]\n",
      "270 [D loss: 0.744213, acc.: 43.75%] [G loss: 0.756760]\n",
      "271 [D loss: 0.696930, acc.: 52.50%] [G loss: 0.814379]\n",
      "272 [D loss: 0.743184, acc.: 42.50%] [G loss: 0.736731]\n",
      "273 [D loss: 0.793345, acc.: 36.25%] [G loss: 0.774664]\n",
      "274 [D loss: 0.715705, acc.: 42.50%] [G loss: 0.748606]\n",
      "275 [D loss: 0.734854, acc.: 47.50%] [G loss: 0.789748]\n",
      "276 [D loss: 0.715871, acc.: 46.25%] [G loss: 0.742637]\n",
      "277 [D loss: 0.724677, acc.: 48.75%] [G loss: 0.775921]\n",
      "278 [D loss: 0.753387, acc.: 45.00%] [G loss: 0.761423]\n",
      "279 [D loss: 0.739719, acc.: 46.25%] [G loss: 0.780560]\n",
      "280 [D loss: 0.735861, acc.: 43.75%] [G loss: 0.763242]\n",
      "281 [D loss: 0.742130, acc.: 40.00%] [G loss: 0.822861]\n",
      "282 [D loss: 0.704232, acc.: 56.25%] [G loss: 0.814889]\n",
      "283 [D loss: 0.741183, acc.: 42.50%] [G loss: 0.761508]\n",
      "284 [D loss: 0.729813, acc.: 46.25%] [G loss: 0.800243]\n",
      "285 [D loss: 0.770694, acc.: 37.50%] [G loss: 0.817493]\n",
      "286 [D loss: 0.742451, acc.: 43.75%] [G loss: 0.811049]\n",
      "287 [D loss: 0.737529, acc.: 48.75%] [G loss: 0.791380]\n",
      "288 [D loss: 0.743739, acc.: 45.00%] [G loss: 0.844516]\n",
      "289 [D loss: 0.770765, acc.: 37.50%] [G loss: 0.878527]\n",
      "290 [D loss: 0.717218, acc.: 43.75%] [G loss: 0.831983]\n",
      "291 [D loss: 0.749987, acc.: 45.00%] [G loss: 0.825705]\n",
      "292 [D loss: 0.731947, acc.: 45.00%] [G loss: 0.800727]\n",
      "293 [D loss: 0.723878, acc.: 46.25%] [G loss: 0.875425]\n",
      "294 [D loss: 0.745917, acc.: 40.00%] [G loss: 0.828492]\n",
      "295 [D loss: 0.733088, acc.: 43.75%] [G loss: 0.784768]\n",
      "296 [D loss: 0.724387, acc.: 48.75%] [G loss: 0.791469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.737064, acc.: 37.50%] [G loss: 0.802394]\n",
      "298 [D loss: 0.766250, acc.: 35.00%] [G loss: 0.788006]\n",
      "299 [D loss: 0.728366, acc.: 46.25%] [G loss: 0.828989]\n",
      "300 [D loss: 0.727783, acc.: 52.50%] [G loss: 0.791333]\n",
      "301 [D loss: 0.765643, acc.: 43.75%] [G loss: 0.758970]\n",
      "302 [D loss: 0.718334, acc.: 42.50%] [G loss: 0.772771]\n",
      "303 [D loss: 0.739874, acc.: 45.00%] [G loss: 0.804739]\n",
      "304 [D loss: 0.745814, acc.: 47.50%] [G loss: 0.812010]\n",
      "305 [D loss: 0.733071, acc.: 46.25%] [G loss: 0.780859]\n",
      "306 [D loss: 0.686564, acc.: 60.00%] [G loss: 0.819390]\n",
      "307 [D loss: 0.706257, acc.: 48.75%] [G loss: 0.764173]\n",
      "308 [D loss: 0.774022, acc.: 38.75%] [G loss: 0.784164]\n",
      "309 [D loss: 0.736348, acc.: 48.75%] [G loss: 0.788109]\n",
      "310 [D loss: 0.695478, acc.: 53.75%] [G loss: 0.781896]\n",
      "311 [D loss: 0.723680, acc.: 46.25%] [G loss: 0.820045]\n",
      "312 [D loss: 0.738310, acc.: 46.25%] [G loss: 0.748634]\n",
      "313 [D loss: 0.737150, acc.: 48.75%] [G loss: 0.791084]\n",
      "314 [D loss: 0.724838, acc.: 48.75%] [G loss: 0.771664]\n",
      "315 [D loss: 0.695337, acc.: 50.00%] [G loss: 0.714723]\n",
      "316 [D loss: 0.772204, acc.: 40.00%] [G loss: 0.770146]\n",
      "317 [D loss: 0.730409, acc.: 43.75%] [G loss: 0.798610]\n",
      "318 [D loss: 0.729984, acc.: 45.00%] [G loss: 0.760987]\n",
      "319 [D loss: 0.721378, acc.: 47.50%] [G loss: 0.747510]\n",
      "320 [D loss: 0.753976, acc.: 45.00%] [G loss: 0.727165]\n",
      "321 [D loss: 0.734844, acc.: 46.25%] [G loss: 0.751279]\n",
      "322 [D loss: 0.749826, acc.: 42.50%] [G loss: 0.851958]\n",
      "323 [D loss: 0.754741, acc.: 35.00%] [G loss: 0.787724]\n",
      "324 [D loss: 0.727595, acc.: 46.25%] [G loss: 0.766062]\n",
      "325 [D loss: 0.727644, acc.: 41.25%] [G loss: 0.808054]\n",
      "326 [D loss: 0.734294, acc.: 48.75%] [G loss: 0.783003]\n",
      "327 [D loss: 0.739348, acc.: 41.25%] [G loss: 0.726284]\n",
      "328 [D loss: 0.742637, acc.: 40.00%] [G loss: 0.789232]\n",
      "329 [D loss: 0.720739, acc.: 41.25%] [G loss: 0.772942]\n",
      "330 [D loss: 0.737671, acc.: 45.00%] [G loss: 0.766455]\n",
      "331 [D loss: 0.742613, acc.: 41.25%] [G loss: 0.748641]\n",
      "332 [D loss: 0.718894, acc.: 45.00%] [G loss: 0.816350]\n",
      "333 [D loss: 0.710902, acc.: 52.50%] [G loss: 0.722522]\n",
      "334 [D loss: 0.743036, acc.: 45.00%] [G loss: 0.792052]\n",
      "335 [D loss: 0.713302, acc.: 47.50%] [G loss: 0.743856]\n",
      "336 [D loss: 0.739359, acc.: 43.75%] [G loss: 0.765602]\n",
      "337 [D loss: 0.736716, acc.: 47.50%] [G loss: 0.798759]\n",
      "338 [D loss: 0.771415, acc.: 35.00%] [G loss: 0.746911]\n",
      "339 [D loss: 0.740766, acc.: 45.00%] [G loss: 0.780680]\n",
      "340 [D loss: 0.729058, acc.: 42.50%] [G loss: 0.767301]\n",
      "341 [D loss: 0.736101, acc.: 37.50%] [G loss: 0.794297]\n",
      "342 [D loss: 0.745355, acc.: 42.50%] [G loss: 0.843646]\n",
      "343 [D loss: 0.720218, acc.: 45.00%] [G loss: 0.783892]\n",
      "344 [D loss: 0.749419, acc.: 41.25%] [G loss: 0.805279]\n",
      "345 [D loss: 0.724528, acc.: 45.00%] [G loss: 0.794097]\n",
      "346 [D loss: 0.727193, acc.: 43.75%] [G loss: 0.811048]\n",
      "347 [D loss: 0.708305, acc.: 53.75%] [G loss: 0.777874]\n",
      "348 [D loss: 0.729831, acc.: 47.50%] [G loss: 0.750988]\n",
      "349 [D loss: 0.749120, acc.: 46.25%] [G loss: 0.763843]\n",
      "350 [D loss: 0.729125, acc.: 48.75%] [G loss: 0.798761]\n",
      "351 [D loss: 0.737164, acc.: 42.50%] [G loss: 0.817784]\n",
      "352 [D loss: 0.741277, acc.: 48.75%] [G loss: 0.781038]\n",
      "353 [D loss: 0.727654, acc.: 48.75%] [G loss: 0.807579]\n",
      "354 [D loss: 0.724492, acc.: 47.50%] [G loss: 0.767227]\n",
      "355 [D loss: 0.720970, acc.: 50.00%] [G loss: 0.797273]\n",
      "356 [D loss: 0.704190, acc.: 48.75%] [G loss: 0.756987]\n",
      "357 [D loss: 0.708715, acc.: 51.25%] [G loss: 0.783062]\n",
      "358 [D loss: 0.752701, acc.: 41.25%] [G loss: 0.769414]\n",
      "359 [D loss: 0.748273, acc.: 37.50%] [G loss: 0.745862]\n",
      "360 [D loss: 0.721017, acc.: 47.50%] [G loss: 0.744143]\n",
      "361 [D loss: 0.773276, acc.: 37.50%] [G loss: 0.760677]\n",
      "362 [D loss: 0.774231, acc.: 37.50%] [G loss: 0.829441]\n",
      "363 [D loss: 0.684567, acc.: 51.25%] [G loss: 0.781294]\n",
      "364 [D loss: 0.732207, acc.: 38.75%] [G loss: 0.804138]\n",
      "365 [D loss: 0.772287, acc.: 35.00%] [G loss: 0.780100]\n",
      "366 [D loss: 0.707498, acc.: 47.50%] [G loss: 0.781634]\n",
      "367 [D loss: 0.778643, acc.: 37.50%] [G loss: 0.795134]\n",
      "368 [D loss: 0.696547, acc.: 51.25%] [G loss: 0.838719]\n",
      "369 [D loss: 0.748060, acc.: 47.50%] [G loss: 0.783478]\n",
      "370 [D loss: 0.707972, acc.: 50.00%] [G loss: 0.814094]\n",
      "371 [D loss: 0.744980, acc.: 41.25%] [G loss: 0.775519]\n",
      "372 [D loss: 0.735641, acc.: 37.50%] [G loss: 0.792322]\n",
      "373 [D loss: 0.764805, acc.: 40.00%] [G loss: 0.833179]\n",
      "374 [D loss: 0.768881, acc.: 42.50%] [G loss: 0.793933]\n",
      "375 [D loss: 0.729146, acc.: 45.00%] [G loss: 0.844864]\n",
      "376 [D loss: 0.746125, acc.: 48.75%] [G loss: 0.830397]\n",
      "377 [D loss: 0.718325, acc.: 50.00%] [G loss: 0.768819]\n",
      "378 [D loss: 0.739016, acc.: 43.75%] [G loss: 0.790843]\n",
      "379 [D loss: 0.714410, acc.: 50.00%] [G loss: 0.770052]\n",
      "380 [D loss: 0.750128, acc.: 43.75%] [G loss: 0.742323]\n",
      "381 [D loss: 0.732012, acc.: 43.75%] [G loss: 0.822787]\n",
      "382 [D loss: 0.723745, acc.: 38.75%] [G loss: 0.786606]\n",
      "383 [D loss: 0.718577, acc.: 41.25%] [G loss: 0.763625]\n",
      "384 [D loss: 0.727809, acc.: 42.50%] [G loss: 0.768200]\n",
      "385 [D loss: 0.727890, acc.: 46.25%] [G loss: 0.794513]\n",
      "386 [D loss: 0.741979, acc.: 45.00%] [G loss: 0.748660]\n",
      "387 [D loss: 0.726254, acc.: 51.25%] [G loss: 0.787822]\n",
      "388 [D loss: 0.728514, acc.: 47.50%] [G loss: 0.776664]\n",
      "389 [D loss: 0.722851, acc.: 40.00%] [G loss: 0.741266]\n",
      "390 [D loss: 0.742925, acc.: 41.25%] [G loss: 0.820487]\n",
      "391 [D loss: 0.761893, acc.: 41.25%] [G loss: 0.846358]\n",
      "392 [D loss: 0.658377, acc.: 60.00%] [G loss: 0.810574]\n",
      "393 [D loss: 0.756054, acc.: 42.50%] [G loss: 0.823668]\n",
      "394 [D loss: 0.733664, acc.: 48.75%] [G loss: 0.768909]\n",
      "395 [D loss: 0.714806, acc.: 45.00%] [G loss: 0.782889]\n",
      "396 [D loss: 0.769932, acc.: 36.25%] [G loss: 0.784178]\n",
      "397 [D loss: 0.741892, acc.: 45.00%] [G loss: 0.765098]\n",
      "398 [D loss: 0.735357, acc.: 42.50%] [G loss: 0.800317]\n",
      "399 [D loss: 0.719469, acc.: 51.25%] [G loss: 0.790839]\n",
      "400 [D loss: 0.724318, acc.: 43.75%] [G loss: 0.841469]\n",
      "401 [D loss: 0.735968, acc.: 47.50%] [G loss: 0.742215]\n",
      "402 [D loss: 0.739061, acc.: 46.25%] [G loss: 0.819537]\n",
      "403 [D loss: 0.769453, acc.: 38.75%] [G loss: 0.818713]\n",
      "404 [D loss: 0.696014, acc.: 52.50%] [G loss: 0.817616]\n",
      "405 [D loss: 0.744708, acc.: 45.00%] [G loss: 0.791860]\n",
      "406 [D loss: 0.727570, acc.: 45.00%] [G loss: 0.829634]\n",
      "407 [D loss: 0.716215, acc.: 45.00%] [G loss: 0.823969]\n",
      "408 [D loss: 0.738358, acc.: 45.00%] [G loss: 0.776986]\n",
      "409 [D loss: 0.754894, acc.: 37.50%] [G loss: 0.767354]\n",
      "410 [D loss: 0.720030, acc.: 45.00%] [G loss: 0.760588]\n",
      "411 [D loss: 0.744122, acc.: 42.50%] [G loss: 0.762861]\n",
      "412 [D loss: 0.729620, acc.: 48.75%] [G loss: 0.790596]\n",
      "413 [D loss: 0.717252, acc.: 53.75%] [G loss: 0.796729]\n",
      "414 [D loss: 0.786076, acc.: 45.00%] [G loss: 0.785120]\n",
      "415 [D loss: 0.745609, acc.: 45.00%] [G loss: 0.810381]\n",
      "416 [D loss: 0.742442, acc.: 47.50%] [G loss: 0.836691]\n",
      "417 [D loss: 0.699765, acc.: 50.00%] [G loss: 0.783881]\n",
      "418 [D loss: 0.729749, acc.: 43.75%] [G loss: 0.750945]\n",
      "419 [D loss: 0.745026, acc.: 45.00%] [G loss: 0.820281]\n",
      "420 [D loss: 0.776127, acc.: 37.50%] [G loss: 0.794855]\n",
      "421 [D loss: 0.729937, acc.: 50.00%] [G loss: 0.798482]\n",
      "422 [D loss: 0.743773, acc.: 38.75%] [G loss: 0.826221]\n",
      "423 [D loss: 0.715735, acc.: 43.75%] [G loss: 0.746348]\n",
      "424 [D loss: 0.751076, acc.: 41.25%] [G loss: 0.795718]\n",
      "425 [D loss: 0.749617, acc.: 40.00%] [G loss: 0.753953]\n",
      "426 [D loss: 0.749101, acc.: 37.50%] [G loss: 0.743217]\n",
      "427 [D loss: 0.733163, acc.: 42.50%] [G loss: 0.818752]\n",
      "428 [D loss: 0.730942, acc.: 43.75%] [G loss: 0.786361]\n",
      "429 [D loss: 0.728555, acc.: 43.75%] [G loss: 0.809373]\n",
      "430 [D loss: 0.718336, acc.: 48.75%] [G loss: 0.725260]\n",
      "431 [D loss: 0.738168, acc.: 41.25%] [G loss: 0.702293]\n",
      "432 [D loss: 0.738430, acc.: 37.50%] [G loss: 0.838452]\n",
      "433 [D loss: 0.687127, acc.: 58.75%] [G loss: 0.759454]\n",
      "434 [D loss: 0.727179, acc.: 42.50%] [G loss: 0.779087]\n",
      "435 [D loss: 0.728000, acc.: 45.00%] [G loss: 0.759518]\n",
      "436 [D loss: 0.727341, acc.: 41.25%] [G loss: 0.817166]\n",
      "437 [D loss: 0.728604, acc.: 43.75%] [G loss: 0.793063]\n",
      "438 [D loss: 0.744063, acc.: 41.25%] [G loss: 0.725757]\n",
      "439 [D loss: 0.732779, acc.: 37.50%] [G loss: 0.808472]\n",
      "440 [D loss: 0.694976, acc.: 56.25%] [G loss: 0.787364]\n",
      "441 [D loss: 0.687478, acc.: 53.75%] [G loss: 0.821791]\n",
      "442 [D loss: 0.740716, acc.: 42.50%] [G loss: 0.769760]\n",
      "443 [D loss: 0.722399, acc.: 41.25%] [G loss: 0.832043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 0.760544, acc.: 37.50%] [G loss: 0.726516]\n",
      "445 [D loss: 0.730081, acc.: 45.00%] [G loss: 0.769558]\n",
      "446 [D loss: 0.745187, acc.: 41.25%] [G loss: 0.852853]\n",
      "447 [D loss: 0.695938, acc.: 51.25%] [G loss: 0.803717]\n",
      "448 [D loss: 0.718131, acc.: 50.00%] [G loss: 0.775113]\n",
      "449 [D loss: 0.714238, acc.: 42.50%] [G loss: 0.739248]\n",
      "450 [D loss: 0.675510, acc.: 53.75%] [G loss: 0.739588]\n",
      "451 [D loss: 0.749993, acc.: 42.50%] [G loss: 0.780425]\n",
      "452 [D loss: 0.711980, acc.: 51.25%] [G loss: 0.777367]\n",
      "453 [D loss: 0.742979, acc.: 42.50%] [G loss: 0.795889]\n",
      "454 [D loss: 0.714476, acc.: 42.50%] [G loss: 0.749510]\n",
      "455 [D loss: 0.718182, acc.: 47.50%] [G loss: 0.735536]\n",
      "456 [D loss: 0.686900, acc.: 56.25%] [G loss: 0.789914]\n",
      "457 [D loss: 0.713226, acc.: 51.25%] [G loss: 0.763884]\n",
      "458 [D loss: 0.745151, acc.: 37.50%] [G loss: 0.745402]\n",
      "459 [D loss: 0.754401, acc.: 37.50%] [G loss: 0.774110]\n",
      "460 [D loss: 0.705907, acc.: 51.25%] [G loss: 0.819781]\n",
      "461 [D loss: 0.764202, acc.: 41.25%] [G loss: 0.747327]\n",
      "462 [D loss: 0.733382, acc.: 35.00%] [G loss: 0.756486]\n",
      "463 [D loss: 0.761782, acc.: 40.00%] [G loss: 0.791615]\n",
      "464 [D loss: 0.712784, acc.: 56.25%] [G loss: 0.793238]\n",
      "465 [D loss: 0.751029, acc.: 37.50%] [G loss: 0.770647]\n",
      "466 [D loss: 0.722069, acc.: 40.00%] [G loss: 0.805321]\n",
      "467 [D loss: 0.745328, acc.: 45.00%] [G loss: 0.811099]\n",
      "468 [D loss: 0.739012, acc.: 45.00%] [G loss: 0.793563]\n",
      "469 [D loss: 0.733748, acc.: 42.50%] [G loss: 0.797725]\n",
      "470 [D loss: 0.768745, acc.: 43.75%] [G loss: 0.803225]\n",
      "471 [D loss: 0.717926, acc.: 45.00%] [G loss: 0.796253]\n",
      "472 [D loss: 0.755436, acc.: 37.50%] [G loss: 0.799615]\n",
      "473 [D loss: 0.710639, acc.: 50.00%] [G loss: 0.750291]\n",
      "474 [D loss: 0.737693, acc.: 42.50%] [G loss: 0.778838]\n",
      "475 [D loss: 0.753183, acc.: 41.25%] [G loss: 0.851605]\n",
      "476 [D loss: 0.720406, acc.: 47.50%] [G loss: 0.791456]\n",
      "477 [D loss: 0.709604, acc.: 45.00%] [G loss: 0.765904]\n",
      "478 [D loss: 0.733458, acc.: 41.25%] [G loss: 0.785426]\n",
      "479 [D loss: 0.725767, acc.: 47.50%] [G loss: 0.826883]\n",
      "480 [D loss: 0.702196, acc.: 51.25%] [G loss: 0.773696]\n",
      "481 [D loss: 0.737608, acc.: 38.75%] [G loss: 0.734026]\n",
      "482 [D loss: 0.714392, acc.: 48.75%] [G loss: 0.799163]\n",
      "483 [D loss: 0.704977, acc.: 53.75%] [G loss: 0.760178]\n",
      "484 [D loss: 0.728264, acc.: 47.50%] [G loss: 0.747093]\n",
      "485 [D loss: 0.734058, acc.: 46.25%] [G loss: 0.757754]\n",
      "486 [D loss: 0.719268, acc.: 47.50%] [G loss: 0.773866]\n",
      "487 [D loss: 0.759661, acc.: 40.00%] [G loss: 0.822223]\n",
      "488 [D loss: 0.728368, acc.: 50.00%] [G loss: 0.807422]\n",
      "489 [D loss: 0.715413, acc.: 47.50%] [G loss: 0.791800]\n",
      "490 [D loss: 0.733845, acc.: 46.25%] [G loss: 0.774222]\n",
      "491 [D loss: 0.752484, acc.: 36.25%] [G loss: 0.803320]\n",
      "492 [D loss: 0.703120, acc.: 52.50%] [G loss: 0.810088]\n",
      "493 [D loss: 0.703636, acc.: 53.75%] [G loss: 0.744457]\n",
      "494 [D loss: 0.748256, acc.: 40.00%] [G loss: 0.784035]\n",
      "495 [D loss: 0.719435, acc.: 45.00%] [G loss: 0.810085]\n",
      "496 [D loss: 0.743969, acc.: 42.50%] [G loss: 0.748685]\n",
      "497 [D loss: 0.736578, acc.: 45.00%] [G loss: 0.760938]\n",
      "498 [D loss: 0.742362, acc.: 41.25%] [G loss: 0.780156]\n",
      "499 [D loss: 0.737852, acc.: 35.00%] [G loss: 0.763928]\n",
      "500 [D loss: 0.734413, acc.: 48.75%] [G loss: 0.782446]\n",
      "501 [D loss: 0.687467, acc.: 58.75%] [G loss: 0.766752]\n",
      "502 [D loss: 0.697417, acc.: 51.25%] [G loss: 0.730729]\n",
      "503 [D loss: 0.721444, acc.: 40.00%] [G loss: 0.759916]\n",
      "504 [D loss: 0.733407, acc.: 56.25%] [G loss: 0.784398]\n",
      "505 [D loss: 0.744161, acc.: 47.50%] [G loss: 0.761736]\n",
      "506 [D loss: 0.695384, acc.: 52.50%] [G loss: 0.737417]\n",
      "507 [D loss: 0.683056, acc.: 55.00%] [G loss: 0.812240]\n",
      "508 [D loss: 0.762752, acc.: 33.75%] [G loss: 0.759312]\n",
      "509 [D loss: 0.753214, acc.: 45.00%] [G loss: 0.791456]\n",
      "510 [D loss: 0.726177, acc.: 50.00%] [G loss: 0.809597]\n",
      "511 [D loss: 0.751188, acc.: 37.50%] [G loss: 0.817496]\n",
      "512 [D loss: 0.709770, acc.: 47.50%] [G loss: 0.738261]\n",
      "513 [D loss: 0.724364, acc.: 41.25%] [G loss: 0.822158]\n",
      "514 [D loss: 0.715699, acc.: 46.25%] [G loss: 0.832292]\n",
      "515 [D loss: 0.708166, acc.: 48.75%] [G loss: 0.776268]\n",
      "516 [D loss: 0.710329, acc.: 47.50%] [G loss: 0.761788]\n",
      "517 [D loss: 0.765277, acc.: 36.25%] [G loss: 0.799474]\n",
      "518 [D loss: 0.739761, acc.: 38.75%] [G loss: 0.761819]\n",
      "519 [D loss: 0.700691, acc.: 52.50%] [G loss: 0.746657]\n",
      "520 [D loss: 0.728268, acc.: 43.75%] [G loss: 0.810549]\n",
      "521 [D loss: 0.738116, acc.: 41.25%] [G loss: 0.801753]\n",
      "522 [D loss: 0.761342, acc.: 38.75%] [G loss: 0.788927]\n",
      "523 [D loss: 0.709231, acc.: 56.25%] [G loss: 0.797335]\n",
      "524 [D loss: 0.729644, acc.: 43.75%] [G loss: 0.762176]\n",
      "525 [D loss: 0.725643, acc.: 50.00%] [G loss: 0.779583]\n",
      "526 [D loss: 0.751743, acc.: 41.25%] [G loss: 0.804039]\n",
      "527 [D loss: 0.714374, acc.: 40.00%] [G loss: 0.822881]\n",
      "528 [D loss: 0.736606, acc.: 37.50%] [G loss: 0.831527]\n",
      "529 [D loss: 0.748574, acc.: 41.25%] [G loss: 0.783945]\n",
      "530 [D loss: 0.725822, acc.: 47.50%] [G loss: 0.835336]\n",
      "531 [D loss: 0.755851, acc.: 45.00%] [G loss: 0.800544]\n",
      "532 [D loss: 0.707050, acc.: 47.50%] [G loss: 0.809064]\n",
      "533 [D loss: 0.747996, acc.: 41.25%] [G loss: 0.823928]\n",
      "534 [D loss: 0.738753, acc.: 41.25%] [G loss: 0.818835]\n",
      "535 [D loss: 0.712371, acc.: 48.75%] [G loss: 0.798866]\n",
      "536 [D loss: 0.748600, acc.: 42.50%] [G loss: 0.786669]\n",
      "537 [D loss: 0.708354, acc.: 46.25%] [G loss: 0.789928]\n",
      "538 [D loss: 0.752829, acc.: 42.50%] [G loss: 0.827227]\n",
      "539 [D loss: 0.698888, acc.: 60.00%] [G loss: 0.797909]\n",
      "540 [D loss: 0.724326, acc.: 48.75%] [G loss: 0.767583]\n",
      "541 [D loss: 0.708827, acc.: 47.50%] [G loss: 0.788627]\n",
      "542 [D loss: 0.727223, acc.: 47.50%] [G loss: 0.772668]\n",
      "543 [D loss: 0.731734, acc.: 51.25%] [G loss: 0.755551]\n",
      "544 [D loss: 0.741636, acc.: 43.75%] [G loss: 0.811101]\n",
      "545 [D loss: 0.726515, acc.: 43.75%] [G loss: 0.741189]\n",
      "546 [D loss: 0.713416, acc.: 46.25%] [G loss: 0.805600]\n",
      "547 [D loss: 0.761735, acc.: 42.50%] [G loss: 0.841018]\n",
      "548 [D loss: 0.745840, acc.: 46.25%] [G loss: 0.834798]\n",
      "549 [D loss: 0.740257, acc.: 38.75%] [G loss: 0.801170]\n",
      "550 [D loss: 0.697792, acc.: 47.50%] [G loss: 0.808592]\n",
      "551 [D loss: 0.725346, acc.: 42.50%] [G loss: 0.755136]\n",
      "552 [D loss: 0.732873, acc.: 40.00%] [G loss: 0.730540]\n",
      "553 [D loss: 0.693391, acc.: 51.25%] [G loss: 0.774047]\n",
      "554 [D loss: 0.737723, acc.: 40.00%] [G loss: 0.779315]\n",
      "555 [D loss: 0.743588, acc.: 40.00%] [G loss: 0.766325]\n",
      "556 [D loss: 0.729170, acc.: 47.50%] [G loss: 0.748287]\n",
      "557 [D loss: 0.712804, acc.: 50.00%] [G loss: 0.797318]\n",
      "558 [D loss: 0.738275, acc.: 41.25%] [G loss: 0.746788]\n",
      "559 [D loss: 0.736474, acc.: 36.25%] [G loss: 0.817963]\n",
      "560 [D loss: 0.733974, acc.: 43.75%] [G loss: 0.793660]\n",
      "561 [D loss: 0.703029, acc.: 53.75%] [G loss: 0.775422]\n",
      "562 [D loss: 0.750346, acc.: 42.50%] [G loss: 0.770371]\n",
      "563 [D loss: 0.734172, acc.: 43.75%] [G loss: 0.776083]\n",
      "564 [D loss: 0.750681, acc.: 42.50%] [G loss: 0.758474]\n",
      "565 [D loss: 0.709681, acc.: 56.25%] [G loss: 0.796739]\n",
      "566 [D loss: 0.723171, acc.: 42.50%] [G loss: 0.751776]\n",
      "567 [D loss: 0.741017, acc.: 41.25%] [G loss: 0.831124]\n",
      "568 [D loss: 0.741638, acc.: 40.00%] [G loss: 0.815415]\n",
      "569 [D loss: 0.718914, acc.: 46.25%] [G loss: 0.796437]\n",
      "570 [D loss: 0.732790, acc.: 41.25%] [G loss: 0.792928]\n",
      "571 [D loss: 0.726463, acc.: 38.75%] [G loss: 0.746345]\n",
      "572 [D loss: 0.709839, acc.: 53.75%] [G loss: 0.769490]\n",
      "573 [D loss: 0.732917, acc.: 46.25%] [G loss: 0.784537]\n",
      "574 [D loss: 0.710693, acc.: 43.75%] [G loss: 0.820461]\n",
      "575 [D loss: 0.698455, acc.: 56.25%] [G loss: 0.687982]\n",
      "576 [D loss: 0.713633, acc.: 47.50%] [G loss: 0.753590]\n",
      "577 [D loss: 0.728128, acc.: 52.50%] [G loss: 0.788528]\n",
      "578 [D loss: 0.714691, acc.: 50.00%] [G loss: 0.778297]\n",
      "579 [D loss: 0.733139, acc.: 45.00%] [G loss: 0.751061]\n",
      "580 [D loss: 0.726415, acc.: 46.25%] [G loss: 0.760746]\n",
      "581 [D loss: 0.729568, acc.: 45.00%] [G loss: 0.728057]\n",
      "582 [D loss: 0.773703, acc.: 32.50%] [G loss: 0.790866]\n",
      "583 [D loss: 0.697357, acc.: 47.50%] [G loss: 0.733942]\n",
      "584 [D loss: 0.744370, acc.: 37.50%] [G loss: 0.768523]\n",
      "585 [D loss: 0.732633, acc.: 43.75%] [G loss: 0.799300]\n",
      "586 [D loss: 0.777969, acc.: 36.25%] [G loss: 0.745626]\n",
      "587 [D loss: 0.725156, acc.: 50.00%] [G loss: 0.740854]\n",
      "588 [D loss: 0.722242, acc.: 51.25%] [G loss: 0.780597]\n",
      "589 [D loss: 0.725218, acc.: 52.50%] [G loss: 0.804173]\n",
      "590 [D loss: 0.745150, acc.: 50.00%] [G loss: 0.784114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.756504, acc.: 40.00%] [G loss: 0.808268]\n",
      "592 [D loss: 0.719278, acc.: 40.00%] [G loss: 0.845229]\n",
      "593 [D loss: 0.749280, acc.: 43.75%] [G loss: 0.811628]\n",
      "594 [D loss: 0.731402, acc.: 46.25%] [G loss: 0.808179]\n",
      "595 [D loss: 0.728243, acc.: 42.50%] [G loss: 0.783468]\n",
      "596 [D loss: 0.749127, acc.: 40.00%] [G loss: 0.779215]\n",
      "597 [D loss: 0.723506, acc.: 50.00%] [G loss: 0.816638]\n",
      "598 [D loss: 0.706061, acc.: 48.75%] [G loss: 0.810441]\n",
      "599 [D loss: 0.734855, acc.: 41.25%] [G loss: 0.789597]\n",
      "600 [D loss: 0.727691, acc.: 47.50%] [G loss: 0.814744]\n",
      "601 [D loss: 0.712505, acc.: 45.00%] [G loss: 0.789998]\n",
      "602 [D loss: 0.744479, acc.: 43.75%] [G loss: 0.757590]\n",
      "603 [D loss: 0.732906, acc.: 46.25%] [G loss: 0.757306]\n",
      "604 [D loss: 0.734518, acc.: 38.75%] [G loss: 0.781717]\n",
      "605 [D loss: 0.697585, acc.: 47.50%] [G loss: 0.797658]\n",
      "606 [D loss: 0.722084, acc.: 51.25%] [G loss: 0.764922]\n",
      "607 [D loss: 0.767992, acc.: 40.00%] [G loss: 0.793518]\n",
      "608 [D loss: 0.689077, acc.: 55.00%] [G loss: 0.740257]\n",
      "609 [D loss: 0.746150, acc.: 40.00%] [G loss: 0.772653]\n",
      "610 [D loss: 0.720993, acc.: 51.25%] [G loss: 0.763421]\n",
      "611 [D loss: 0.723078, acc.: 48.75%] [G loss: 0.758167]\n",
      "612 [D loss: 0.735332, acc.: 47.50%] [G loss: 0.756860]\n",
      "613 [D loss: 0.703357, acc.: 51.25%] [G loss: 0.750819]\n",
      "614 [D loss: 0.751803, acc.: 38.75%] [G loss: 0.812698]\n",
      "615 [D loss: 0.714672, acc.: 50.00%] [G loss: 0.830720]\n",
      "616 [D loss: 0.735844, acc.: 43.75%] [G loss: 0.807202]\n",
      "617 [D loss: 0.703625, acc.: 48.75%] [G loss: 0.743246]\n",
      "618 [D loss: 0.725451, acc.: 45.00%] [G loss: 0.721433]\n",
      "619 [D loss: 0.719542, acc.: 43.75%] [G loss: 0.748411]\n",
      "620 [D loss: 0.712131, acc.: 52.50%] [G loss: 0.739274]\n",
      "621 [D loss: 0.742773, acc.: 45.00%] [G loss: 0.754418]\n",
      "622 [D loss: 0.748388, acc.: 48.75%] [G loss: 0.839075]\n",
      "623 [D loss: 0.716016, acc.: 48.75%] [G loss: 0.759103]\n",
      "624 [D loss: 0.708230, acc.: 52.50%] [G loss: 0.729258]\n",
      "625 [D loss: 0.726599, acc.: 42.50%] [G loss: 0.753960]\n",
      "626 [D loss: 0.746319, acc.: 41.25%] [G loss: 0.783190]\n",
      "627 [D loss: 0.723758, acc.: 45.00%] [G loss: 0.784751]\n",
      "628 [D loss: 0.740999, acc.: 38.75%] [G loss: 0.753717]\n",
      "629 [D loss: 0.751811, acc.: 43.75%] [G loss: 0.737463]\n",
      "630 [D loss: 0.715304, acc.: 53.75%] [G loss: 0.808121]\n",
      "631 [D loss: 0.721955, acc.: 45.00%] [G loss: 0.812546]\n",
      "632 [D loss: 0.762386, acc.: 33.75%] [G loss: 0.771074]\n",
      "633 [D loss: 0.707292, acc.: 52.50%] [G loss: 0.770995]\n",
      "634 [D loss: 0.693093, acc.: 55.00%] [G loss: 0.781914]\n",
      "635 [D loss: 0.741733, acc.: 42.50%] [G loss: 0.818310]\n",
      "636 [D loss: 0.727791, acc.: 45.00%] [G loss: 0.783780]\n",
      "637 [D loss: 0.752895, acc.: 42.50%] [G loss: 0.754533]\n",
      "638 [D loss: 0.694874, acc.: 53.75%] [G loss: 0.771313]\n",
      "639 [D loss: 0.712164, acc.: 48.75%] [G loss: 0.760419]\n",
      "640 [D loss: 0.713530, acc.: 51.25%] [G loss: 0.733196]\n",
      "641 [D loss: 0.739394, acc.: 47.50%] [G loss: 0.755265]\n",
      "642 [D loss: 0.717318, acc.: 43.75%] [G loss: 0.807575]\n",
      "643 [D loss: 0.748084, acc.: 42.50%] [G loss: 0.764262]\n",
      "644 [D loss: 0.694682, acc.: 53.75%] [G loss: 0.761513]\n",
      "645 [D loss: 0.702712, acc.: 55.00%] [G loss: 0.781882]\n",
      "646 [D loss: 0.731804, acc.: 48.75%] [G loss: 0.794688]\n",
      "647 [D loss: 0.722055, acc.: 42.50%] [G loss: 0.698907]\n",
      "648 [D loss: 0.737922, acc.: 47.50%] [G loss: 0.732837]\n",
      "649 [D loss: 0.716734, acc.: 43.75%] [G loss: 0.801157]\n",
      "650 [D loss: 0.713824, acc.: 42.50%] [G loss: 0.757740]\n",
      "651 [D loss: 0.743167, acc.: 45.00%] [G loss: 0.821280]\n",
      "652 [D loss: 0.706624, acc.: 50.00%] [G loss: 0.815833]\n",
      "653 [D loss: 0.733458, acc.: 47.50%] [G loss: 0.800087]\n",
      "654 [D loss: 0.713561, acc.: 47.50%] [G loss: 0.821211]\n",
      "655 [D loss: 0.717527, acc.: 50.00%] [G loss: 0.774442]\n",
      "656 [D loss: 0.723861, acc.: 43.75%] [G loss: 0.740000]\n",
      "657 [D loss: 0.748796, acc.: 38.75%] [G loss: 0.781169]\n",
      "658 [D loss: 0.709511, acc.: 47.50%] [G loss: 0.751791]\n",
      "659 [D loss: 0.745106, acc.: 41.25%] [G loss: 0.741972]\n",
      "660 [D loss: 0.724280, acc.: 50.00%] [G loss: 0.791156]\n",
      "661 [D loss: 0.737803, acc.: 38.75%] [G loss: 0.809031]\n",
      "662 [D loss: 0.713167, acc.: 47.50%] [G loss: 0.800276]\n",
      "663 [D loss: 0.698707, acc.: 52.50%] [G loss: 0.778936]\n",
      "664 [D loss: 0.719557, acc.: 52.50%] [G loss: 0.772455]\n",
      "665 [D loss: 0.762483, acc.: 38.75%] [G loss: 0.825093]\n",
      "666 [D loss: 0.748380, acc.: 41.25%] [G loss: 0.773095]\n",
      "667 [D loss: 0.735952, acc.: 48.75%] [G loss: 0.799914]\n",
      "668 [D loss: 0.739576, acc.: 41.25%] [G loss: 0.753450]\n",
      "669 [D loss: 0.734456, acc.: 45.00%] [G loss: 0.716765]\n",
      "670 [D loss: 0.710897, acc.: 42.50%] [G loss: 0.738837]\n",
      "671 [D loss: 0.728551, acc.: 43.75%] [G loss: 0.749273]\n",
      "672 [D loss: 0.726443, acc.: 45.00%] [G loss: 0.779477]\n",
      "673 [D loss: 0.722586, acc.: 47.50%] [G loss: 0.797194]\n",
      "674 [D loss: 0.717228, acc.: 43.75%] [G loss: 0.770408]\n",
      "675 [D loss: 0.723657, acc.: 48.75%] [G loss: 0.774351]\n",
      "676 [D loss: 0.729892, acc.: 42.50%] [G loss: 0.797555]\n",
      "677 [D loss: 0.737145, acc.: 40.00%] [G loss: 0.750940]\n",
      "678 [D loss: 0.741310, acc.: 41.25%] [G loss: 0.759296]\n",
      "679 [D loss: 0.716029, acc.: 47.50%] [G loss: 0.742541]\n",
      "680 [D loss: 0.741387, acc.: 40.00%] [G loss: 0.781907]\n",
      "681 [D loss: 0.722196, acc.: 45.00%] [G loss: 0.806431]\n",
      "682 [D loss: 0.723068, acc.: 51.25%] [G loss: 0.771955]\n",
      "683 [D loss: 0.740561, acc.: 41.25%] [G loss: 0.805888]\n",
      "684 [D loss: 0.716528, acc.: 47.50%] [G loss: 0.743411]\n",
      "685 [D loss: 0.727577, acc.: 47.50%] [G loss: 0.800131]\n",
      "686 [D loss: 0.712015, acc.: 52.50%] [G loss: 0.763655]\n",
      "687 [D loss: 0.732748, acc.: 41.25%] [G loss: 0.773701]\n",
      "688 [D loss: 0.703510, acc.: 51.25%] [G loss: 0.748147]\n",
      "689 [D loss: 0.729468, acc.: 46.25%] [G loss: 0.795232]\n",
      "690 [D loss: 0.737221, acc.: 43.75%] [G loss: 0.727757]\n",
      "691 [D loss: 0.724094, acc.: 45.00%] [G loss: 0.734948]\n",
      "692 [D loss: 0.728823, acc.: 46.25%] [G loss: 0.761185]\n",
      "693 [D loss: 0.768631, acc.: 45.00%] [G loss: 0.797755]\n",
      "694 [D loss: 0.711796, acc.: 48.75%] [G loss: 0.828799]\n",
      "695 [D loss: 0.737265, acc.: 45.00%] [G loss: 0.746509]\n",
      "696 [D loss: 0.729869, acc.: 46.25%] [G loss: 0.809174]\n",
      "697 [D loss: 0.705632, acc.: 45.00%] [G loss: 0.759882]\n",
      "698 [D loss: 0.690203, acc.: 53.75%] [G loss: 0.769480]\n",
      "699 [D loss: 0.707998, acc.: 51.25%] [G loss: 0.794720]\n",
      "700 [D loss: 0.736124, acc.: 42.50%] [G loss: 0.807316]\n",
      "701 [D loss: 0.719936, acc.: 46.25%] [G loss: 0.782752]\n",
      "702 [D loss: 0.721765, acc.: 46.25%] [G loss: 0.769842]\n",
      "703 [D loss: 0.759775, acc.: 37.50%] [G loss: 0.755059]\n",
      "704 [D loss: 0.735029, acc.: 47.50%] [G loss: 0.800233]\n",
      "705 [D loss: 0.725702, acc.: 43.75%] [G loss: 0.804979]\n",
      "706 [D loss: 0.724095, acc.: 47.50%] [G loss: 0.743080]\n",
      "707 [D loss: 0.776565, acc.: 41.25%] [G loss: 0.768569]\n",
      "708 [D loss: 0.703484, acc.: 50.00%] [G loss: 0.775184]\n",
      "709 [D loss: 0.740443, acc.: 47.50%] [G loss: 0.729153]\n",
      "710 [D loss: 0.720363, acc.: 52.50%] [G loss: 0.721342]\n",
      "711 [D loss: 0.727425, acc.: 45.00%] [G loss: 0.742807]\n",
      "712 [D loss: 0.711567, acc.: 51.25%] [G loss: 0.818321]\n",
      "713 [D loss: 0.704255, acc.: 53.75%] [G loss: 0.747748]\n",
      "714 [D loss: 0.724447, acc.: 43.75%] [G loss: 0.774960]\n",
      "715 [D loss: 0.749090, acc.: 38.75%] [G loss: 0.748483]\n",
      "716 [D loss: 0.748258, acc.: 43.75%] [G loss: 0.760079]\n",
      "717 [D loss: 0.730967, acc.: 37.50%] [G loss: 0.792367]\n",
      "718 [D loss: 0.701796, acc.: 52.50%] [G loss: 0.789537]\n",
      "719 [D loss: 0.743805, acc.: 41.25%] [G loss: 0.778346]\n",
      "720 [D loss: 0.746948, acc.: 47.50%] [G loss: 0.758333]\n",
      "721 [D loss: 0.703760, acc.: 58.75%] [G loss: 0.776871]\n",
      "722 [D loss: 0.717862, acc.: 53.75%] [G loss: 0.784057]\n",
      "723 [D loss: 0.711532, acc.: 50.00%] [G loss: 0.785001]\n",
      "724 [D loss: 0.706012, acc.: 47.50%] [G loss: 0.791766]\n",
      "725 [D loss: 0.729025, acc.: 45.00%] [G loss: 0.801937]\n",
      "726 [D loss: 0.736482, acc.: 40.00%] [G loss: 0.793583]\n",
      "727 [D loss: 0.768637, acc.: 32.50%] [G loss: 0.775612]\n",
      "728 [D loss: 0.723294, acc.: 51.25%] [G loss: 0.807406]\n",
      "729 [D loss: 0.687802, acc.: 53.75%] [G loss: 0.772648]\n",
      "730 [D loss: 0.728626, acc.: 45.00%] [G loss: 0.835401]\n",
      "731 [D loss: 0.724371, acc.: 46.25%] [G loss: 0.787218]\n",
      "732 [D loss: 0.739651, acc.: 43.75%] [G loss: 0.785189]\n",
      "733 [D loss: 0.766501, acc.: 38.75%] [G loss: 0.766771]\n",
      "734 [D loss: 0.707625, acc.: 51.25%] [G loss: 0.741703]\n",
      "735 [D loss: 0.717535, acc.: 48.75%] [G loss: 0.795327]\n",
      "736 [D loss: 0.743420, acc.: 42.50%] [G loss: 0.724529]\n",
      "737 [D loss: 0.726156, acc.: 48.75%] [G loss: 0.806698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 0.738346, acc.: 43.75%] [G loss: 0.790543]\n",
      "739 [D loss: 0.694362, acc.: 50.00%] [G loss: 0.725959]\n",
      "740 [D loss: 0.766979, acc.: 35.00%] [G loss: 0.839900]\n",
      "741 [D loss: 0.721536, acc.: 47.50%] [G loss: 0.779534]\n",
      "742 [D loss: 0.733537, acc.: 32.50%] [G loss: 0.789264]\n",
      "743 [D loss: 0.722675, acc.: 51.25%] [G loss: 0.770489]\n",
      "744 [D loss: 0.723121, acc.: 42.50%] [G loss: 0.747773]\n",
      "745 [D loss: 0.701876, acc.: 47.50%] [G loss: 0.763227]\n",
      "746 [D loss: 0.737218, acc.: 42.50%] [G loss: 0.763448]\n",
      "747 [D loss: 0.723898, acc.: 48.75%] [G loss: 0.760991]\n",
      "748 [D loss: 0.727372, acc.: 45.00%] [G loss: 0.785934]\n",
      "749 [D loss: 0.752865, acc.: 38.75%] [G loss: 0.747209]\n",
      "750 [D loss: 0.747640, acc.: 40.00%] [G loss: 0.758812]\n",
      "751 [D loss: 0.714162, acc.: 51.25%] [G loss: 0.825136]\n",
      "752 [D loss: 0.763275, acc.: 31.25%] [G loss: 0.791058]\n",
      "753 [D loss: 0.723528, acc.: 50.00%] [G loss: 0.800050]\n",
      "754 [D loss: 0.707989, acc.: 48.75%] [G loss: 0.771407]\n",
      "755 [D loss: 0.762448, acc.: 40.00%] [G loss: 0.737197]\n",
      "756 [D loss: 0.725369, acc.: 51.25%] [G loss: 0.744840]\n",
      "757 [D loss: 0.734904, acc.: 43.75%] [G loss: 0.785362]\n",
      "758 [D loss: 0.726128, acc.: 43.75%] [G loss: 0.827744]\n",
      "759 [D loss: 0.705800, acc.: 55.00%] [G loss: 0.762104]\n",
      "760 [D loss: 0.723045, acc.: 45.00%] [G loss: 0.774900]\n",
      "761 [D loss: 0.707615, acc.: 51.25%] [G loss: 0.783320]\n",
      "762 [D loss: 0.742065, acc.: 42.50%] [G loss: 0.796911]\n",
      "763 [D loss: 0.771280, acc.: 35.00%] [G loss: 0.769833]\n",
      "764 [D loss: 0.758504, acc.: 38.75%] [G loss: 0.788021]\n",
      "765 [D loss: 0.721842, acc.: 40.00%] [G loss: 0.774144]\n",
      "766 [D loss: 0.730476, acc.: 42.50%] [G loss: 0.750488]\n",
      "767 [D loss: 0.703310, acc.: 52.50%] [G loss: 0.761104]\n",
      "768 [D loss: 0.733834, acc.: 45.00%] [G loss: 0.774740]\n",
      "769 [D loss: 0.751194, acc.: 43.75%] [G loss: 0.733398]\n",
      "770 [D loss: 0.750714, acc.: 40.00%] [G loss: 0.838667]\n",
      "771 [D loss: 0.729923, acc.: 45.00%] [G loss: 0.790099]\n",
      "772 [D loss: 0.745933, acc.: 42.50%] [G loss: 0.732635]\n",
      "773 [D loss: 0.724649, acc.: 42.50%] [G loss: 0.780426]\n",
      "774 [D loss: 0.726011, acc.: 48.75%] [G loss: 0.789201]\n",
      "775 [D loss: 0.765825, acc.: 40.00%] [G loss: 0.814801]\n",
      "776 [D loss: 0.727143, acc.: 45.00%] [G loss: 0.825743]\n",
      "777 [D loss: 0.699834, acc.: 55.00%] [G loss: 0.789373]\n",
      "778 [D loss: 0.689353, acc.: 57.50%] [G loss: 0.766225]\n",
      "779 [D loss: 0.720570, acc.: 47.50%] [G loss: 0.737341]\n",
      "780 [D loss: 0.718784, acc.: 48.75%] [G loss: 0.826700]\n",
      "781 [D loss: 0.739566, acc.: 40.00%] [G loss: 0.796433]\n",
      "782 [D loss: 0.700416, acc.: 46.25%] [G loss: 0.757660]\n",
      "783 [D loss: 0.742808, acc.: 41.25%] [G loss: 0.764442]\n",
      "784 [D loss: 0.742628, acc.: 38.75%] [G loss: 0.806149]\n",
      "785 [D loss: 0.714689, acc.: 47.50%] [G loss: 0.793513]\n",
      "786 [D loss: 0.716811, acc.: 52.50%] [G loss: 0.790827]\n",
      "787 [D loss: 0.745329, acc.: 43.75%] [G loss: 0.763988]\n",
      "788 [D loss: 0.739488, acc.: 50.00%] [G loss: 0.711461]\n",
      "789 [D loss: 0.731569, acc.: 53.75%] [G loss: 0.785707]\n",
      "790 [D loss: 0.749430, acc.: 42.50%] [G loss: 0.840270]\n",
      "791 [D loss: 0.692010, acc.: 53.75%] [G loss: 0.810319]\n",
      "792 [D loss: 0.731420, acc.: 45.00%] [G loss: 0.823110]\n",
      "793 [D loss: 0.743675, acc.: 46.25%] [G loss: 0.716090]\n",
      "794 [D loss: 0.757695, acc.: 38.75%] [G loss: 0.772526]\n",
      "795 [D loss: 0.738110, acc.: 45.00%] [G loss: 0.828305]\n",
      "796 [D loss: 0.752773, acc.: 36.25%] [G loss: 0.771383]\n",
      "797 [D loss: 0.707082, acc.: 46.25%] [G loss: 0.775233]\n",
      "798 [D loss: 0.720074, acc.: 47.50%] [G loss: 0.749918]\n",
      "799 [D loss: 0.713412, acc.: 52.50%] [G loss: 0.774480]\n",
      "800 [D loss: 0.745121, acc.: 43.75%] [G loss: 0.742551]\n",
      "801 [D loss: 0.720975, acc.: 48.75%] [G loss: 0.758162]\n",
      "802 [D loss: 0.674017, acc.: 56.25%] [G loss: 0.779248]\n",
      "803 [D loss: 0.713506, acc.: 45.00%] [G loss: 0.774863]\n",
      "804 [D loss: 0.728217, acc.: 43.75%] [G loss: 0.769923]\n",
      "805 [D loss: 0.664360, acc.: 61.25%] [G loss: 0.801004]\n",
      "806 [D loss: 0.741420, acc.: 43.75%] [G loss: 0.784265]\n",
      "807 [D loss: 0.726575, acc.: 52.50%] [G loss: 0.784049]\n",
      "808 [D loss: 0.731949, acc.: 46.25%] [G loss: 0.785469]\n",
      "809 [D loss: 0.716806, acc.: 50.00%] [G loss: 0.765462]\n",
      "810 [D loss: 0.712662, acc.: 47.50%] [G loss: 0.808048]\n",
      "811 [D loss: 0.733817, acc.: 43.75%] [G loss: 0.755943]\n",
      "812 [D loss: 0.730919, acc.: 46.25%] [G loss: 0.762972]\n",
      "813 [D loss: 0.756847, acc.: 32.50%] [G loss: 0.793649]\n",
      "814 [D loss: 0.719688, acc.: 41.25%] [G loss: 0.752764]\n",
      "815 [D loss: 0.706707, acc.: 48.75%] [G loss: 0.807983]\n",
      "816 [D loss: 0.717147, acc.: 43.75%] [G loss: 0.767772]\n",
      "817 [D loss: 0.746264, acc.: 43.75%] [G loss: 0.803056]\n",
      "818 [D loss: 0.731735, acc.: 46.25%] [G loss: 0.744165]\n",
      "819 [D loss: 0.732717, acc.: 46.25%] [G loss: 0.810547]\n",
      "820 [D loss: 0.740440, acc.: 37.50%] [G loss: 0.735072]\n",
      "821 [D loss: 0.702464, acc.: 52.50%] [G loss: 0.778761]\n",
      "822 [D loss: 0.740728, acc.: 38.75%] [G loss: 0.792254]\n",
      "823 [D loss: 0.688645, acc.: 51.25%] [G loss: 0.782337]\n",
      "824 [D loss: 0.733292, acc.: 45.00%] [G loss: 0.771022]\n",
      "825 [D loss: 0.721094, acc.: 50.00%] [G loss: 0.791345]\n",
      "826 [D loss: 0.740895, acc.: 38.75%] [G loss: 0.801749]\n",
      "827 [D loss: 0.685529, acc.: 57.50%] [G loss: 0.760988]\n",
      "828 [D loss: 0.711534, acc.: 41.25%] [G loss: 0.771784]\n",
      "829 [D loss: 0.755098, acc.: 31.25%] [G loss: 0.761501]\n",
      "830 [D loss: 0.718300, acc.: 52.50%] [G loss: 0.767105]\n",
      "831 [D loss: 0.739609, acc.: 40.00%] [G loss: 0.764891]\n",
      "832 [D loss: 0.717665, acc.: 48.75%] [G loss: 0.756733]\n",
      "833 [D loss: 0.725265, acc.: 43.75%] [G loss: 0.737733]\n",
      "834 [D loss: 0.742249, acc.: 46.25%] [G loss: 0.802909]\n",
      "835 [D loss: 0.697514, acc.: 52.50%] [G loss: 0.803817]\n",
      "836 [D loss: 0.708000, acc.: 46.25%] [G loss: 0.785335]\n",
      "837 [D loss: 0.696285, acc.: 48.75%] [G loss: 0.755878]\n",
      "838 [D loss: 0.711313, acc.: 41.25%] [G loss: 0.727079]\n",
      "839 [D loss: 0.722696, acc.: 48.75%] [G loss: 0.738198]\n",
      "840 [D loss: 0.712671, acc.: 48.75%] [G loss: 0.763785]\n",
      "841 [D loss: 0.726539, acc.: 41.25%] [G loss: 0.778539]\n",
      "842 [D loss: 0.753885, acc.: 37.50%] [G loss: 0.784548]\n",
      "843 [D loss: 0.728727, acc.: 41.25%] [G loss: 0.758968]\n",
      "844 [D loss: 0.723497, acc.: 48.75%] [G loss: 0.750178]\n",
      "845 [D loss: 0.713744, acc.: 45.00%] [G loss: 0.767270]\n",
      "846 [D loss: 0.729704, acc.: 45.00%] [G loss: 0.737560]\n",
      "847 [D loss: 0.722239, acc.: 35.00%] [G loss: 0.779032]\n",
      "848 [D loss: 0.744949, acc.: 38.75%] [G loss: 0.826460]\n",
      "849 [D loss: 0.762655, acc.: 37.50%] [G loss: 0.829875]\n",
      "850 [D loss: 0.710128, acc.: 48.75%] [G loss: 0.753443]\n",
      "851 [D loss: 0.735371, acc.: 41.25%] [G loss: 0.767232]\n",
      "852 [D loss: 0.700377, acc.: 53.75%] [G loss: 0.786512]\n",
      "853 [D loss: 0.736607, acc.: 42.50%] [G loss: 0.764190]\n",
      "854 [D loss: 0.725728, acc.: 45.00%] [G loss: 0.761786]\n",
      "855 [D loss: 0.718606, acc.: 42.50%] [G loss: 0.736356]\n",
      "856 [D loss: 0.714688, acc.: 47.50%] [G loss: 0.740506]\n",
      "857 [D loss: 0.740070, acc.: 40.00%] [G loss: 0.781120]\n",
      "858 [D loss: 0.701769, acc.: 46.25%] [G loss: 0.768215]\n",
      "859 [D loss: 0.667585, acc.: 61.25%] [G loss: 0.774138]\n",
      "860 [D loss: 0.729990, acc.: 43.75%] [G loss: 0.716282]\n",
      "861 [D loss: 0.739297, acc.: 45.00%] [G loss: 0.779330]\n",
      "862 [D loss: 0.707968, acc.: 43.75%] [G loss: 0.764590]\n",
      "863 [D loss: 0.752319, acc.: 38.75%] [G loss: 0.770111]\n",
      "864 [D loss: 0.712267, acc.: 48.75%] [G loss: 0.726584]\n",
      "865 [D loss: 0.725772, acc.: 42.50%] [G loss: 0.759730]\n",
      "866 [D loss: 0.754534, acc.: 42.50%] [G loss: 0.803822]\n",
      "867 [D loss: 0.727113, acc.: 45.00%] [G loss: 0.762647]\n",
      "868 [D loss: 0.714979, acc.: 52.50%] [G loss: 0.799805]\n",
      "869 [D loss: 0.749758, acc.: 38.75%] [G loss: 0.784143]\n",
      "870 [D loss: 0.726860, acc.: 43.75%] [G loss: 0.748987]\n",
      "871 [D loss: 0.719772, acc.: 47.50%] [G loss: 0.736490]\n",
      "872 [D loss: 0.717543, acc.: 42.50%] [G loss: 0.775175]\n",
      "873 [D loss: 0.699029, acc.: 51.25%] [G loss: 0.832665]\n",
      "874 [D loss: 0.702761, acc.: 47.50%] [G loss: 0.770770]\n",
      "875 [D loss: 0.749296, acc.: 42.50%] [G loss: 0.783448]\n",
      "876 [D loss: 0.749861, acc.: 42.50%] [G loss: 0.760668]\n",
      "877 [D loss: 0.781326, acc.: 38.75%] [G loss: 0.781218]\n",
      "878 [D loss: 0.734370, acc.: 43.75%] [G loss: 0.793334]\n",
      "879 [D loss: 0.727370, acc.: 48.75%] [G loss: 0.809977]\n",
      "880 [D loss: 0.707258, acc.: 46.25%] [G loss: 0.800052]\n",
      "881 [D loss: 0.736496, acc.: 40.00%] [G loss: 0.783895]\n",
      "882 [D loss: 0.721158, acc.: 42.50%] [G loss: 0.775892]\n",
      "883 [D loss: 0.717087, acc.: 46.25%] [G loss: 0.748942]\n",
      "884 [D loss: 0.725683, acc.: 46.25%] [G loss: 0.786153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.744457, acc.: 43.75%] [G loss: 0.788950]\n",
      "886 [D loss: 0.733330, acc.: 43.75%] [G loss: 0.791884]\n",
      "887 [D loss: 0.710806, acc.: 52.50%] [G loss: 0.716539]\n",
      "888 [D loss: 0.757738, acc.: 35.00%] [G loss: 0.763314]\n",
      "889 [D loss: 0.756618, acc.: 37.50%] [G loss: 0.798935]\n",
      "890 [D loss: 0.744588, acc.: 41.25%] [G loss: 0.772416]\n",
      "891 [D loss: 0.709501, acc.: 47.50%] [G loss: 0.791540]\n",
      "892 [D loss: 0.740668, acc.: 42.50%] [G loss: 0.835621]\n",
      "893 [D loss: 0.712259, acc.: 51.25%] [G loss: 0.787794]\n",
      "894 [D loss: 0.728090, acc.: 48.75%] [G loss: 0.813519]\n",
      "895 [D loss: 0.727206, acc.: 43.75%] [G loss: 0.754462]\n",
      "896 [D loss: 0.739767, acc.: 33.75%] [G loss: 0.760702]\n",
      "897 [D loss: 0.717226, acc.: 48.75%] [G loss: 0.764627]\n",
      "898 [D loss: 0.737438, acc.: 43.75%] [G loss: 0.805173]\n",
      "899 [D loss: 0.738364, acc.: 40.00%] [G loss: 0.797612]\n",
      "900 [D loss: 0.741788, acc.: 45.00%] [G loss: 0.816182]\n",
      "901 [D loss: 0.745503, acc.: 42.50%] [G loss: 0.790426]\n",
      "902 [D loss: 0.715379, acc.: 48.75%] [G loss: 0.752500]\n",
      "903 [D loss: 0.724765, acc.: 41.25%] [G loss: 0.782997]\n",
      "904 [D loss: 0.718268, acc.: 43.75%] [G loss: 0.860768]\n",
      "905 [D loss: 0.726449, acc.: 50.00%] [G loss: 0.782077]\n",
      "906 [D loss: 0.736226, acc.: 48.75%] [G loss: 0.721875]\n",
      "907 [D loss: 0.717559, acc.: 47.50%] [G loss: 0.788445]\n",
      "908 [D loss: 0.733833, acc.: 38.75%] [G loss: 0.765794]\n",
      "909 [D loss: 0.722588, acc.: 43.75%] [G loss: 0.805778]\n",
      "910 [D loss: 0.726150, acc.: 43.75%] [G loss: 0.765176]\n",
      "911 [D loss: 0.737256, acc.: 45.00%] [G loss: 0.760417]\n",
      "912 [D loss: 0.764270, acc.: 25.00%] [G loss: 0.749733]\n",
      "913 [D loss: 0.697359, acc.: 56.25%] [G loss: 0.734623]\n",
      "914 [D loss: 0.708124, acc.: 52.50%] [G loss: 0.820039]\n",
      "915 [D loss: 0.717297, acc.: 45.00%] [G loss: 0.776180]\n",
      "916 [D loss: 0.719462, acc.: 38.75%] [G loss: 0.790632]\n",
      "917 [D loss: 0.745703, acc.: 46.25%] [G loss: 0.777565]\n",
      "918 [D loss: 0.720003, acc.: 38.75%] [G loss: 0.773565]\n",
      "919 [D loss: 0.744183, acc.: 45.00%] [G loss: 0.848005]\n",
      "920 [D loss: 0.747054, acc.: 37.50%] [G loss: 0.805997]\n",
      "921 [D loss: 0.700904, acc.: 48.75%] [G loss: 0.754224]\n",
      "922 [D loss: 0.751489, acc.: 38.75%] [G loss: 0.739586]\n",
      "923 [D loss: 0.705881, acc.: 51.25%] [G loss: 0.789668]\n",
      "924 [D loss: 0.756001, acc.: 43.75%] [G loss: 0.763063]\n",
      "925 [D loss: 0.709208, acc.: 46.25%] [G loss: 0.732395]\n",
      "926 [D loss: 0.737164, acc.: 45.00%] [G loss: 0.794913]\n",
      "927 [D loss: 0.726162, acc.: 38.75%] [G loss: 0.770266]\n",
      "928 [D loss: 0.735983, acc.: 43.75%] [G loss: 0.738892]\n",
      "929 [D loss: 0.715562, acc.: 45.00%] [G loss: 0.769137]\n",
      "930 [D loss: 0.720584, acc.: 53.75%] [G loss: 0.732917]\n",
      "931 [D loss: 0.745256, acc.: 42.50%] [G loss: 0.743699]\n",
      "932 [D loss: 0.747969, acc.: 37.50%] [G loss: 0.787943]\n",
      "933 [D loss: 0.732003, acc.: 43.75%] [G loss: 0.747403]\n",
      "934 [D loss: 0.712846, acc.: 47.50%] [G loss: 0.727166]\n",
      "935 [D loss: 0.725046, acc.: 50.00%] [G loss: 0.785184]\n",
      "936 [D loss: 0.750369, acc.: 42.50%] [G loss: 0.771193]\n",
      "937 [D loss: 0.714462, acc.: 52.50%] [G loss: 0.766149]\n",
      "938 [D loss: 0.734141, acc.: 43.75%] [G loss: 0.764502]\n",
      "939 [D loss: 0.726009, acc.: 45.00%] [G loss: 0.697740]\n",
      "940 [D loss: 0.714736, acc.: 48.75%] [G loss: 0.817752]\n",
      "941 [D loss: 0.729399, acc.: 40.00%] [G loss: 0.731783]\n",
      "942 [D loss: 0.723161, acc.: 45.00%] [G loss: 0.782955]\n",
      "943 [D loss: 0.711567, acc.: 43.75%] [G loss: 0.808344]\n",
      "944 [D loss: 0.713825, acc.: 50.00%] [G loss: 0.807713]\n",
      "945 [D loss: 0.731123, acc.: 50.00%] [G loss: 0.764852]\n",
      "946 [D loss: 0.759255, acc.: 36.25%] [G loss: 0.825004]\n",
      "947 [D loss: 0.718435, acc.: 46.25%] [G loss: 0.814880]\n",
      "948 [D loss: 0.766838, acc.: 40.00%] [G loss: 0.821019]\n",
      "949 [D loss: 0.719967, acc.: 45.00%] [G loss: 0.769904]\n",
      "950 [D loss: 0.715436, acc.: 48.75%] [G loss: 0.762968]\n",
      "951 [D loss: 0.707491, acc.: 52.50%] [G loss: 0.761995]\n",
      "952 [D loss: 0.717541, acc.: 42.50%] [G loss: 0.746326]\n",
      "953 [D loss: 0.740877, acc.: 40.00%] [G loss: 0.794419]\n",
      "954 [D loss: 0.726053, acc.: 43.75%] [G loss: 0.769768]\n",
      "955 [D loss: 0.698143, acc.: 53.75%] [G loss: 0.788519]\n",
      "956 [D loss: 0.718107, acc.: 42.50%] [G loss: 0.745341]\n",
      "957 [D loss: 0.708831, acc.: 48.75%] [G loss: 0.809678]\n",
      "958 [D loss: 0.697679, acc.: 48.75%] [G loss: 0.784494]\n",
      "959 [D loss: 0.728550, acc.: 38.75%] [G loss: 0.805097]\n",
      "960 [D loss: 0.744453, acc.: 45.00%] [G loss: 0.721698]\n",
      "961 [D loss: 0.732813, acc.: 38.75%] [G loss: 0.766537]\n",
      "962 [D loss: 0.718659, acc.: 46.25%] [G loss: 0.792168]\n",
      "963 [D loss: 0.727214, acc.: 45.00%] [G loss: 0.748937]\n",
      "964 [D loss: 0.699207, acc.: 52.50%] [G loss: 0.787349]\n",
      "965 [D loss: 0.764322, acc.: 32.50%] [G loss: 0.763552]\n",
      "966 [D loss: 0.751011, acc.: 35.00%] [G loss: 0.769052]\n",
      "967 [D loss: 0.724562, acc.: 48.75%] [G loss: 0.732341]\n",
      "968 [D loss: 0.765352, acc.: 38.75%] [G loss: 0.791023]\n",
      "969 [D loss: 0.699284, acc.: 50.00%] [G loss: 0.812665]\n",
      "970 [D loss: 0.730951, acc.: 42.50%] [G loss: 0.747725]\n",
      "971 [D loss: 0.707697, acc.: 42.50%] [G loss: 0.783949]\n",
      "972 [D loss: 0.703381, acc.: 48.75%] [G loss: 0.764512]\n",
      "973 [D loss: 0.716237, acc.: 51.25%] [G loss: 0.777598]\n",
      "974 [D loss: 0.733740, acc.: 42.50%] [G loss: 0.729307]\n",
      "975 [D loss: 0.734837, acc.: 46.25%] [G loss: 0.788227]\n",
      "976 [D loss: 0.701799, acc.: 52.50%] [G loss: 0.753474]\n",
      "977 [D loss: 0.742112, acc.: 42.50%] [G loss: 0.782291]\n",
      "978 [D loss: 0.693276, acc.: 53.75%] [G loss: 0.768731]\n",
      "979 [D loss: 0.727419, acc.: 46.25%] [G loss: 0.777102]\n",
      "980 [D loss: 0.728305, acc.: 46.25%] [G loss: 0.762197]\n",
      "981 [D loss: 0.712429, acc.: 51.25%] [G loss: 0.756856]\n",
      "982 [D loss: 0.718709, acc.: 42.50%] [G loss: 0.757589]\n",
      "983 [D loss: 0.699850, acc.: 45.00%] [G loss: 0.780821]\n",
      "984 [D loss: 0.706142, acc.: 45.00%] [G loss: 0.783580]\n",
      "985 [D loss: 0.735108, acc.: 41.25%] [G loss: 0.765636]\n",
      "986 [D loss: 0.712776, acc.: 48.75%] [G loss: 0.708301]\n",
      "987 [D loss: 0.720518, acc.: 46.25%] [G loss: 0.772639]\n",
      "988 [D loss: 0.730487, acc.: 48.75%] [G loss: 0.735226]\n",
      "989 [D loss: 0.720231, acc.: 48.75%] [G loss: 0.753226]\n",
      "990 [D loss: 0.733537, acc.: 46.25%] [G loss: 0.774805]\n",
      "991 [D loss: 0.704792, acc.: 48.75%] [G loss: 0.803971]\n",
      "992 [D loss: 0.720691, acc.: 47.50%] [G loss: 0.765180]\n",
      "993 [D loss: 0.728782, acc.: 43.75%] [G loss: 0.800015]\n",
      "994 [D loss: 0.727663, acc.: 38.75%] [G loss: 0.764521]\n",
      "995 [D loss: 0.718961, acc.: 47.50%] [G loss: 0.756741]\n",
      "996 [D loss: 0.706490, acc.: 51.25%] [G loss: 0.755918]\n",
      "997 [D loss: 0.720690, acc.: 52.50%] [G loss: 0.786484]\n",
      "998 [D loss: 0.733946, acc.: 46.25%] [G loss: 0.795269]\n",
      "999 [D loss: 0.725717, acc.: 46.25%] [G loss: 0.771323]\n",
      "1000 [D loss: 0.699057, acc.: 47.50%] [G loss: 0.761005]\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "lstmgan.train(epochs=1001, batch_size=40, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
