{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "        x_train = np.load('blues.npy',allow_pickle=True)\n",
    "        x_train = x_train.reshape(len(x_train),4,4)\n",
    "        return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 4\n",
    "        self.img_cols = 4\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 16\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(4,4))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        #encoder\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(4, 4)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(Dropout(0.2))\n",
    "        #specifying output to have 16 timesteps\n",
    "        model.add(RepeatVector(16))\n",
    "        #decoder\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(TimeDistributed(Dense(256)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(4,4))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True), input_shape=(16, 1)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(256)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(TimeDistributed(Dense(300)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(300)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(300)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=(16,1))\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "        \n",
    "        g_loss_epochs = np.zeros((epochs, 1))\n",
    "        d_loss_epochs = np.zeros((epochs, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),16,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size,4,4))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            \n",
    "            #save loss history\n",
    "            g_loss_epochs[epoch] = g_loss\n",
    "            d_loss_epochs[epoch] = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.generator.save(\"LSTM_generator2.h5\")\n",
    "        return g_loss_epochs, d_loss_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 16, 512)           528384    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 300)            153900    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 300)            90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 300)            90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 1, 1)              301       \n",
      "=================================================================\n",
      "Total params: 2,438,097\n",
      "Trainable params: 2,438,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 4, 256)            136192    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 16, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 16, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 16, 256)           65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 16, 1)             257       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 1)             0         \n",
      "=================================================================\n",
      "Total params: 1,384,961\n",
      "Trainable params: 1,384,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 6.329776, acc.: 50.00%] [G loss: 0.992736]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 3.965131, acc.: 50.00%] [G loss: 0.983331]\n",
      "2 [D loss: 2.579336, acc.: 50.00%] [G loss: 0.975786]\n",
      "3 [D loss: 1.854793, acc.: 50.00%] [G loss: 0.967728]\n",
      "4 [D loss: 1.766160, acc.: 50.00%] [G loss: 0.959747]\n",
      "5 [D loss: 1.599696, acc.: 50.00%] [G loss: 0.953548]\n",
      "6 [D loss: 1.466838, acc.: 50.00%] [G loss: 0.945630]\n",
      "7 [D loss: 1.404760, acc.: 50.00%] [G loss: 0.938912]\n",
      "8 [D loss: 1.349913, acc.: 50.00%] [G loss: 0.924370]\n",
      "9 [D loss: 1.332416, acc.: 50.00%] [G loss: 0.907913]\n",
      "10 [D loss: 1.268564, acc.: 50.00%] [G loss: 0.890704]\n",
      "11 [D loss: 1.197978, acc.: 50.00%] [G loss: 0.867377]\n",
      "12 [D loss: 1.123904, acc.: 50.00%] [G loss: 0.832422]\n",
      "13 [D loss: 1.134848, acc.: 50.00%] [G loss: 0.789734]\n",
      "14 [D loss: 1.066167, acc.: 50.00%] [G loss: 0.728232]\n",
      "15 [D loss: 1.065434, acc.: 50.00%] [G loss: 0.636531]\n",
      "16 [D loss: 1.096040, acc.: 48.12%] [G loss: 0.496772]\n",
      "17 [D loss: 1.135619, acc.: 38.12%] [G loss: 0.386252]\n",
      "18 [D loss: 1.498121, acc.: 21.25%] [G loss: 0.338997]\n",
      "19 [D loss: 1.428331, acc.: 20.62%] [G loss: 0.317775]\n",
      "20 [D loss: 2.191219, acc.: 10.63%] [G loss: 0.275313]\n",
      "21 [D loss: 1.726236, acc.: 11.25%] [G loss: 0.359669]\n",
      "22 [D loss: 1.461356, acc.: 33.13%] [G loss: 0.407544]\n",
      "23 [D loss: 1.499861, acc.: 35.00%] [G loss: 0.427695]\n",
      "24 [D loss: 1.464162, acc.: 40.62%] [G loss: 0.507458]\n",
      "25 [D loss: 1.420041, acc.: 45.00%] [G loss: 0.557444]\n",
      "26 [D loss: 1.343992, acc.: 47.50%] [G loss: 0.619611]\n",
      "27 [D loss: 1.327164, acc.: 49.38%] [G loss: 0.662732]\n",
      "28 [D loss: 1.268229, acc.: 49.38%] [G loss: 0.679445]\n",
      "29 [D loss: 1.250712, acc.: 50.00%] [G loss: 0.708367]\n",
      "30 [D loss: 1.236078, acc.: 49.38%] [G loss: 0.746051]\n",
      "31 [D loss: 1.168631, acc.: 49.38%] [G loss: 0.709196]\n",
      "32 [D loss: 1.165216, acc.: 49.38%] [G loss: 0.655241]\n",
      "33 [D loss: 1.118109, acc.: 50.00%] [G loss: 0.684958]\n",
      "34 [D loss: 1.105682, acc.: 50.00%] [G loss: 0.636120]\n",
      "35 [D loss: 1.099881, acc.: 50.00%] [G loss: 0.615139]\n",
      "36 [D loss: 1.056352, acc.: 48.75%] [G loss: 0.594482]\n",
      "37 [D loss: 1.054610, acc.: 50.00%] [G loss: 0.610188]\n",
      "38 [D loss: 1.051099, acc.: 50.00%] [G loss: 0.596490]\n",
      "39 [D loss: 1.024929, acc.: 49.38%] [G loss: 0.575898]\n",
      "40 [D loss: 0.978017, acc.: 48.75%] [G loss: 0.569068]\n",
      "41 [D loss: 0.969288, acc.: 48.75%] [G loss: 0.578835]\n",
      "42 [D loss: 0.939803, acc.: 50.00%] [G loss: 0.555779]\n",
      "43 [D loss: 0.951840, acc.: 49.38%] [G loss: 0.556757]\n",
      "44 [D loss: 0.919853, acc.: 50.00%] [G loss: 0.562883]\n",
      "45 [D loss: 0.925179, acc.: 47.50%] [G loss: 0.556441]\n",
      "46 [D loss: 0.898971, acc.: 49.38%] [G loss: 0.536532]\n",
      "47 [D loss: 0.895110, acc.: 48.75%] [G loss: 0.492789]\n",
      "48 [D loss: 0.868528, acc.: 49.38%] [G loss: 0.511355]\n",
      "49 [D loss: 0.858024, acc.: 48.12%] [G loss: 0.483107]\n",
      "50 [D loss: 0.841299, acc.: 48.12%] [G loss: 0.467280]\n",
      "51 [D loss: 0.841878, acc.: 46.88%] [G loss: 0.456328]\n",
      "52 [D loss: 0.811614, acc.: 44.37%] [G loss: 0.441369]\n",
      "53 [D loss: 0.836649, acc.: 45.63%] [G loss: 0.432858]\n",
      "54 [D loss: 0.806253, acc.: 44.37%] [G loss: 0.426710]\n",
      "55 [D loss: 0.789229, acc.: 49.38%] [G loss: 0.407198]\n",
      "56 [D loss: 0.833974, acc.: 41.25%] [G loss: 0.395668]\n",
      "57 [D loss: 0.774818, acc.: 43.75%] [G loss: 0.401939]\n",
      "58 [D loss: 0.820579, acc.: 40.62%] [G loss: 0.391681]\n",
      "59 [D loss: 0.783591, acc.: 45.62%] [G loss: 0.380496]\n",
      "60 [D loss: 0.802254, acc.: 39.37%] [G loss: 0.404086]\n",
      "61 [D loss: 0.786828, acc.: 40.62%] [G loss: 0.377096]\n",
      "62 [D loss: 0.794751, acc.: 41.25%] [G loss: 0.385723]\n",
      "63 [D loss: 0.810556, acc.: 38.12%] [G loss: 0.365863]\n",
      "64 [D loss: 0.764090, acc.: 42.50%] [G loss: 0.385284]\n",
      "65 [D loss: 0.765540, acc.: 50.63%] [G loss: 0.355920]\n",
      "66 [D loss: 0.755291, acc.: 45.00%] [G loss: 0.356944]\n",
      "67 [D loss: 0.767560, acc.: 44.38%] [G loss: 0.342720]\n",
      "68 [D loss: 0.758688, acc.: 42.50%] [G loss: 0.346016]\n",
      "69 [D loss: 0.780938, acc.: 43.13%] [G loss: 0.362163]\n",
      "70 [D loss: 0.747919, acc.: 48.12%] [G loss: 0.335247]\n",
      "71 [D loss: 0.760755, acc.: 43.13%] [G loss: 0.336366]\n",
      "72 [D loss: 0.769625, acc.: 44.37%] [G loss: 0.331487]\n",
      "73 [D loss: 0.816223, acc.: 33.75%] [G loss: 0.327174]\n",
      "74 [D loss: 0.762781, acc.: 45.63%] [G loss: 0.333654]\n",
      "75 [D loss: 0.744702, acc.: 49.38%] [G loss: 0.334853]\n",
      "76 [D loss: 0.743631, acc.: 42.50%] [G loss: 0.326836]\n",
      "77 [D loss: 0.758497, acc.: 46.25%] [G loss: 0.331818]\n",
      "78 [D loss: 0.749826, acc.: 40.00%] [G loss: 0.340439]\n",
      "79 [D loss: 0.773693, acc.: 39.37%] [G loss: 0.334815]\n",
      "80 [D loss: 0.787424, acc.: 36.25%] [G loss: 0.334075]\n",
      "81 [D loss: 0.748189, acc.: 40.00%] [G loss: 0.349178]\n",
      "82 [D loss: 0.739918, acc.: 44.37%] [G loss: 0.297559]\n",
      "83 [D loss: 0.745241, acc.: 50.63%] [G loss: 0.321656]\n",
      "84 [D loss: 0.735057, acc.: 47.50%] [G loss: 0.324882]\n",
      "85 [D loss: 0.761084, acc.: 35.62%] [G loss: 0.347421]\n",
      "86 [D loss: 0.750884, acc.: 41.87%] [G loss: 0.316657]\n",
      "87 [D loss: 0.731900, acc.: 46.88%] [G loss: 0.332331]\n",
      "88 [D loss: 0.766000, acc.: 42.50%] [G loss: 0.321330]\n",
      "89 [D loss: 0.779558, acc.: 40.62%] [G loss: 0.326676]\n",
      "90 [D loss: 0.768862, acc.: 42.50%] [G loss: 0.334554]\n",
      "91 [D loss: 0.745112, acc.: 47.50%] [G loss: 0.316720]\n",
      "92 [D loss: 0.758584, acc.: 40.62%] [G loss: 0.312708]\n",
      "93 [D loss: 0.776555, acc.: 39.37%] [G loss: 0.323521]\n",
      "94 [D loss: 0.742925, acc.: 47.50%] [G loss: 0.307188]\n",
      "95 [D loss: 0.777707, acc.: 35.62%] [G loss: 0.290631]\n",
      "96 [D loss: 0.765711, acc.: 40.62%] [G loss: 0.319135]\n",
      "97 [D loss: 0.761885, acc.: 38.12%] [G loss: 0.325807]\n",
      "98 [D loss: 0.768637, acc.: 41.87%] [G loss: 0.326434]\n",
      "99 [D loss: 0.771091, acc.: 40.62%] [G loss: 0.321642]\n",
      "100 [D loss: 0.754214, acc.: 40.62%] [G loss: 0.314085]\n",
      "101 [D loss: 0.766925, acc.: 45.00%] [G loss: 0.323651]\n",
      "102 [D loss: 0.743178, acc.: 40.62%] [G loss: 0.324898]\n",
      "103 [D loss: 0.752323, acc.: 44.37%] [G loss: 0.317003]\n",
      "104 [D loss: 0.754509, acc.: 43.12%] [G loss: 0.336433]\n",
      "105 [D loss: 0.747809, acc.: 46.25%] [G loss: 0.310486]\n",
      "106 [D loss: 0.756994, acc.: 41.25%] [G loss: 0.319601]\n",
      "107 [D loss: 0.736596, acc.: 40.62%] [G loss: 0.314730]\n",
      "108 [D loss: 0.735676, acc.: 44.37%] [G loss: 0.325203]\n",
      "109 [D loss: 0.755686, acc.: 43.75%] [G loss: 0.307453]\n",
      "110 [D loss: 0.754775, acc.: 38.75%] [G loss: 0.319368]\n",
      "111 [D loss: 0.724292, acc.: 49.38%] [G loss: 0.319446]\n",
      "112 [D loss: 0.728906, acc.: 45.00%] [G loss: 0.337055]\n",
      "113 [D loss: 0.768282, acc.: 41.87%] [G loss: 0.313162]\n",
      "114 [D loss: 0.737616, acc.: 48.75%] [G loss: 0.308327]\n",
      "115 [D loss: 0.748080, acc.: 41.87%] [G loss: 0.294667]\n",
      "116 [D loss: 0.760050, acc.: 45.63%] [G loss: 0.308061]\n",
      "117 [D loss: 0.767483, acc.: 39.38%] [G loss: 0.316087]\n",
      "118 [D loss: 0.742595, acc.: 44.38%] [G loss: 0.310740]\n",
      "119 [D loss: 0.742884, acc.: 46.25%] [G loss: 0.334873]\n",
      "120 [D loss: 0.744848, acc.: 40.62%] [G loss: 0.306430]\n",
      "121 [D loss: 0.745087, acc.: 44.37%] [G loss: 0.338386]\n",
      "122 [D loss: 0.750352, acc.: 45.00%] [G loss: 0.335442]\n",
      "123 [D loss: 0.743988, acc.: 46.25%] [G loss: 0.343143]\n",
      "124 [D loss: 0.835807, acc.: 49.37%] [G loss: 0.338702]\n",
      "125 [D loss: 0.772693, acc.: 43.13%] [G loss: 0.324946]\n",
      "126 [D loss: 0.742892, acc.: 44.37%] [G loss: 0.314424]\n",
      "127 [D loss: 0.737146, acc.: 46.25%] [G loss: 0.317586]\n",
      "128 [D loss: 0.758043, acc.: 41.25%] [G loss: 0.317306]\n",
      "129 [D loss: 0.775460, acc.: 35.00%] [G loss: 0.304844]\n",
      "130 [D loss: 0.765947, acc.: 35.00%] [G loss: 0.323312]\n",
      "131 [D loss: 0.727075, acc.: 51.88%] [G loss: 0.310712]\n",
      "132 [D loss: 0.756439, acc.: 43.13%] [G loss: 0.321497]\n",
      "133 [D loss: 0.741619, acc.: 41.25%] [G loss: 0.334876]\n",
      "134 [D loss: 0.752950, acc.: 48.75%] [G loss: 0.324508]\n",
      "135 [D loss: 0.726478, acc.: 47.50%] [G loss: 0.323213]\n",
      "136 [D loss: 0.747504, acc.: 42.50%] [G loss: 0.319658]\n",
      "137 [D loss: 0.726426, acc.: 43.12%] [G loss: 0.309902]\n",
      "138 [D loss: 0.753713, acc.: 41.87%] [G loss: 0.287244]\n",
      "139 [D loss: 0.750647, acc.: 41.87%] [G loss: 0.315279]\n",
      "140 [D loss: 0.731192, acc.: 48.12%] [G loss: 0.314721]\n",
      "141 [D loss: 0.759270, acc.: 38.12%] [G loss: 0.317310]\n",
      "142 [D loss: 0.747325, acc.: 41.25%] [G loss: 0.299045]\n",
      "143 [D loss: 0.752957, acc.: 42.50%] [G loss: 0.311218]\n",
      "144 [D loss: 0.739708, acc.: 48.75%] [G loss: 0.316030]\n",
      "145 [D loss: 0.733893, acc.: 43.75%] [G loss: 0.333918]\n",
      "146 [D loss: 0.724381, acc.: 49.38%] [G loss: 0.313143]\n",
      "147 [D loss: 0.758756, acc.: 38.75%] [G loss: 0.337711]\n",
      "148 [D loss: 0.750151, acc.: 41.25%] [G loss: 0.322099]\n",
      "149 [D loss: 0.748453, acc.: 42.50%] [G loss: 0.306260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.747883, acc.: 41.25%] [G loss: 0.308455]\n",
      "151 [D loss: 0.748091, acc.: 45.63%] [G loss: 0.320907]\n",
      "152 [D loss: 0.754858, acc.: 41.88%] [G loss: 0.295014]\n",
      "153 [D loss: 0.751035, acc.: 38.12%] [G loss: 0.297716]\n",
      "154 [D loss: 0.732105, acc.: 45.63%] [G loss: 0.311758]\n",
      "155 [D loss: 0.765972, acc.: 40.00%] [G loss: 0.312755]\n",
      "156 [D loss: 0.763481, acc.: 39.38%] [G loss: 0.321277]\n",
      "157 [D loss: 0.760051, acc.: 37.50%] [G loss: 0.329811]\n",
      "158 [D loss: 0.733112, acc.: 46.88%] [G loss: 0.309459]\n",
      "159 [D loss: 0.715792, acc.: 48.75%] [G loss: 0.317091]\n",
      "160 [D loss: 0.717586, acc.: 48.12%] [G loss: 0.311710]\n",
      "161 [D loss: 0.769477, acc.: 38.13%] [G loss: 0.313308]\n",
      "162 [D loss: 0.750765, acc.: 43.75%] [G loss: 0.288415]\n",
      "163 [D loss: 0.744260, acc.: 41.25%] [G loss: 0.308652]\n",
      "164 [D loss: 0.746955, acc.: 43.75%] [G loss: 0.305110]\n",
      "165 [D loss: 0.752315, acc.: 40.00%] [G loss: 0.312670]\n",
      "166 [D loss: 0.722809, acc.: 47.50%] [G loss: 0.301522]\n",
      "167 [D loss: 0.772371, acc.: 40.62%] [G loss: 0.315770]\n",
      "168 [D loss: 0.743333, acc.: 43.75%] [G loss: 0.315086]\n",
      "169 [D loss: 0.736234, acc.: 46.25%] [G loss: 0.299171]\n",
      "170 [D loss: 0.708536, acc.: 51.88%] [G loss: 0.307738]\n",
      "171 [D loss: 0.712374, acc.: 45.62%] [G loss: 0.311722]\n",
      "172 [D loss: 0.747273, acc.: 43.75%] [G loss: 0.302202]\n",
      "173 [D loss: 0.744077, acc.: 43.13%] [G loss: 0.310503]\n",
      "174 [D loss: 0.750431, acc.: 41.87%] [G loss: 0.306520]\n",
      "175 [D loss: 0.745108, acc.: 41.87%] [G loss: 0.312672]\n",
      "176 [D loss: 0.746554, acc.: 38.75%] [G loss: 0.303694]\n",
      "177 [D loss: 0.738871, acc.: 44.37%] [G loss: 0.311653]\n",
      "178 [D loss: 0.764044, acc.: 40.62%] [G loss: 0.315299]\n",
      "179 [D loss: 0.756827, acc.: 40.00%] [G loss: 0.302855]\n",
      "180 [D loss: 0.745230, acc.: 42.50%] [G loss: 0.304598]\n",
      "181 [D loss: 0.724330, acc.: 50.00%] [G loss: 0.305704]\n",
      "182 [D loss: 0.753471, acc.: 44.38%] [G loss: 0.304511]\n",
      "183 [D loss: 0.726779, acc.: 46.25%] [G loss: 0.306968]\n",
      "184 [D loss: 0.731300, acc.: 41.87%] [G loss: 0.312579]\n",
      "185 [D loss: 0.742733, acc.: 45.63%] [G loss: 0.294311]\n",
      "186 [D loss: 0.752055, acc.: 39.38%] [G loss: 0.317045]\n",
      "187 [D loss: 0.752030, acc.: 38.12%] [G loss: 0.318487]\n",
      "188 [D loss: 0.755298, acc.: 40.00%] [G loss: 0.317497]\n",
      "189 [D loss: 0.756351, acc.: 43.13%] [G loss: 0.320878]\n",
      "190 [D loss: 0.746156, acc.: 41.25%] [G loss: 0.315507]\n",
      "191 [D loss: 0.727344, acc.: 42.50%] [G loss: 0.309735]\n",
      "192 [D loss: 0.752864, acc.: 39.38%] [G loss: 0.302983]\n",
      "193 [D loss: 0.773345, acc.: 36.87%] [G loss: 0.304979]\n",
      "194 [D loss: 0.732462, acc.: 46.25%] [G loss: 0.310774]\n",
      "195 [D loss: 0.757129, acc.: 43.13%] [G loss: 0.310231]\n",
      "196 [D loss: 0.731425, acc.: 48.12%] [G loss: 0.315435]\n",
      "197 [D loss: 0.739577, acc.: 48.13%] [G loss: 0.296564]\n",
      "198 [D loss: 0.748402, acc.: 43.75%] [G loss: 0.313493]\n",
      "199 [D loss: 0.758766, acc.: 42.50%] [G loss: 0.302871]\n",
      "200 [D loss: 0.719830, acc.: 46.88%] [G loss: 0.295274]\n",
      "201 [D loss: 0.749197, acc.: 46.88%] [G loss: 0.308958]\n",
      "202 [D loss: 0.740560, acc.: 41.87%] [G loss: 0.298406]\n",
      "203 [D loss: 0.739998, acc.: 46.88%] [G loss: 0.313112]\n",
      "204 [D loss: 0.726050, acc.: 43.75%] [G loss: 0.293674]\n",
      "205 [D loss: 0.719299, acc.: 46.88%] [G loss: 0.293216]\n",
      "206 [D loss: 0.744313, acc.: 45.63%] [G loss: 0.315592]\n",
      "207 [D loss: 0.744580, acc.: 41.25%] [G loss: 0.305465]\n",
      "208 [D loss: 0.752486, acc.: 42.50%] [G loss: 0.302863]\n",
      "209 [D loss: 0.735178, acc.: 46.25%] [G loss: 0.327856]\n",
      "210 [D loss: 0.737998, acc.: 39.38%] [G loss: 0.297714]\n",
      "211 [D loss: 0.748464, acc.: 41.88%] [G loss: 0.313537]\n",
      "212 [D loss: 0.740682, acc.: 41.25%] [G loss: 0.299975]\n",
      "213 [D loss: 0.758973, acc.: 43.75%] [G loss: 0.294209]\n",
      "214 [D loss: 0.752065, acc.: 38.12%] [G loss: 0.301812]\n",
      "215 [D loss: 0.725494, acc.: 50.00%] [G loss: 0.287154]\n",
      "216 [D loss: 0.742899, acc.: 43.75%] [G loss: 0.296296]\n",
      "217 [D loss: 0.767910, acc.: 35.62%] [G loss: 0.294969]\n",
      "218 [D loss: 0.761155, acc.: 40.00%] [G loss: 0.301179]\n",
      "219 [D loss: 0.722885, acc.: 45.63%] [G loss: 0.310191]\n",
      "220 [D loss: 0.745815, acc.: 41.25%] [G loss: 0.311202]\n",
      "221 [D loss: 0.743219, acc.: 43.13%] [G loss: 0.323565]\n",
      "222 [D loss: 0.763722, acc.: 41.25%] [G loss: 0.300468]\n",
      "223 [D loss: 0.730644, acc.: 46.25%] [G loss: 0.318881]\n",
      "224 [D loss: 0.740403, acc.: 43.75%] [G loss: 0.306179]\n",
      "225 [D loss: 0.719256, acc.: 47.50%] [G loss: 0.278161]\n",
      "226 [D loss: 0.720738, acc.: 49.38%] [G loss: 0.287674]\n",
      "227 [D loss: 0.717570, acc.: 51.88%] [G loss: 0.293807]\n",
      "228 [D loss: 0.703850, acc.: 48.75%] [G loss: 0.280050]\n",
      "229 [D loss: 0.729585, acc.: 47.50%] [G loss: 0.288296]\n",
      "230 [D loss: 0.745607, acc.: 41.25%] [G loss: 0.295824]\n",
      "231 [D loss: 0.754870, acc.: 35.62%] [G loss: 0.309135]\n",
      "232 [D loss: 0.754192, acc.: 36.87%] [G loss: 0.313604]\n",
      "233 [D loss: 0.745995, acc.: 41.25%] [G loss: 0.330444]\n",
      "234 [D loss: 0.758649, acc.: 39.38%] [G loss: 0.318399]\n",
      "235 [D loss: 0.739726, acc.: 41.25%] [G loss: 0.310424]\n",
      "236 [D loss: 0.726921, acc.: 46.25%] [G loss: 0.314859]\n",
      "237 [D loss: 0.755254, acc.: 38.75%] [G loss: 0.298889]\n",
      "238 [D loss: 0.766412, acc.: 38.12%] [G loss: 0.304439]\n",
      "239 [D loss: 0.750777, acc.: 44.37%] [G loss: 0.300820]\n",
      "240 [D loss: 0.716540, acc.: 46.88%] [G loss: 0.309401]\n",
      "241 [D loss: 0.728280, acc.: 43.12%] [G loss: 0.283889]\n",
      "242 [D loss: 0.764654, acc.: 40.62%] [G loss: 0.295477]\n",
      "243 [D loss: 0.765742, acc.: 38.12%] [G loss: 0.300531]\n",
      "244 [D loss: 0.761735, acc.: 35.00%] [G loss: 0.320730]\n",
      "245 [D loss: 0.712316, acc.: 51.25%] [G loss: 0.320792]\n",
      "246 [D loss: 0.734630, acc.: 45.63%] [G loss: 0.319843]\n",
      "247 [D loss: 0.744063, acc.: 40.00%] [G loss: 0.337324]\n",
      "248 [D loss: 0.736255, acc.: 43.75%] [G loss: 0.329478]\n",
      "249 [D loss: 0.752907, acc.: 48.12%] [G loss: 0.311279]\n",
      "250 [D loss: 0.738946, acc.: 45.00%] [G loss: 0.294711]\n",
      "251 [D loss: 0.738696, acc.: 43.75%] [G loss: 0.304736]\n",
      "252 [D loss: 0.725403, acc.: 45.63%] [G loss: 0.288973]\n",
      "253 [D loss: 0.742483, acc.: 42.50%] [G loss: 0.290248]\n",
      "254 [D loss: 0.710579, acc.: 49.37%] [G loss: 0.275216]\n",
      "255 [D loss: 0.735090, acc.: 50.00%] [G loss: 0.286636]\n",
      "256 [D loss: 0.750303, acc.: 43.75%] [G loss: 0.298766]\n",
      "257 [D loss: 0.763275, acc.: 41.25%] [G loss: 0.287911]\n",
      "258 [D loss: 0.733845, acc.: 46.25%] [G loss: 0.308709]\n",
      "259 [D loss: 0.728059, acc.: 48.12%] [G loss: 0.321226]\n",
      "260 [D loss: 0.754509, acc.: 43.13%] [G loss: 0.321391]\n",
      "261 [D loss: 0.736733, acc.: 43.75%] [G loss: 0.337941]\n",
      "262 [D loss: 0.733335, acc.: 45.00%] [G loss: 0.316253]\n",
      "263 [D loss: 0.717631, acc.: 53.12%] [G loss: 0.332319]\n",
      "264 [D loss: 0.766315, acc.: 43.12%] [G loss: 0.314053]\n",
      "265 [D loss: 0.741820, acc.: 43.13%] [G loss: 0.285546]\n",
      "266 [D loss: 0.736022, acc.: 45.63%] [G loss: 0.282003]\n",
      "267 [D loss: 0.721922, acc.: 46.25%] [G loss: 0.287872]\n",
      "268 [D loss: 0.733722, acc.: 45.63%] [G loss: 0.293454]\n",
      "269 [D loss: 0.717093, acc.: 46.88%] [G loss: 0.287451]\n",
      "270 [D loss: 0.735249, acc.: 43.75%] [G loss: 0.291478]\n",
      "271 [D loss: 0.729116, acc.: 45.00%] [G loss: 0.281705]\n",
      "272 [D loss: 0.744710, acc.: 43.13%] [G loss: 0.288954]\n",
      "273 [D loss: 0.737100, acc.: 41.87%] [G loss: 0.295751]\n",
      "274 [D loss: 0.750671, acc.: 41.25%] [G loss: 0.296158]\n",
      "275 [D loss: 0.738861, acc.: 45.00%] [G loss: 0.325171]\n",
      "276 [D loss: 0.750753, acc.: 41.25%] [G loss: 0.340831]\n",
      "277 [D loss: 0.736386, acc.: 46.25%] [G loss: 0.324633]\n",
      "278 [D loss: 0.730921, acc.: 48.12%] [G loss: 0.322360]\n",
      "279 [D loss: 0.717194, acc.: 48.75%] [G loss: 0.302405]\n",
      "280 [D loss: 0.771166, acc.: 35.00%] [G loss: 0.312461]\n",
      "281 [D loss: 0.747582, acc.: 44.37%] [G loss: 0.299006]\n",
      "282 [D loss: 0.736609, acc.: 43.12%] [G loss: 0.300893]\n",
      "283 [D loss: 0.727760, acc.: 41.87%] [G loss: 0.280041]\n",
      "284 [D loss: 0.752684, acc.: 39.38%] [G loss: 0.290425]\n",
      "285 [D loss: 0.740324, acc.: 38.13%] [G loss: 0.282288]\n",
      "286 [D loss: 0.744924, acc.: 43.75%] [G loss: 0.296427]\n",
      "287 [D loss: 0.743989, acc.: 42.50%] [G loss: 0.292303]\n",
      "288 [D loss: 0.710835, acc.: 45.00%] [G loss: 0.277329]\n",
      "289 [D loss: 0.729857, acc.: 45.63%] [G loss: 0.271179]\n",
      "290 [D loss: 0.737182, acc.: 42.50%] [G loss: 0.294950]\n",
      "291 [D loss: 0.743703, acc.: 46.25%] [G loss: 0.287656]\n",
      "292 [D loss: 0.745885, acc.: 40.00%] [G loss: 0.321310]\n",
      "293 [D loss: 0.740673, acc.: 44.37%] [G loss: 0.332814]\n",
      "294 [D loss: 0.731507, acc.: 46.25%] [G loss: 0.349743]\n",
      "295 [D loss: 0.731941, acc.: 44.38%] [G loss: 0.327318]\n",
      "296 [D loss: 0.733859, acc.: 46.88%] [G loss: 0.338731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.728179, acc.: 48.75%] [G loss: 0.328107]\n",
      "298 [D loss: 0.752918, acc.: 45.63%] [G loss: 0.323237]\n",
      "299 [D loss: 0.736608, acc.: 45.63%] [G loss: 0.307872]\n",
      "300 [D loss: 0.717801, acc.: 48.12%] [G loss: 0.294191]\n",
      "301 [D loss: 0.752033, acc.: 39.38%] [G loss: 0.280144]\n",
      "302 [D loss: 0.744575, acc.: 39.38%] [G loss: 0.279876]\n",
      "303 [D loss: 0.756599, acc.: 38.13%] [G loss: 0.289382]\n",
      "304 [D loss: 0.711522, acc.: 46.25%] [G loss: 0.293333]\n",
      "305 [D loss: 0.719062, acc.: 49.37%] [G loss: 0.297652]\n",
      "306 [D loss: 0.733623, acc.: 42.50%] [G loss: 0.288458]\n",
      "307 [D loss: 0.745414, acc.: 39.38%] [G loss: 0.279201]\n",
      "308 [D loss: 0.741216, acc.: 41.87%] [G loss: 0.288602]\n",
      "309 [D loss: 0.743447, acc.: 41.25%] [G loss: 0.277170]\n",
      "310 [D loss: 0.759563, acc.: 38.13%] [G loss: 0.275399]\n",
      "311 [D loss: 0.731029, acc.: 41.87%] [G loss: 0.292054]\n",
      "312 [D loss: 0.744377, acc.: 40.62%] [G loss: 0.302809]\n",
      "313 [D loss: 0.758613, acc.: 41.25%] [G loss: 0.336176]\n",
      "314 [D loss: 0.743617, acc.: 43.13%] [G loss: 0.327271]\n",
      "315 [D loss: 0.745939, acc.: 47.50%] [G loss: 0.318618]\n",
      "316 [D loss: 0.749226, acc.: 42.50%] [G loss: 0.333552]\n",
      "317 [D loss: 0.745741, acc.: 41.25%] [G loss: 0.337917]\n",
      "318 [D loss: 0.724268, acc.: 48.12%] [G loss: 0.322992]\n",
      "319 [D loss: 0.712977, acc.: 53.12%] [G loss: 0.327772]\n",
      "320 [D loss: 0.745786, acc.: 45.00%] [G loss: 0.309429]\n",
      "321 [D loss: 0.764436, acc.: 39.38%] [G loss: 0.302114]\n",
      "322 [D loss: 0.728699, acc.: 41.87%] [G loss: 0.279745]\n",
      "323 [D loss: 0.732446, acc.: 42.50%] [G loss: 0.293692]\n",
      "324 [D loss: 0.715217, acc.: 51.25%] [G loss: 0.296121]\n",
      "325 [D loss: 0.759399, acc.: 41.87%] [G loss: 0.287647]\n",
      "326 [D loss: 0.744478, acc.: 38.13%] [G loss: 0.281961]\n",
      "327 [D loss: 0.714775, acc.: 45.63%] [G loss: 0.280152]\n",
      "328 [D loss: 0.702725, acc.: 50.00%] [G loss: 0.287451]\n",
      "329 [D loss: 0.704484, acc.: 55.62%] [G loss: 0.274413]\n",
      "330 [D loss: 0.735638, acc.: 40.62%] [G loss: 0.277808]\n",
      "331 [D loss: 0.727911, acc.: 44.38%] [G loss: 0.278616]\n",
      "332 [D loss: 0.727895, acc.: 43.13%] [G loss: 0.289218]\n",
      "333 [D loss: 0.748307, acc.: 37.50%] [G loss: 0.284705]\n",
      "334 [D loss: 0.733493, acc.: 43.75%] [G loss: 0.273866]\n",
      "335 [D loss: 0.741399, acc.: 40.00%] [G loss: 0.310576]\n",
      "336 [D loss: 0.739293, acc.: 44.37%] [G loss: 0.300149]\n",
      "337 [D loss: 0.734336, acc.: 46.88%] [G loss: 0.311886]\n",
      "338 [D loss: 0.757381, acc.: 40.00%] [G loss: 0.321032]\n",
      "339 [D loss: 0.725256, acc.: 48.12%] [G loss: 0.349825]\n",
      "340 [D loss: 0.723214, acc.: 46.25%] [G loss: 0.336268]\n",
      "341 [D loss: 0.716301, acc.: 47.50%] [G loss: 0.331534]\n",
      "342 [D loss: 0.723860, acc.: 45.62%] [G loss: 0.301408]\n",
      "343 [D loss: 0.711875, acc.: 56.25%] [G loss: 0.312719]\n",
      "344 [D loss: 0.763390, acc.: 36.88%] [G loss: 0.281376]\n",
      "345 [D loss: 0.746532, acc.: 41.87%] [G loss: 0.301363]\n",
      "346 [D loss: 0.715097, acc.: 46.25%] [G loss: 0.285510]\n",
      "347 [D loss: 0.726736, acc.: 45.63%] [G loss: 0.281456]\n",
      "348 [D loss: 0.697392, acc.: 51.88%] [G loss: 0.275853]\n",
      "349 [D loss: 0.735413, acc.: 46.25%] [G loss: 0.283545]\n",
      "350 [D loss: 0.725858, acc.: 46.25%] [G loss: 0.284402]\n",
      "351 [D loss: 0.726729, acc.: 47.50%] [G loss: 0.284404]\n",
      "352 [D loss: 0.693637, acc.: 51.87%] [G loss: 0.285115]\n",
      "353 [D loss: 0.731513, acc.: 45.00%] [G loss: 0.267139]\n",
      "354 [D loss: 0.711425, acc.: 49.38%] [G loss: 0.271830]\n",
      "355 [D loss: 0.729648, acc.: 42.50%] [G loss: 0.261756]\n",
      "356 [D loss: 0.724648, acc.: 47.50%] [G loss: 0.267879]\n",
      "357 [D loss: 0.754644, acc.: 37.50%] [G loss: 0.286568]\n",
      "358 [D loss: 0.714417, acc.: 51.87%] [G loss: 0.284572]\n",
      "359 [D loss: 0.758963, acc.: 41.87%] [G loss: 0.301850]\n",
      "360 [D loss: 0.780045, acc.: 35.00%] [G loss: 0.312199]\n",
      "361 [D loss: 0.711659, acc.: 49.37%] [G loss: 0.318231]\n",
      "362 [D loss: 0.725269, acc.: 48.75%] [G loss: 0.339262]\n",
      "363 [D loss: 0.748596, acc.: 43.13%] [G loss: 0.335869]\n",
      "364 [D loss: 0.732368, acc.: 47.50%] [G loss: 0.311929]\n",
      "365 [D loss: 0.725863, acc.: 44.37%] [G loss: 0.321665]\n",
      "366 [D loss: 0.727142, acc.: 47.50%] [G loss: 0.297947]\n",
      "367 [D loss: 0.724640, acc.: 43.13%] [G loss: 0.316825]\n",
      "368 [D loss: 0.746489, acc.: 38.75%] [G loss: 0.313010]\n",
      "369 [D loss: 0.730769, acc.: 48.13%] [G loss: 0.302703]\n",
      "370 [D loss: 0.735912, acc.: 44.37%] [G loss: 0.294428]\n",
      "371 [D loss: 0.764677, acc.: 41.87%] [G loss: 0.301785]\n",
      "372 [D loss: 0.743674, acc.: 38.75%] [G loss: 0.299755]\n",
      "373 [D loss: 0.743243, acc.: 43.13%] [G loss: 0.279532]\n",
      "374 [D loss: 0.733814, acc.: 40.62%] [G loss: 0.294839]\n",
      "375 [D loss: 0.747989, acc.: 39.38%] [G loss: 0.287647]\n",
      "376 [D loss: 0.700799, acc.: 50.63%] [G loss: 0.302486]\n",
      "377 [D loss: 0.737311, acc.: 41.87%] [G loss: 0.285274]\n",
      "378 [D loss: 0.727026, acc.: 44.37%] [G loss: 0.277879]\n",
      "379 [D loss: 0.718684, acc.: 43.12%] [G loss: 0.278474]\n",
      "380 [D loss: 0.714168, acc.: 49.37%] [G loss: 0.277568]\n",
      "381 [D loss: 0.711019, acc.: 51.88%] [G loss: 0.274242]\n",
      "382 [D loss: 0.732694, acc.: 40.62%] [G loss: 0.288653]\n",
      "383 [D loss: 0.734709, acc.: 45.00%] [G loss: 0.269945]\n",
      "384 [D loss: 0.744786, acc.: 43.75%] [G loss: 0.280659]\n",
      "385 [D loss: 0.745343, acc.: 40.62%] [G loss: 0.299910]\n",
      "386 [D loss: 0.723828, acc.: 43.75%] [G loss: 0.294174]\n",
      "387 [D loss: 0.739075, acc.: 43.75%] [G loss: 0.309737]\n",
      "388 [D loss: 0.760198, acc.: 42.50%] [G loss: 0.324996]\n",
      "389 [D loss: 0.714597, acc.: 49.37%] [G loss: 0.327924]\n",
      "390 [D loss: 0.706575, acc.: 52.50%] [G loss: 0.322384]\n",
      "391 [D loss: 0.736905, acc.: 46.88%] [G loss: 0.334129]\n",
      "392 [D loss: 0.715313, acc.: 48.75%] [G loss: 0.300733]\n",
      "393 [D loss: 0.741721, acc.: 40.00%] [G loss: 0.318656]\n",
      "394 [D loss: 0.724204, acc.: 48.75%] [G loss: 0.305896]\n",
      "395 [D loss: 0.733190, acc.: 46.88%] [G loss: 0.294254]\n",
      "396 [D loss: 0.735100, acc.: 43.13%] [G loss: 0.304863]\n",
      "397 [D loss: 0.737749, acc.: 42.50%] [G loss: 0.300602]\n",
      "398 [D loss: 0.738806, acc.: 41.25%] [G loss: 0.287648]\n",
      "399 [D loss: 0.745073, acc.: 42.50%] [G loss: 0.278484]\n",
      "400 [D loss: 0.738466, acc.: 42.50%] [G loss: 0.277802]\n",
      "401 [D loss: 0.722439, acc.: 46.88%] [G loss: 0.286066]\n",
      "402 [D loss: 0.729156, acc.: 44.38%] [G loss: 0.285512]\n",
      "403 [D loss: 0.714743, acc.: 41.25%] [G loss: 0.295668]\n",
      "404 [D loss: 0.741085, acc.: 41.25%] [G loss: 0.288045]\n",
      "405 [D loss: 0.716436, acc.: 47.50%] [G loss: 0.283584]\n",
      "406 [D loss: 0.697986, acc.: 52.50%] [G loss: 0.285103]\n",
      "407 [D loss: 0.711167, acc.: 53.12%] [G loss: 0.302335]\n",
      "408 [D loss: 0.728418, acc.: 46.88%] [G loss: 0.283707]\n",
      "409 [D loss: 0.723487, acc.: 44.38%] [G loss: 0.283801]\n",
      "410 [D loss: 0.743037, acc.: 38.75%] [G loss: 0.264364]\n",
      "411 [D loss: 0.735488, acc.: 45.63%] [G loss: 0.271125]\n",
      "412 [D loss: 0.739449, acc.: 43.75%] [G loss: 0.276109]\n",
      "413 [D loss: 0.738109, acc.: 44.37%] [G loss: 0.288111]\n",
      "414 [D loss: 0.726267, acc.: 49.38%] [G loss: 0.310086]\n",
      "415 [D loss: 0.748326, acc.: 43.13%] [G loss: 0.308185]\n",
      "416 [D loss: 0.729287, acc.: 46.88%] [G loss: 0.327897]\n",
      "417 [D loss: 0.712948, acc.: 48.75%] [G loss: 0.306467]\n",
      "418 [D loss: 0.754089, acc.: 40.62%] [G loss: 0.335808]\n",
      "419 [D loss: 0.725371, acc.: 49.38%] [G loss: 0.338284]\n",
      "420 [D loss: 0.735902, acc.: 40.00%] [G loss: 0.321387]\n",
      "421 [D loss: 0.734619, acc.: 44.37%] [G loss: 0.321979]\n",
      "422 [D loss: 0.730551, acc.: 43.75%] [G loss: 0.313852]\n",
      "423 [D loss: 0.720281, acc.: 44.37%] [G loss: 0.298193]\n",
      "424 [D loss: 0.721256, acc.: 46.88%] [G loss: 0.298285]\n",
      "425 [D loss: 0.751822, acc.: 38.75%] [G loss: 0.293200]\n",
      "426 [D loss: 0.737889, acc.: 43.12%] [G loss: 0.301062]\n",
      "427 [D loss: 0.736079, acc.: 45.00%] [G loss: 0.299903]\n",
      "428 [D loss: 0.716259, acc.: 45.63%] [G loss: 0.294482]\n",
      "429 [D loss: 0.719274, acc.: 47.50%] [G loss: 0.272330]\n",
      "430 [D loss: 0.704862, acc.: 51.87%] [G loss: 0.275087]\n",
      "431 [D loss: 0.725243, acc.: 45.00%] [G loss: 0.281355]\n",
      "432 [D loss: 0.741907, acc.: 40.62%] [G loss: 0.285001]\n",
      "433 [D loss: 0.714168, acc.: 41.87%] [G loss: 0.278992]\n",
      "434 [D loss: 0.710818, acc.: 46.88%] [G loss: 0.279759]\n",
      "435 [D loss: 0.720126, acc.: 45.63%] [G loss: 0.278168]\n",
      "436 [D loss: 0.699144, acc.: 51.88%] [G loss: 0.288311]\n",
      "437 [D loss: 0.717735, acc.: 47.50%] [G loss: 0.275932]\n",
      "438 [D loss: 0.732607, acc.: 48.13%] [G loss: 0.272111]\n",
      "439 [D loss: 0.736552, acc.: 45.00%] [G loss: 0.288552]\n",
      "440 [D loss: 0.719783, acc.: 44.37%] [G loss: 0.275665]\n",
      "441 [D loss: 0.736150, acc.: 43.13%] [G loss: 0.282819]\n",
      "442 [D loss: 0.723589, acc.: 46.25%] [G loss: 0.301840]\n",
      "443 [D loss: 0.709959, acc.: 49.37%] [G loss: 0.301750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 0.719914, acc.: 52.50%] [G loss: 0.293186]\n",
      "445 [D loss: 0.724152, acc.: 48.75%] [G loss: 0.311736]\n",
      "446 [D loss: 0.727647, acc.: 43.75%] [G loss: 0.317792]\n",
      "447 [D loss: 0.732114, acc.: 45.63%] [G loss: 0.320636]\n",
      "448 [D loss: 0.743513, acc.: 39.38%] [G loss: 0.316746]\n",
      "449 [D loss: 0.725820, acc.: 44.37%] [G loss: 0.307972]\n",
      "450 [D loss: 0.713667, acc.: 45.00%] [G loss: 0.304385]\n",
      "451 [D loss: 0.727626, acc.: 46.88%] [G loss: 0.303676]\n",
      "452 [D loss: 0.716545, acc.: 51.25%] [G loss: 0.306651]\n",
      "453 [D loss: 0.725338, acc.: 46.25%] [G loss: 0.282147]\n",
      "454 [D loss: 0.718252, acc.: 45.62%] [G loss: 0.264329]\n",
      "455 [D loss: 0.753180, acc.: 33.75%] [G loss: 0.282175]\n",
      "456 [D loss: 0.731217, acc.: 45.00%] [G loss: 0.279751]\n",
      "457 [D loss: 0.729408, acc.: 41.25%] [G loss: 0.292908]\n",
      "458 [D loss: 0.717643, acc.: 46.88%] [G loss: 0.289828]\n",
      "459 [D loss: 0.726744, acc.: 46.25%] [G loss: 0.253895]\n",
      "460 [D loss: 0.739065, acc.: 40.62%] [G loss: 0.282050]\n",
      "461 [D loss: 0.726146, acc.: 45.00%] [G loss: 0.279255]\n",
      "462 [D loss: 0.716694, acc.: 46.88%] [G loss: 0.285233]\n",
      "463 [D loss: 0.718156, acc.: 49.38%] [G loss: 0.290276]\n",
      "464 [D loss: 0.708028, acc.: 50.63%] [G loss: 0.294307]\n",
      "465 [D loss: 0.717440, acc.: 46.88%] [G loss: 0.281796]\n",
      "466 [D loss: 0.718564, acc.: 48.75%] [G loss: 0.263862]\n",
      "467 [D loss: 0.723586, acc.: 45.62%] [G loss: 0.274428]\n",
      "468 [D loss: 0.746221, acc.: 38.12%] [G loss: 0.272990]\n",
      "469 [D loss: 0.710137, acc.: 50.63%] [G loss: 0.276715]\n",
      "470 [D loss: 0.721334, acc.: 41.87%] [G loss: 0.257681]\n",
      "471 [D loss: 0.716895, acc.: 49.37%] [G loss: 0.270390]\n",
      "472 [D loss: 0.732486, acc.: 48.12%] [G loss: 0.286370]\n",
      "473 [D loss: 0.726328, acc.: 42.50%] [G loss: 0.270606]\n",
      "474 [D loss: 0.759353, acc.: 40.00%] [G loss: 0.307423]\n",
      "475 [D loss: 0.738968, acc.: 38.75%] [G loss: 0.309221]\n",
      "476 [D loss: 0.692506, acc.: 46.88%] [G loss: 0.317230]\n",
      "477 [D loss: 0.721492, acc.: 41.87%] [G loss: 0.324882]\n",
      "478 [D loss: 0.722937, acc.: 48.12%] [G loss: 0.320481]\n",
      "479 [D loss: 0.728951, acc.: 45.63%] [G loss: 0.298649]\n",
      "480 [D loss: 0.717132, acc.: 45.00%] [G loss: 0.300633]\n",
      "481 [D loss: 0.736895, acc.: 43.75%] [G loss: 0.298472]\n",
      "482 [D loss: 0.722040, acc.: 51.25%] [G loss: 0.290031]\n",
      "483 [D loss: 0.731675, acc.: 38.75%] [G loss: 0.295664]\n",
      "484 [D loss: 0.723659, acc.: 46.88%] [G loss: 0.281807]\n",
      "485 [D loss: 0.714842, acc.: 47.50%] [G loss: 0.286576]\n",
      "486 [D loss: 0.731924, acc.: 50.63%] [G loss: 0.304295]\n",
      "487 [D loss: 0.728329, acc.: 43.75%] [G loss: 0.282459]\n",
      "488 [D loss: 0.722285, acc.: 45.00%] [G loss: 0.276890]\n",
      "489 [D loss: 0.723053, acc.: 45.63%] [G loss: 0.280814]\n",
      "490 [D loss: 0.735029, acc.: 41.87%] [G loss: 0.286895]\n",
      "491 [D loss: 0.741765, acc.: 43.13%] [G loss: 0.284123]\n",
      "492 [D loss: 0.723511, acc.: 51.25%] [G loss: 0.267546]\n",
      "493 [D loss: 0.707588, acc.: 50.63%] [G loss: 0.279123]\n",
      "494 [D loss: 0.736772, acc.: 41.25%] [G loss: 0.284701]\n",
      "495 [D loss: 0.696125, acc.: 55.62%] [G loss: 0.277754]\n",
      "496 [D loss: 0.711112, acc.: 50.00%] [G loss: 0.275608]\n",
      "497 [D loss: 0.719297, acc.: 43.12%] [G loss: 0.266930]\n",
      "498 [D loss: 0.736890, acc.: 51.25%] [G loss: 0.262087]\n",
      "499 [D loss: 0.731161, acc.: 45.00%] [G loss: 0.283448]\n",
      "500 [D loss: 0.728581, acc.: 45.62%] [G loss: 0.266990]\n",
      "501 [D loss: 0.714932, acc.: 46.25%] [G loss: 0.274414]\n",
      "502 [D loss: 0.749329, acc.: 40.62%] [G loss: 0.269602]\n",
      "503 [D loss: 0.746705, acc.: 40.00%] [G loss: 0.269004]\n",
      "504 [D loss: 0.724645, acc.: 47.50%] [G loss: 0.297874]\n",
      "505 [D loss: 0.721092, acc.: 45.63%] [G loss: 0.305291]\n",
      "506 [D loss: 0.720460, acc.: 48.75%] [G loss: 0.302028]\n",
      "507 [D loss: 0.729083, acc.: 44.37%] [G loss: 0.306368]\n",
      "508 [D loss: 0.718738, acc.: 45.00%] [G loss: 0.316596]\n",
      "509 [D loss: 0.724685, acc.: 44.37%] [G loss: 0.312210]\n",
      "510 [D loss: 0.724034, acc.: 46.25%] [G loss: 0.313260]\n",
      "511 [D loss: 0.739113, acc.: 40.00%] [G loss: 0.324967]\n",
      "512 [D loss: 0.712462, acc.: 50.00%] [G loss: 0.320817]\n",
      "513 [D loss: 0.742571, acc.: 41.25%] [G loss: 0.302531]\n",
      "514 [D loss: 0.729486, acc.: 41.25%] [G loss: 0.300720]\n",
      "515 [D loss: 0.725432, acc.: 38.75%] [G loss: 0.294530]\n",
      "516 [D loss: 0.726567, acc.: 43.75%] [G loss: 0.296113]\n",
      "517 [D loss: 0.714995, acc.: 50.00%] [G loss: 0.268956]\n",
      "518 [D loss: 0.759310, acc.: 34.38%] [G loss: 0.286438]\n",
      "519 [D loss: 0.742257, acc.: 38.75%] [G loss: 0.296825]\n",
      "520 [D loss: 0.712421, acc.: 48.75%] [G loss: 0.291858]\n",
      "521 [D loss: 0.736355, acc.: 40.62%] [G loss: 0.297580]\n",
      "522 [D loss: 0.741184, acc.: 38.75%] [G loss: 0.302811]\n",
      "523 [D loss: 0.729939, acc.: 45.00%] [G loss: 0.274855]\n",
      "524 [D loss: 0.739000, acc.: 41.87%] [G loss: 0.287999]\n",
      "525 [D loss: 0.713919, acc.: 49.38%] [G loss: 0.278460]\n",
      "526 [D loss: 0.689555, acc.: 53.75%] [G loss: 0.291958]\n",
      "527 [D loss: 0.736763, acc.: 41.87%] [G loss: 0.275550]\n",
      "528 [D loss: 0.712519, acc.: 48.12%] [G loss: 0.272197]\n",
      "529 [D loss: 0.719951, acc.: 42.50%] [G loss: 0.281248]\n",
      "530 [D loss: 0.726512, acc.: 48.12%] [G loss: 0.274162]\n",
      "531 [D loss: 0.708100, acc.: 49.37%] [G loss: 0.278555]\n",
      "532 [D loss: 0.722605, acc.: 45.63%] [G loss: 0.258785]\n",
      "533 [D loss: 0.730259, acc.: 44.38%] [G loss: 0.248611]\n",
      "534 [D loss: 0.733396, acc.: 40.00%] [G loss: 0.273767]\n",
      "535 [D loss: 0.740939, acc.: 39.38%] [G loss: 0.271871]\n",
      "536 [D loss: 0.721481, acc.: 43.12%] [G loss: 0.282679]\n",
      "537 [D loss: 0.758710, acc.: 36.25%] [G loss: 0.285289]\n",
      "538 [D loss: 0.734079, acc.: 45.00%] [G loss: 0.295389]\n",
      "539 [D loss: 0.735407, acc.: 45.00%] [G loss: 0.331783]\n",
      "540 [D loss: 0.711140, acc.: 47.50%] [G loss: 0.325495]\n",
      "541 [D loss: 0.716758, acc.: 48.75%] [G loss: 0.327085]\n",
      "542 [D loss: 0.729232, acc.: 47.50%] [G loss: 0.316096]\n",
      "543 [D loss: 0.732046, acc.: 45.00%] [G loss: 0.312055]\n",
      "544 [D loss: 0.707007, acc.: 48.12%] [G loss: 0.329581]\n",
      "545 [D loss: 0.728628, acc.: 41.25%] [G loss: 0.299337]\n",
      "546 [D loss: 0.736110, acc.: 43.75%] [G loss: 0.299571]\n",
      "547 [D loss: 0.720942, acc.: 49.37%] [G loss: 0.281278]\n",
      "548 [D loss: 0.705332, acc.: 50.00%] [G loss: 0.292653]\n",
      "549 [D loss: 0.721165, acc.: 45.00%] [G loss: 0.290109]\n",
      "550 [D loss: 0.715267, acc.: 47.50%] [G loss: 0.280586]\n",
      "551 [D loss: 0.744423, acc.: 41.25%] [G loss: 0.276573]\n",
      "552 [D loss: 0.761211, acc.: 40.62%] [G loss: 0.286395]\n",
      "553 [D loss: 0.710452, acc.: 49.38%] [G loss: 0.293540]\n",
      "554 [D loss: 0.695569, acc.: 51.87%] [G loss: 0.273664]\n",
      "555 [D loss: 0.716673, acc.: 48.75%] [G loss: 0.283560]\n",
      "556 [D loss: 0.718517, acc.: 43.75%] [G loss: 0.285147]\n",
      "557 [D loss: 0.715529, acc.: 46.25%] [G loss: 0.283938]\n",
      "558 [D loss: 0.728558, acc.: 48.12%] [G loss: 0.287032]\n",
      "559 [D loss: 0.711940, acc.: 51.25%] [G loss: 0.281514]\n",
      "560 [D loss: 0.707508, acc.: 50.00%] [G loss: 0.284358]\n",
      "561 [D loss: 0.723138, acc.: 50.00%] [G loss: 0.276804]\n",
      "562 [D loss: 0.741423, acc.: 41.25%] [G loss: 0.277529]\n",
      "563 [D loss: 0.738679, acc.: 37.50%] [G loss: 0.281971]\n",
      "564 [D loss: 0.709986, acc.: 47.50%] [G loss: 0.277285]\n",
      "565 [D loss: 0.743271, acc.: 36.25%] [G loss: 0.279509]\n",
      "566 [D loss: 0.723554, acc.: 44.38%] [G loss: 0.289095]\n",
      "567 [D loss: 0.722316, acc.: 46.88%] [G loss: 0.279744]\n",
      "568 [D loss: 0.714829, acc.: 48.75%] [G loss: 0.295449]\n",
      "569 [D loss: 0.754316, acc.: 36.87%] [G loss: 0.278637]\n",
      "570 [D loss: 0.728137, acc.: 43.75%] [G loss: 0.294617]\n",
      "571 [D loss: 0.723077, acc.: 50.00%] [G loss: 0.293467]\n",
      "572 [D loss: 0.727419, acc.: 45.00%] [G loss: 0.310196]\n",
      "573 [D loss: 0.732163, acc.: 51.25%] [G loss: 0.314395]\n",
      "574 [D loss: 0.713728, acc.: 49.38%] [G loss: 0.311559]\n",
      "575 [D loss: 0.715134, acc.: 46.88%] [G loss: 0.315985]\n",
      "576 [D loss: 0.731143, acc.: 43.75%] [G loss: 0.299934]\n",
      "577 [D loss: 0.727016, acc.: 47.50%] [G loss: 0.297010]\n",
      "578 [D loss: 0.738072, acc.: 40.00%] [G loss: 0.294108]\n",
      "579 [D loss: 0.714453, acc.: 48.75%] [G loss: 0.308687]\n",
      "580 [D loss: 0.718821, acc.: 43.75%] [G loss: 0.290550]\n",
      "581 [D loss: 0.742528, acc.: 38.12%] [G loss: 0.299526]\n",
      "582 [D loss: 0.706803, acc.: 49.37%] [G loss: 0.292204]\n",
      "583 [D loss: 0.722344, acc.: 45.63%] [G loss: 0.287688]\n",
      "584 [D loss: 0.727677, acc.: 41.87%] [G loss: 0.287948]\n",
      "585 [D loss: 0.716765, acc.: 45.63%] [G loss: 0.279568]\n",
      "586 [D loss: 0.725952, acc.: 41.25%] [G loss: 0.271395]\n",
      "587 [D loss: 0.712924, acc.: 46.88%] [G loss: 0.279643]\n",
      "588 [D loss: 0.726634, acc.: 40.62%] [G loss: 0.287943]\n",
      "589 [D loss: 0.702854, acc.: 50.00%] [G loss: 0.276721]\n",
      "590 [D loss: 0.705743, acc.: 48.12%] [G loss: 0.272728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.716120, acc.: 45.63%] [G loss: 0.270075]\n",
      "592 [D loss: 0.704222, acc.: 50.00%] [G loss: 0.273951]\n",
      "593 [D loss: 0.722376, acc.: 49.38%] [G loss: 0.274327]\n",
      "594 [D loss: 0.703143, acc.: 52.50%] [G loss: 0.284487]\n",
      "595 [D loss: 0.718127, acc.: 50.00%] [G loss: 0.263249]\n",
      "596 [D loss: 0.727340, acc.: 41.87%] [G loss: 0.272236]\n",
      "597 [D loss: 0.726688, acc.: 48.75%] [G loss: 0.282256]\n",
      "598 [D loss: 0.718152, acc.: 45.00%] [G loss: 0.273895]\n",
      "599 [D loss: 0.746416, acc.: 40.00%] [G loss: 0.283937]\n",
      "600 [D loss: 0.755272, acc.: 37.50%] [G loss: 0.278736]\n",
      "601 [D loss: 0.751678, acc.: 36.25%] [G loss: 0.288194]\n",
      "602 [D loss: 0.734025, acc.: 43.12%] [G loss: 0.302125]\n",
      "603 [D loss: 0.728791, acc.: 43.13%] [G loss: 0.300576]\n",
      "604 [D loss: 0.722744, acc.: 48.75%] [G loss: 0.306012]\n",
      "605 [D loss: 0.728058, acc.: 48.75%] [G loss: 0.313484]\n",
      "606 [D loss: 0.705056, acc.: 46.88%] [G loss: 0.335909]\n",
      "607 [D loss: 0.724820, acc.: 44.37%] [G loss: 0.323327]\n",
      "608 [D loss: 0.707121, acc.: 46.88%] [G loss: 0.307898]\n",
      "609 [D loss: 0.725900, acc.: 47.50%] [G loss: 0.298836]\n",
      "610 [D loss: 0.737541, acc.: 42.50%] [G loss: 0.297214]\n",
      "611 [D loss: 0.707253, acc.: 51.25%] [G loss: 0.296807]\n",
      "612 [D loss: 0.731398, acc.: 40.00%] [G loss: 0.298339]\n",
      "613 [D loss: 0.720883, acc.: 48.12%] [G loss: 0.295055]\n",
      "614 [D loss: 0.715133, acc.: 46.88%] [G loss: 0.272655]\n",
      "615 [D loss: 0.731993, acc.: 41.87%] [G loss: 0.291895]\n",
      "616 [D loss: 0.717632, acc.: 51.25%] [G loss: 0.288956]\n",
      "617 [D loss: 0.721133, acc.: 40.62%] [G loss: 0.278841]\n",
      "618 [D loss: 0.729483, acc.: 41.87%] [G loss: 0.282488]\n",
      "619 [D loss: 0.713992, acc.: 48.75%] [G loss: 0.307341]\n",
      "620 [D loss: 0.711886, acc.: 45.63%] [G loss: 0.279688]\n",
      "621 [D loss: 0.735379, acc.: 40.00%] [G loss: 0.294781]\n",
      "622 [D loss: 0.699444, acc.: 48.12%] [G loss: 0.280207]\n",
      "623 [D loss: 0.723503, acc.: 47.50%] [G loss: 0.278660]\n",
      "624 [D loss: 0.711642, acc.: 48.12%] [G loss: 0.270926]\n",
      "625 [D loss: 0.716023, acc.: 46.25%] [G loss: 0.268833]\n",
      "626 [D loss: 0.708696, acc.: 45.00%] [G loss: 0.273051]\n",
      "627 [D loss: 0.736445, acc.: 41.87%] [G loss: 0.283751]\n",
      "628 [D loss: 0.714575, acc.: 46.88%] [G loss: 0.275497]\n",
      "629 [D loss: 0.716106, acc.: 45.00%] [G loss: 0.286091]\n",
      "630 [D loss: 0.706420, acc.: 47.50%] [G loss: 0.274376]\n",
      "631 [D loss: 0.732820, acc.: 46.88%] [G loss: 0.289529]\n",
      "632 [D loss: 0.716372, acc.: 45.00%] [G loss: 0.291078]\n",
      "633 [D loss: 0.713747, acc.: 50.00%] [G loss: 0.285427]\n",
      "634 [D loss: 0.717120, acc.: 46.88%] [G loss: 0.277003]\n",
      "635 [D loss: 0.735218, acc.: 39.38%] [G loss: 0.275284]\n",
      "636 [D loss: 0.723747, acc.: 46.88%] [G loss: 0.282877]\n",
      "637 [D loss: 0.727980, acc.: 45.63%] [G loss: 0.280233]\n",
      "638 [D loss: 0.733719, acc.: 38.75%] [G loss: 0.275442]\n",
      "639 [D loss: 0.728865, acc.: 43.75%] [G loss: 0.287235]\n",
      "640 [D loss: 0.720360, acc.: 43.13%] [G loss: 0.285182]\n",
      "641 [D loss: 0.726527, acc.: 41.87%] [G loss: 0.289517]\n",
      "642 [D loss: 0.719185, acc.: 43.75%] [G loss: 0.292462]\n",
      "643 [D loss: 0.749729, acc.: 36.88%] [G loss: 0.283619]\n",
      "644 [D loss: 0.721199, acc.: 41.87%] [G loss: 0.276887]\n",
      "645 [D loss: 0.726622, acc.: 43.13%] [G loss: 0.274774]\n",
      "646 [D loss: 0.730089, acc.: 41.87%] [G loss: 0.276781]\n",
      "647 [D loss: 0.712312, acc.: 45.00%] [G loss: 0.276735]\n",
      "648 [D loss: 0.723365, acc.: 45.00%] [G loss: 0.273960]\n",
      "649 [D loss: 0.710985, acc.: 48.75%] [G loss: 0.274117]\n",
      "650 [D loss: 0.723799, acc.: 47.50%] [G loss: 0.277915]\n",
      "651 [D loss: 0.715397, acc.: 45.62%] [G loss: 0.289943]\n",
      "652 [D loss: 0.722276, acc.: 45.00%] [G loss: 0.305522]\n",
      "653 [D loss: 0.702955, acc.: 52.50%] [G loss: 0.275747]\n",
      "654 [D loss: 0.718405, acc.: 43.75%] [G loss: 0.300286]\n",
      "655 [D loss: 0.710294, acc.: 42.50%] [G loss: 0.276000]\n",
      "656 [D loss: 0.726692, acc.: 39.38%] [G loss: 0.273347]\n",
      "657 [D loss: 0.727852, acc.: 43.75%] [G loss: 0.278653]\n",
      "658 [D loss: 0.711457, acc.: 50.63%] [G loss: 0.294982]\n",
      "659 [D loss: 0.724002, acc.: 45.63%] [G loss: 0.288474]\n",
      "660 [D loss: 0.711902, acc.: 44.37%] [G loss: 0.281557]\n",
      "661 [D loss: 0.721310, acc.: 43.13%] [G loss: 0.281547]\n",
      "662 [D loss: 0.710802, acc.: 51.25%] [G loss: 0.291881]\n",
      "663 [D loss: 0.710793, acc.: 46.25%] [G loss: 0.294905]\n",
      "664 [D loss: 0.728313, acc.: 38.75%] [G loss: 0.277756]\n",
      "665 [D loss: 0.731550, acc.: 46.25%] [G loss: 0.291007]\n",
      "666 [D loss: 0.708548, acc.: 53.75%] [G loss: 0.273005]\n",
      "667 [D loss: 0.722536, acc.: 50.00%] [G loss: 0.284481]\n",
      "668 [D loss: 0.740626, acc.: 36.87%] [G loss: 0.279572]\n",
      "669 [D loss: 0.741462, acc.: 38.13%] [G loss: 0.276253]\n",
      "670 [D loss: 0.709344, acc.: 48.12%] [G loss: 0.284640]\n",
      "671 [D loss: 0.710271, acc.: 48.75%] [G loss: 0.279992]\n",
      "672 [D loss: 0.724352, acc.: 46.25%] [G loss: 0.273772]\n",
      "673 [D loss: 0.704753, acc.: 50.63%] [G loss: 0.280219]\n",
      "674 [D loss: 0.732123, acc.: 43.75%] [G loss: 0.266132]\n",
      "675 [D loss: 0.722710, acc.: 45.63%] [G loss: 0.277596]\n",
      "676 [D loss: 0.712654, acc.: 50.00%] [G loss: 0.278326]\n",
      "677 [D loss: 0.731420, acc.: 43.12%] [G loss: 0.265231]\n",
      "678 [D loss: 0.716397, acc.: 43.13%] [G loss: 0.297673]\n",
      "679 [D loss: 0.705221, acc.: 51.25%] [G loss: 0.284224]\n",
      "680 [D loss: 0.731846, acc.: 37.50%] [G loss: 0.302278]\n",
      "681 [D loss: 0.711540, acc.: 48.13%] [G loss: 0.288959]\n",
      "682 [D loss: 0.721067, acc.: 43.12%] [G loss: 0.278971]\n",
      "683 [D loss: 0.711194, acc.: 47.50%] [G loss: 0.281569]\n",
      "684 [D loss: 0.723199, acc.: 46.25%] [G loss: 0.297125]\n",
      "685 [D loss: 0.720771, acc.: 47.50%] [G loss: 0.295384]\n",
      "686 [D loss: 0.735400, acc.: 38.75%] [G loss: 0.279021]\n",
      "687 [D loss: 0.717472, acc.: 44.37%] [G loss: 0.283173]\n",
      "688 [D loss: 0.739013, acc.: 44.38%] [G loss: 0.295870]\n",
      "689 [D loss: 0.722963, acc.: 46.25%] [G loss: 0.283311]\n",
      "690 [D loss: 0.730844, acc.: 43.75%] [G loss: 0.266622]\n",
      "691 [D loss: 0.719414, acc.: 48.13%] [G loss: 0.284364]\n",
      "692 [D loss: 0.712926, acc.: 48.75%] [G loss: 0.285095]\n",
      "693 [D loss: 0.724000, acc.: 45.00%] [G loss: 0.267703]\n",
      "694 [D loss: 0.720366, acc.: 45.00%] [G loss: 0.273273]\n",
      "695 [D loss: 0.726839, acc.: 43.75%] [G loss: 0.273949]\n",
      "696 [D loss: 0.703048, acc.: 49.38%] [G loss: 0.278318]\n",
      "697 [D loss: 0.709131, acc.: 49.37%] [G loss: 0.275499]\n",
      "698 [D loss: 0.716211, acc.: 45.63%] [G loss: 0.267602]\n",
      "699 [D loss: 0.714989, acc.: 43.75%] [G loss: 0.278882]\n",
      "700 [D loss: 0.730193, acc.: 44.38%] [G loss: 0.296698]\n",
      "701 [D loss: 0.711527, acc.: 45.63%] [G loss: 0.274155]\n",
      "702 [D loss: 0.733384, acc.: 41.25%] [G loss: 0.278195]\n",
      "703 [D loss: 0.723826, acc.: 43.75%] [G loss: 0.283604]\n",
      "704 [D loss: 0.735186, acc.: 43.13%] [G loss: 0.287639]\n",
      "705 [D loss: 0.715148, acc.: 48.12%] [G loss: 0.290217]\n",
      "706 [D loss: 0.740898, acc.: 36.87%] [G loss: 0.286007]\n",
      "707 [D loss: 0.715088, acc.: 47.50%] [G loss: 0.294176]\n",
      "708 [D loss: 0.717981, acc.: 40.62%] [G loss: 0.303888]\n",
      "709 [D loss: 0.714321, acc.: 50.00%] [G loss: 0.288011]\n",
      "710 [D loss: 0.705490, acc.: 49.37%] [G loss: 0.284564]\n",
      "711 [D loss: 0.711972, acc.: 46.25%] [G loss: 0.294773]\n",
      "712 [D loss: 0.751929, acc.: 40.00%] [G loss: 0.292971]\n",
      "713 [D loss: 0.708911, acc.: 46.25%] [G loss: 0.268956]\n",
      "714 [D loss: 0.724209, acc.: 48.12%] [G loss: 0.292081]\n",
      "715 [D loss: 0.744098, acc.: 36.87%] [G loss: 0.274931]\n",
      "716 [D loss: 0.690759, acc.: 57.50%] [G loss: 0.287571]\n",
      "717 [D loss: 0.711002, acc.: 45.00%] [G loss: 0.278853]\n",
      "718 [D loss: 0.710975, acc.: 51.25%] [G loss: 0.274285]\n",
      "719 [D loss: 0.719353, acc.: 45.00%] [G loss: 0.278128]\n",
      "720 [D loss: 0.733824, acc.: 41.87%] [G loss: 0.269589]\n",
      "721 [D loss: 0.735560, acc.: 43.13%] [G loss: 0.268796]\n",
      "722 [D loss: 0.710353, acc.: 48.75%] [G loss: 0.284425]\n",
      "723 [D loss: 0.725945, acc.: 43.13%] [G loss: 0.264960]\n",
      "724 [D loss: 0.727377, acc.: 42.50%] [G loss: 0.287483]\n",
      "725 [D loss: 0.713757, acc.: 48.12%] [G loss: 0.291597]\n",
      "726 [D loss: 0.702458, acc.: 52.50%] [G loss: 0.294999]\n",
      "727 [D loss: 0.710560, acc.: 49.38%] [G loss: 0.271051]\n",
      "728 [D loss: 0.731592, acc.: 47.50%] [G loss: 0.285544]\n",
      "729 [D loss: 0.719230, acc.: 48.13%] [G loss: 0.297798]\n",
      "730 [D loss: 0.719388, acc.: 41.25%] [G loss: 0.282316]\n",
      "731 [D loss: 0.718772, acc.: 44.38%] [G loss: 0.275846]\n",
      "732 [D loss: 0.705950, acc.: 46.88%] [G loss: 0.300240]\n",
      "733 [D loss: 0.713244, acc.: 45.63%] [G loss: 0.281595]\n",
      "734 [D loss: 0.716409, acc.: 43.12%] [G loss: 0.290182]\n",
      "735 [D loss: 0.716953, acc.: 46.25%] [G loss: 0.294303]\n",
      "736 [D loss: 0.717365, acc.: 52.50%] [G loss: 0.277568]\n",
      "737 [D loss: 0.731393, acc.: 42.50%] [G loss: 0.278805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 0.710531, acc.: 51.25%] [G loss: 0.283675]\n",
      "739 [D loss: 0.736104, acc.: 43.13%] [G loss: 0.266891]\n",
      "740 [D loss: 0.715184, acc.: 46.25%] [G loss: 0.287714]\n",
      "741 [D loss: 0.738205, acc.: 39.38%] [G loss: 0.285682]\n",
      "742 [D loss: 0.718916, acc.: 45.62%] [G loss: 0.282610]\n",
      "743 [D loss: 0.709053, acc.: 46.25%] [G loss: 0.287585]\n",
      "744 [D loss: 0.717538, acc.: 43.75%] [G loss: 0.291341]\n",
      "745 [D loss: 0.733650, acc.: 43.75%] [G loss: 0.283884]\n",
      "746 [D loss: 0.710442, acc.: 47.50%] [G loss: 0.271519]\n",
      "747 [D loss: 0.719635, acc.: 41.87%] [G loss: 0.287503]\n",
      "748 [D loss: 0.709067, acc.: 48.75%] [G loss: 0.287196]\n",
      "749 [D loss: 0.720158, acc.: 46.88%] [G loss: 0.288267]\n",
      "750 [D loss: 0.705363, acc.: 51.88%] [G loss: 0.292014]\n",
      "751 [D loss: 0.733866, acc.: 36.88%] [G loss: 0.281070]\n",
      "752 [D loss: 0.702361, acc.: 49.38%] [G loss: 0.295603]\n",
      "753 [D loss: 0.713989, acc.: 43.75%] [G loss: 0.293414]\n",
      "754 [D loss: 0.729674, acc.: 41.25%] [G loss: 0.284656]\n",
      "755 [D loss: 0.710352, acc.: 45.63%] [G loss: 0.287689]\n",
      "756 [D loss: 0.709174, acc.: 44.37%] [G loss: 0.282642]\n",
      "757 [D loss: 0.722212, acc.: 41.87%] [G loss: 0.266317]\n",
      "758 [D loss: 0.719586, acc.: 46.88%] [G loss: 0.278748]\n",
      "759 [D loss: 0.697840, acc.: 47.50%] [G loss: 0.276951]\n",
      "760 [D loss: 0.729871, acc.: 41.87%] [G loss: 0.266265]\n",
      "761 [D loss: 0.702121, acc.: 53.75%] [G loss: 0.260540]\n",
      "762 [D loss: 0.711157, acc.: 47.50%] [G loss: 0.262654]\n",
      "763 [D loss: 0.707705, acc.: 50.00%] [G loss: 0.262939]\n",
      "764 [D loss: 0.708131, acc.: 49.37%] [G loss: 0.279428]\n",
      "765 [D loss: 0.724407, acc.: 43.75%] [G loss: 0.278516]\n",
      "766 [D loss: 0.724425, acc.: 41.87%] [G loss: 0.282766]\n",
      "767 [D loss: 0.680147, acc.: 54.37%] [G loss: 0.271852]\n",
      "768 [D loss: 0.719891, acc.: 45.62%] [G loss: 0.283605]\n",
      "769 [D loss: 0.744136, acc.: 42.50%] [G loss: 0.282000]\n",
      "770 [D loss: 0.715743, acc.: 45.63%] [G loss: 0.273535]\n",
      "771 [D loss: 0.714814, acc.: 47.50%] [G loss: 0.291110]\n",
      "772 [D loss: 0.729895, acc.: 45.00%] [G loss: 0.287309]\n",
      "773 [D loss: 0.705516, acc.: 50.63%] [G loss: 0.286896]\n",
      "774 [D loss: 0.716732, acc.: 46.88%] [G loss: 0.298831]\n",
      "775 [D loss: 0.708475, acc.: 49.38%] [G loss: 0.281211]\n",
      "776 [D loss: 0.720308, acc.: 46.25%] [G loss: 0.283987]\n",
      "777 [D loss: 0.713174, acc.: 49.37%] [G loss: 0.285288]\n",
      "778 [D loss: 0.722570, acc.: 47.50%] [G loss: 0.283536]\n",
      "779 [D loss: 0.725080, acc.: 40.62%] [G loss: 0.272064]\n",
      "780 [D loss: 0.720945, acc.: 48.75%] [G loss: 0.280974]\n",
      "781 [D loss: 0.715415, acc.: 45.00%] [G loss: 0.272785]\n",
      "782 [D loss: 0.712838, acc.: 45.00%] [G loss: 0.271961]\n",
      "783 [D loss: 0.716156, acc.: 46.88%] [G loss: 0.272304]\n",
      "784 [D loss: 0.705945, acc.: 48.75%] [G loss: 0.272408]\n",
      "785 [D loss: 0.716030, acc.: 47.50%] [G loss: 0.282446]\n",
      "786 [D loss: 0.724671, acc.: 46.88%] [G loss: 0.273392]\n",
      "787 [D loss: 0.699527, acc.: 53.75%] [G loss: 0.266956]\n",
      "788 [D loss: 0.700939, acc.: 48.75%] [G loss: 0.280213]\n",
      "789 [D loss: 0.702843, acc.: 50.00%] [G loss: 0.271024]\n",
      "790 [D loss: 0.722359, acc.: 46.88%] [G loss: 0.263609]\n",
      "791 [D loss: 0.710874, acc.: 46.25%] [G loss: 0.277451]\n",
      "792 [D loss: 0.724075, acc.: 49.37%] [G loss: 0.280205]\n",
      "793 [D loss: 0.719725, acc.: 46.25%] [G loss: 0.285835]\n",
      "794 [D loss: 0.717496, acc.: 46.88%] [G loss: 0.282936]\n",
      "795 [D loss: 0.707137, acc.: 46.88%] [G loss: 0.274129]\n",
      "796 [D loss: 0.717743, acc.: 46.25%] [G loss: 0.281792]\n",
      "797 [D loss: 0.714263, acc.: 50.00%] [G loss: 0.287947]\n",
      "798 [D loss: 0.720027, acc.: 44.37%] [G loss: 0.282689]\n",
      "799 [D loss: 0.699873, acc.: 48.12%] [G loss: 0.289902]\n",
      "800 [D loss: 0.730311, acc.: 43.13%] [G loss: 0.285975]\n",
      "801 [D loss: 0.709895, acc.: 46.88%] [G loss: 0.290493]\n",
      "802 [D loss: 0.718140, acc.: 44.38%] [G loss: 0.284789]\n",
      "803 [D loss: 0.726556, acc.: 43.75%] [G loss: 0.280820]\n",
      "804 [D loss: 0.733659, acc.: 40.62%] [G loss: 0.272069]\n",
      "805 [D loss: 0.720108, acc.: 40.62%] [G loss: 0.273432]\n",
      "806 [D loss: 0.712511, acc.: 45.63%] [G loss: 0.274102]\n",
      "807 [D loss: 0.720616, acc.: 41.87%] [G loss: 0.277645]\n",
      "808 [D loss: 0.699243, acc.: 53.12%] [G loss: 0.284848]\n",
      "809 [D loss: 0.723598, acc.: 46.88%] [G loss: 0.265594]\n",
      "810 [D loss: 0.717421, acc.: 46.88%] [G loss: 0.273526]\n",
      "811 [D loss: 0.710856, acc.: 45.00%] [G loss: 0.272168]\n",
      "812 [D loss: 0.718218, acc.: 44.37%] [G loss: 0.275172]\n",
      "813 [D loss: 0.724389, acc.: 41.87%] [G loss: 0.283883]\n",
      "814 [D loss: 0.712472, acc.: 45.62%] [G loss: 0.287568]\n",
      "815 [D loss: 0.712640, acc.: 43.75%] [G loss: 0.286424]\n",
      "816 [D loss: 0.733992, acc.: 43.12%] [G loss: 0.281448]\n",
      "817 [D loss: 0.723569, acc.: 44.37%] [G loss: 0.270469]\n",
      "818 [D loss: 0.743872, acc.: 43.12%] [G loss: 0.294534]\n",
      "819 [D loss: 0.708600, acc.: 48.12%] [G loss: 0.288625]\n",
      "820 [D loss: 0.730142, acc.: 43.12%] [G loss: 0.286292]\n",
      "821 [D loss: 0.704748, acc.: 48.12%] [G loss: 0.291401]\n",
      "822 [D loss: 0.705130, acc.: 49.37%] [G loss: 0.276636]\n",
      "823 [D loss: 0.707143, acc.: 51.88%] [G loss: 0.288806]\n",
      "824 [D loss: 0.709163, acc.: 50.63%] [G loss: 0.288322]\n",
      "825 [D loss: 0.735419, acc.: 41.25%] [G loss: 0.296604]\n",
      "826 [D loss: 0.728047, acc.: 45.63%] [G loss: 0.289574]\n",
      "827 [D loss: 0.717271, acc.: 46.88%] [G loss: 0.294091]\n",
      "828 [D loss: 0.725776, acc.: 49.38%] [G loss: 0.274538]\n",
      "829 [D loss: 0.714941, acc.: 51.25%] [G loss: 0.278137]\n",
      "830 [D loss: 0.727615, acc.: 41.25%] [G loss: 0.271558]\n",
      "831 [D loss: 0.720080, acc.: 42.50%] [G loss: 0.271987]\n",
      "832 [D loss: 0.692175, acc.: 54.37%] [G loss: 0.278770]\n",
      "833 [D loss: 0.727539, acc.: 40.00%] [G loss: 0.269316]\n",
      "834 [D loss: 0.706921, acc.: 45.00%] [G loss: 0.276611]\n",
      "835 [D loss: 0.706215, acc.: 50.63%] [G loss: 0.254424]\n",
      "836 [D loss: 0.730116, acc.: 43.13%] [G loss: 0.261410]\n",
      "837 [D loss: 0.725881, acc.: 45.00%] [G loss: 0.278264]\n",
      "838 [D loss: 0.715758, acc.: 48.12%] [G loss: 0.273799]\n",
      "839 [D loss: 0.715964, acc.: 45.62%] [G loss: 0.292780]\n",
      "840 [D loss: 0.706110, acc.: 45.62%] [G loss: 0.270985]\n",
      "841 [D loss: 0.709881, acc.: 50.63%] [G loss: 0.272898]\n",
      "842 [D loss: 0.709565, acc.: 48.75%] [G loss: 0.282993]\n",
      "843 [D loss: 0.724367, acc.: 43.75%] [G loss: 0.276180]\n",
      "844 [D loss: 0.703402, acc.: 46.25%] [G loss: 0.279829]\n",
      "845 [D loss: 0.719954, acc.: 41.87%] [G loss: 0.280460]\n",
      "846 [D loss: 0.707071, acc.: 46.25%] [G loss: 0.283327]\n",
      "847 [D loss: 0.687104, acc.: 53.12%] [G loss: 0.269883]\n",
      "848 [D loss: 0.708260, acc.: 44.37%] [G loss: 0.282826]\n",
      "849 [D loss: 0.715860, acc.: 45.63%] [G loss: 0.284887]\n",
      "850 [D loss: 0.735528, acc.: 36.88%] [G loss: 0.283000]\n",
      "851 [D loss: 0.721598, acc.: 45.62%] [G loss: 0.287842]\n",
      "852 [D loss: 0.706038, acc.: 45.63%] [G loss: 0.281327]\n",
      "853 [D loss: 0.732551, acc.: 44.37%] [G loss: 0.288880]\n",
      "854 [D loss: 0.716150, acc.: 44.37%] [G loss: 0.282803]\n",
      "855 [D loss: 0.722448, acc.: 42.50%] [G loss: 0.283845]\n",
      "856 [D loss: 0.721371, acc.: 48.12%] [G loss: 0.270807]\n",
      "857 [D loss: 0.706552, acc.: 43.12%] [G loss: 0.285305]\n",
      "858 [D loss: 0.715322, acc.: 45.63%] [G loss: 0.277278]\n",
      "859 [D loss: 0.701590, acc.: 48.75%] [G loss: 0.275799]\n",
      "860 [D loss: 0.699591, acc.: 53.12%] [G loss: 0.280972]\n",
      "861 [D loss: 0.718923, acc.: 41.25%] [G loss: 0.291129]\n",
      "862 [D loss: 0.719128, acc.: 45.00%] [G loss: 0.296396]\n",
      "863 [D loss: 0.700869, acc.: 50.00%] [G loss: 0.276102]\n",
      "864 [D loss: 0.697609, acc.: 49.37%] [G loss: 0.270380]\n",
      "865 [D loss: 0.721241, acc.: 41.25%] [G loss: 0.257478]\n",
      "866 [D loss: 0.717235, acc.: 41.25%] [G loss: 0.266913]\n",
      "867 [D loss: 0.717981, acc.: 48.13%] [G loss: 0.271854]\n",
      "868 [D loss: 0.726749, acc.: 45.63%] [G loss: 0.276417]\n",
      "869 [D loss: 0.696868, acc.: 53.75%] [G loss: 0.269583]\n",
      "870 [D loss: 0.740998, acc.: 39.38%] [G loss: 0.268464]\n",
      "871 [D loss: 0.700429, acc.: 53.12%] [G loss: 0.280269]\n",
      "872 [D loss: 0.721973, acc.: 45.63%] [G loss: 0.274775]\n",
      "873 [D loss: 0.721230, acc.: 45.00%] [G loss: 0.278763]\n",
      "874 [D loss: 0.708068, acc.: 47.50%] [G loss: 0.282391]\n",
      "875 [D loss: 0.735013, acc.: 41.25%] [G loss: 0.293616]\n",
      "876 [D loss: 0.695777, acc.: 52.50%] [G loss: 0.285656]\n",
      "877 [D loss: 0.713871, acc.: 50.00%] [G loss: 0.280423]\n",
      "878 [D loss: 0.728916, acc.: 40.00%] [G loss: 0.300513]\n",
      "879 [D loss: 0.709575, acc.: 46.88%] [G loss: 0.282175]\n",
      "880 [D loss: 0.732942, acc.: 42.50%] [G loss: 0.277770]\n",
      "881 [D loss: 0.707477, acc.: 45.63%] [G loss: 0.284472]\n",
      "882 [D loss: 0.723966, acc.: 48.12%] [G loss: 0.276119]\n",
      "883 [D loss: 0.706197, acc.: 49.37%] [G loss: 0.280068]\n",
      "884 [D loss: 0.725088, acc.: 41.87%] [G loss: 0.282069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.724616, acc.: 41.25%] [G loss: 0.272544]\n",
      "886 [D loss: 0.721664, acc.: 45.00%] [G loss: 0.279481]\n",
      "887 [D loss: 0.704758, acc.: 51.87%] [G loss: 0.282194]\n",
      "888 [D loss: 0.718421, acc.: 41.25%] [G loss: 0.274279]\n",
      "889 [D loss: 0.717576, acc.: 45.00%] [G loss: 0.293619]\n",
      "890 [D loss: 0.709784, acc.: 48.12%] [G loss: 0.286618]\n",
      "891 [D loss: 0.707560, acc.: 46.25%] [G loss: 0.278481]\n",
      "892 [D loss: 0.715635, acc.: 46.88%] [G loss: 0.272406]\n",
      "893 [D loss: 0.718847, acc.: 48.12%] [G loss: 0.274561]\n",
      "894 [D loss: 0.729947, acc.: 41.25%] [G loss: 0.262810]\n",
      "895 [D loss: 0.707418, acc.: 47.50%] [G loss: 0.269814]\n",
      "896 [D loss: 0.725233, acc.: 44.38%] [G loss: 0.276474]\n",
      "897 [D loss: 0.722256, acc.: 48.75%] [G loss: 0.284782]\n",
      "898 [D loss: 0.686838, acc.: 50.63%] [G loss: 0.268966]\n",
      "899 [D loss: 0.732241, acc.: 38.75%] [G loss: 0.277603]\n",
      "900 [D loss: 0.720917, acc.: 48.75%] [G loss: 0.278791]\n",
      "901 [D loss: 0.719868, acc.: 41.87%] [G loss: 0.285619]\n",
      "902 [D loss: 0.710786, acc.: 51.87%] [G loss: 0.280873]\n",
      "903 [D loss: 0.711193, acc.: 46.25%] [G loss: 0.277729]\n",
      "904 [D loss: 0.717842, acc.: 43.12%] [G loss: 0.280938]\n",
      "905 [D loss: 0.706186, acc.: 51.88%] [G loss: 0.260681]\n",
      "906 [D loss: 0.712708, acc.: 45.63%] [G loss: 0.280312]\n",
      "907 [D loss: 0.698517, acc.: 50.63%] [G loss: 0.280839]\n",
      "908 [D loss: 0.732431, acc.: 41.25%] [G loss: 0.285511]\n",
      "909 [D loss: 0.708753, acc.: 48.75%] [G loss: 0.280234]\n",
      "910 [D loss: 0.711568, acc.: 44.38%] [G loss: 0.291405]\n",
      "911 [D loss: 0.725964, acc.: 40.00%] [G loss: 0.277593]\n",
      "912 [D loss: 0.711058, acc.: 51.88%] [G loss: 0.271585]\n",
      "913 [D loss: 0.707412, acc.: 48.75%] [G loss: 0.265164]\n",
      "914 [D loss: 0.702239, acc.: 46.88%] [G loss: 0.282726]\n",
      "915 [D loss: 0.701678, acc.: 49.38%] [G loss: 0.278731]\n",
      "916 [D loss: 0.708983, acc.: 46.25%] [G loss: 0.265451]\n",
      "917 [D loss: 0.720520, acc.: 44.37%] [G loss: 0.271472]\n",
      "918 [D loss: 0.728788, acc.: 44.38%] [G loss: 0.280534]\n",
      "919 [D loss: 0.709716, acc.: 47.50%] [G loss: 0.282028]\n",
      "920 [D loss: 0.718149, acc.: 45.00%] [G loss: 0.278668]\n",
      "921 [D loss: 0.734115, acc.: 41.88%] [G loss: 0.260287]\n",
      "922 [D loss: 0.706952, acc.: 49.37%] [G loss: 0.278514]\n",
      "923 [D loss: 0.702865, acc.: 46.25%] [G loss: 0.269761]\n",
      "924 [D loss: 0.732771, acc.: 44.37%] [G loss: 0.269833]\n",
      "925 [D loss: 0.727898, acc.: 43.13%] [G loss: 0.276391]\n",
      "926 [D loss: 0.705324, acc.: 47.50%] [G loss: 0.271225]\n",
      "927 [D loss: 0.719915, acc.: 45.00%] [G loss: 0.266740]\n",
      "928 [D loss: 0.706656, acc.: 49.37%] [G loss: 0.280933]\n",
      "929 [D loss: 0.706186, acc.: 47.50%] [G loss: 0.266994]\n",
      "930 [D loss: 0.705623, acc.: 48.75%] [G loss: 0.268284]\n",
      "931 [D loss: 0.723549, acc.: 46.88%] [G loss: 0.260775]\n",
      "932 [D loss: 0.711867, acc.: 44.37%] [G loss: 0.271990]\n",
      "933 [D loss: 0.709705, acc.: 46.88%] [G loss: 0.277947]\n",
      "934 [D loss: 0.715817, acc.: 51.87%] [G loss: 0.279612]\n",
      "935 [D loss: 0.717130, acc.: 46.88%] [G loss: 0.268623]\n",
      "936 [D loss: 0.718329, acc.: 46.25%] [G loss: 0.275869]\n",
      "937 [D loss: 0.728983, acc.: 45.00%] [G loss: 0.279553]\n",
      "938 [D loss: 0.720564, acc.: 43.13%] [G loss: 0.264635]\n",
      "939 [D loss: 0.697866, acc.: 53.12%] [G loss: 0.293599]\n",
      "940 [D loss: 0.734989, acc.: 40.62%] [G loss: 0.281088]\n",
      "941 [D loss: 0.722771, acc.: 49.38%] [G loss: 0.287936]\n",
      "942 [D loss: 0.713160, acc.: 45.00%] [G loss: 0.293572]\n",
      "943 [D loss: 0.712928, acc.: 46.25%] [G loss: 0.268691]\n",
      "944 [D loss: 0.708387, acc.: 45.63%] [G loss: 0.288191]\n",
      "945 [D loss: 0.712744, acc.: 48.75%] [G loss: 0.297258]\n",
      "946 [D loss: 0.709163, acc.: 45.00%] [G loss: 0.288079]\n",
      "947 [D loss: 0.737347, acc.: 38.75%] [G loss: 0.276593]\n",
      "948 [D loss: 0.727545, acc.: 45.63%] [G loss: 0.294984]\n",
      "949 [D loss: 0.699755, acc.: 50.63%] [G loss: 0.265493]\n",
      "950 [D loss: 0.709620, acc.: 46.25%] [G loss: 0.270815]\n",
      "951 [D loss: 0.724279, acc.: 46.88%] [G loss: 0.278065]\n",
      "952 [D loss: 0.699952, acc.: 53.12%] [G loss: 0.271074]\n",
      "953 [D loss: 0.717782, acc.: 45.63%] [G loss: 0.286206]\n",
      "954 [D loss: 0.717550, acc.: 45.63%] [G loss: 0.275963]\n",
      "955 [D loss: 0.702556, acc.: 51.25%] [G loss: 0.260260]\n",
      "956 [D loss: 0.712673, acc.: 43.75%] [G loss: 0.281703]\n",
      "957 [D loss: 0.719917, acc.: 46.25%] [G loss: 0.274462]\n",
      "958 [D loss: 0.716485, acc.: 45.00%] [G loss: 0.287772]\n",
      "959 [D loss: 0.727602, acc.: 45.00%] [G loss: 0.277080]\n",
      "960 [D loss: 0.711487, acc.: 43.12%] [G loss: 0.275081]\n",
      "961 [D loss: 0.716988, acc.: 45.63%] [G loss: 0.267797]\n",
      "962 [D loss: 0.722036, acc.: 40.00%] [G loss: 0.287034]\n",
      "963 [D loss: 0.704666, acc.: 46.25%] [G loss: 0.290940]\n",
      "964 [D loss: 0.697589, acc.: 52.50%] [G loss: 0.285492]\n",
      "965 [D loss: 0.730127, acc.: 41.87%] [G loss: 0.278880]\n",
      "966 [D loss: 0.719165, acc.: 44.37%] [G loss: 0.282981]\n",
      "967 [D loss: 0.704612, acc.: 48.13%] [G loss: 0.285854]\n",
      "968 [D loss: 0.697326, acc.: 51.88%] [G loss: 0.285653]\n",
      "969 [D loss: 0.717675, acc.: 48.13%] [G loss: 0.276825]\n",
      "970 [D loss: 0.707351, acc.: 48.12%] [G loss: 0.275824]\n",
      "971 [D loss: 0.736501, acc.: 36.25%] [G loss: 0.270956]\n",
      "972 [D loss: 0.718412, acc.: 41.87%] [G loss: 0.290282]\n",
      "973 [D loss: 0.705594, acc.: 47.50%] [G loss: 0.281008]\n",
      "974 [D loss: 0.724193, acc.: 41.25%] [G loss: 0.284570]\n",
      "975 [D loss: 0.704096, acc.: 45.63%] [G loss: 0.284001]\n",
      "976 [D loss: 0.712249, acc.: 45.63%] [G loss: 0.284230]\n",
      "977 [D loss: 0.724700, acc.: 43.12%] [G loss: 0.283634]\n",
      "978 [D loss: 0.728893, acc.: 43.13%] [G loss: 0.280435]\n",
      "979 [D loss: 0.704533, acc.: 50.00%] [G loss: 0.270835]\n",
      "980 [D loss: 0.720522, acc.: 43.75%] [G loss: 0.287094]\n",
      "981 [D loss: 0.722226, acc.: 45.00%] [G loss: 0.268844]\n",
      "982 [D loss: 0.717762, acc.: 44.37%] [G loss: 0.266484]\n",
      "983 [D loss: 0.699485, acc.: 48.75%] [G loss: 0.273998]\n",
      "984 [D loss: 0.700026, acc.: 51.25%] [G loss: 0.270679]\n",
      "985 [D loss: 0.713662, acc.: 44.37%] [G loss: 0.278578]\n",
      "986 [D loss: 0.718683, acc.: 40.62%] [G loss: 0.267597]\n",
      "987 [D loss: 0.721880, acc.: 47.50%] [G loss: 0.270590]\n",
      "988 [D loss: 0.719819, acc.: 43.13%] [G loss: 0.274036]\n",
      "989 [D loss: 0.718977, acc.: 44.38%] [G loss: 0.264251]\n",
      "990 [D loss: 0.705166, acc.: 51.88%] [G loss: 0.275822]\n",
      "991 [D loss: 0.717531, acc.: 45.63%] [G loss: 0.275290]\n",
      "992 [D loss: 0.719534, acc.: 43.12%] [G loss: 0.280202]\n",
      "993 [D loss: 0.706648, acc.: 48.75%] [G loss: 0.269762]\n",
      "994 [D loss: 0.710730, acc.: 43.75%] [G loss: 0.273811]\n",
      "995 [D loss: 0.725991, acc.: 40.62%] [G loss: 0.282668]\n",
      "996 [D loss: 0.727950, acc.: 40.00%] [G loss: 0.286526]\n",
      "997 [D loss: 0.707307, acc.: 51.25%] [G loss: 0.290783]\n",
      "998 [D loss: 0.728194, acc.: 40.62%] [G loss: 0.277372]\n",
      "999 [D loss: 0.717020, acc.: 48.12%] [G loss: 0.284138]\n",
      "1000 [D loss: 0.712092, acc.: 46.88%] [G loss: 0.277512]\n"
     ]
    }
   ],
   "source": [
    "g_loss, d_loss = lstmgan.train(epochs=1001, batch_size=80, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_loss)\n",
    "plt.plot(d_loss)\n",
    "plt.title('GAN Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Generator', 'Discriminator'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
