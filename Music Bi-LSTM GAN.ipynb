{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "        x_train = np.load(r'C:\\Users\\Vee\\Desktop\\python\\answers.npy',allow_pickle=True)\n",
    "        x_train = x_train.reshape(721,4,4)\n",
    "        return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 4\n",
    "        self.img_cols = 4\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 16\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        generator = self.generator\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(4,4))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(4, 4)))\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(16))\n",
    "        #specifying 1 feature as the output\n",
    "        model.add(Bidirectional(LSTM(64, activation = 'selu', return_sequences=True, dropout = 0.2)))\n",
    "        model.add(Bidirectional(LSTM(64, activation = 'relu', return_sequences=True, dropout = 0.2)))\n",
    "        model.add(Bidirectional(LSTM(64, activation = 'tanh', return_sequences=True, dropout = 0.2)))\n",
    "        model.add(TimeDistributed(Dense(64, activation = 'selu')))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(TimeDistributed(Dense(64, activation = 'selu')))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(TimeDistributed(Dense(64, activation = 'selu')))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'linear')))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(4,4))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Bidirectional(LSTM(64, activation = 'tanh', dropout = 0.2), input_shape=(16,1)))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(TimeDistributed(Dense(64, activation = 'selu')))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(TimeDistributed(Dense(64, activation = 'selu')))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(TimeDistributed(Dense(64, activation = 'selu')))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'linear')))\n",
    "\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 256\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),16,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size,4,4))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                #self.save_imgs(epoch)\n",
    "                self.generator.save(\"generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_83 (Bidirectio (None, 128)               33792     \n",
      "_________________________________________________________________\n",
      "repeat_vector_20 (RepeatVect (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_109 (TimeDi (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_110 (TimeDi (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_111 (TimeDi (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_112 (TimeDi (None, 1, 1)              65        \n",
      "=================================================================\n",
      "Total params: 50,433\n",
      "Trainable params: 50,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_84 (Bidirectio (None, 4, 128)            35328     \n",
      "_________________________________________________________________\n",
      "bidirectional_85 (Bidirectio (None, 4, 128)            98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_86 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "repeat_vector_21 (RepeatVect (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_87 (Bidirectio (None, 16, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_88 (Bidirectio (None, 16, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_89 (Bidirectio (None, 16, 128)           98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_113 (TimeDi (None, 16, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_114 (TimeDi (None, 16, 64)            4160      \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_115 (TimeDi (None, 16, 64)            4160      \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_116 (TimeDi (None, 16, 1)             65        \n",
      "=================================================================\n",
      "Total params: 546,049\n",
      "Trainable params: 546,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 4.227586, acc.: 50.00%] [G loss: 5.144736]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 1.983242, acc.: 50.00%] [G loss: 6.013484]\n",
      "2 [D loss: 1.696669, acc.: 50.00%] [G loss: 3.178210]\n",
      "3 [D loss: 1.593351, acc.: 50.00%] [G loss: 5.966186]\n",
      "4 [D loss: 1.532439, acc.: 50.00%] [G loss: 3.390188]\n",
      "5 [D loss: 1.623617, acc.: 50.00%] [G loss: 3.210495]\n",
      "6 [D loss: 1.568402, acc.: 50.00%] [G loss: 2.897941]\n",
      "7 [D loss: 1.428355, acc.: 50.00%] [G loss: 2.666708]\n",
      "8 [D loss: 1.431103, acc.: 50.00%] [G loss: 2.685489]\n",
      "9 [D loss: 1.397308, acc.: 50.00%] [G loss: 2.567030]\n",
      "10 [D loss: 1.386629, acc.: 50.00%] [G loss: 3.063162]\n",
      "11 [D loss: 1.380127, acc.: 50.00%] [G loss: 2.629270]\n",
      "12 [D loss: 1.346286, acc.: 50.00%] [G loss: 2.432515]\n",
      "13 [D loss: 1.356521, acc.: 50.00%] [G loss: 2.401710]\n",
      "14 [D loss: 1.204844, acc.: 50.00%] [G loss: 2.284453]\n",
      "15 [D loss: 1.185509, acc.: 50.00%] [G loss: 2.176377]\n",
      "16 [D loss: 1.241452, acc.: 50.00%] [G loss: 1.965274]\n",
      "17 [D loss: 1.255606, acc.: 50.00%] [G loss: 2.217109]\n",
      "18 [D loss: 1.103330, acc.: 50.00%] [G loss: 1.969199]\n",
      "19 [D loss: 1.056835, acc.: 50.00%] [G loss: 2.000985]\n",
      "20 [D loss: 1.055872, acc.: 50.00%] [G loss: 1.833094]\n",
      "21 [D loss: 1.108977, acc.: 50.00%] [G loss: 3.114849]\n",
      "22 [D loss: 1.020719, acc.: 50.00%] [G loss: 2.110896]\n",
      "23 [D loss: 0.993646, acc.: 50.00%] [G loss: 1.938602]\n",
      "24 [D loss: 0.957544, acc.: 50.00%] [G loss: 1.686655]\n",
      "25 [D loss: 0.970200, acc.: 50.00%] [G loss: 1.594883]\n",
      "26 [D loss: 0.966760, acc.: 50.00%] [G loss: 1.680362]\n",
      "27 [D loss: 0.937025, acc.: 50.00%] [G loss: 1.634811]\n",
      "28 [D loss: 0.888376, acc.: 50.00%] [G loss: 1.480893]\n",
      "29 [D loss: 0.857818, acc.: 50.00%] [G loss: 1.401089]\n",
      "30 [D loss: 0.861779, acc.: 50.00%] [G loss: 1.485090]\n",
      "31 [D loss: 0.865689, acc.: 50.00%] [G loss: 1.582811]\n",
      "32 [D loss: 0.873523, acc.: 50.00%] [G loss: 1.506296]\n",
      "33 [D loss: 0.867784, acc.: 50.00%] [G loss: 1.329908]\n",
      "34 [D loss: 0.856715, acc.: 50.00%] [G loss: 1.319057]\n",
      "35 [D loss: 0.794690, acc.: 50.00%] [G loss: 1.189552]\n",
      "36 [D loss: 0.874227, acc.: 45.00%] [G loss: 1.296447]\n",
      "37 [D loss: 0.839984, acc.: 50.00%] [G loss: 1.432208]\n",
      "38 [D loss: 0.731955, acc.: 50.00%] [G loss: 1.204006]\n",
      "39 [D loss: 0.781378, acc.: 50.00%] [G loss: 1.050199]\n",
      "40 [D loss: 0.749347, acc.: 50.00%] [G loss: 1.098311]\n",
      "41 [D loss: 0.819705, acc.: 35.00%] [G loss: 1.210109]\n",
      "42 [D loss: 0.768685, acc.: 50.00%] [G loss: 1.229807]\n",
      "43 [D loss: 0.742777, acc.: 40.00%] [G loss: 1.548966]\n",
      "44 [D loss: 0.780288, acc.: 45.00%] [G loss: 1.003777]\n",
      "45 [D loss: 0.744865, acc.: 50.00%] [G loss: 0.965638]\n",
      "46 [D loss: 0.791550, acc.: 50.00%] [G loss: 1.054489]\n",
      "47 [D loss: 0.766572, acc.: 40.00%] [G loss: 1.218110]\n",
      "48 [D loss: 0.711678, acc.: 55.00%] [G loss: 1.185150]\n",
      "49 [D loss: 0.689728, acc.: 50.00%] [G loss: 0.958999]\n",
      "50 [D loss: 0.695161, acc.: 65.00%] [G loss: 1.013211]\n",
      "51 [D loss: 0.741505, acc.: 50.00%] [G loss: 1.021467]\n",
      "52 [D loss: 0.775360, acc.: 50.00%] [G loss: 0.778144]\n",
      "53 [D loss: 0.753015, acc.: 35.00%] [G loss: 0.988422]\n",
      "54 [D loss: 0.684139, acc.: 60.00%] [G loss: 0.826247]\n",
      "55 [D loss: 0.859633, acc.: 30.00%] [G loss: 0.805649]\n",
      "56 [D loss: 0.589345, acc.: 75.00%] [G loss: 0.833707]\n",
      "57 [D loss: 0.770360, acc.: 55.00%] [G loss: 1.069659]\n",
      "58 [D loss: 0.715704, acc.: 45.00%] [G loss: 0.999640]\n",
      "59 [D loss: 0.667703, acc.: 65.00%] [G loss: 1.100201]\n",
      "60 [D loss: 0.723565, acc.: 45.00%] [G loss: 0.866176]\n",
      "61 [D loss: 0.719733, acc.: 50.00%] [G loss: 0.898471]\n",
      "62 [D loss: 0.762485, acc.: 35.00%] [G loss: 0.788598]\n",
      "63 [D loss: 0.846197, acc.: 25.00%] [G loss: 0.746852]\n",
      "64 [D loss: 1.511781, acc.: 35.00%] [G loss: 0.719950]\n",
      "65 [D loss: 0.891679, acc.: 20.00%] [G loss: 0.914595]\n",
      "66 [D loss: 0.771731, acc.: 35.00%] [G loss: 0.825477]\n",
      "67 [D loss: 0.708340, acc.: 45.00%] [G loss: 0.834794]\n",
      "68 [D loss: 0.695178, acc.: 55.00%] [G loss: 0.819682]\n",
      "69 [D loss: 0.625469, acc.: 60.00%] [G loss: 0.908396]\n",
      "70 [D loss: 0.636548, acc.: 60.00%] [G loss: 2.388978]\n",
      "71 [D loss: 0.641182, acc.: 65.00%] [G loss: 0.974140]\n",
      "72 [D loss: 0.746963, acc.: 50.00%] [G loss: 0.757593]\n",
      "73 [D loss: 0.785928, acc.: 55.00%] [G loss: 0.888329]\n",
      "74 [D loss: 0.770755, acc.: 50.00%] [G loss: 0.777574]\n",
      "75 [D loss: 0.836608, acc.: 45.00%] [G loss: 0.696700]\n",
      "76 [D loss: 0.750662, acc.: 30.00%] [G loss: 0.742187]\n",
      "77 [D loss: 0.723153, acc.: 55.00%] [G loss: 0.767579]\n",
      "78 [D loss: 0.774032, acc.: 50.00%] [G loss: 0.967313]\n",
      "79 [D loss: 0.780079, acc.: 50.00%] [G loss: 0.834798]\n",
      "80 [D loss: 0.709368, acc.: 65.00%] [G loss: 1.184315]\n",
      "81 [D loss: 0.706148, acc.: 55.00%] [G loss: 1.244239]\n",
      "82 [D loss: 0.600153, acc.: 70.00%] [G loss: 1.039845]\n",
      "83 [D loss: 0.835889, acc.: 40.00%] [G loss: 0.933139]\n",
      "84 [D loss: 0.772096, acc.: 40.00%] [G loss: 0.900915]\n",
      "85 [D loss: 0.752721, acc.: 45.00%] [G loss: 0.792218]\n",
      "86 [D loss: 0.746976, acc.: 40.00%] [G loss: 0.627953]\n",
      "87 [D loss: 0.776427, acc.: 40.00%] [G loss: 0.738074]\n",
      "88 [D loss: 0.739344, acc.: 50.00%] [G loss: 0.764710]\n",
      "89 [D loss: 0.706456, acc.: 55.00%] [G loss: 0.817678]\n",
      "90 [D loss: 0.690486, acc.: 45.00%] [G loss: 0.703160]\n",
      "91 [D loss: 0.835030, acc.: 35.00%] [G loss: 0.851649]\n",
      "92 [D loss: 0.739952, acc.: 50.00%] [G loss: 0.978734]\n",
      "93 [D loss: 0.616934, acc.: 70.00%] [G loss: 0.997423]\n",
      "94 [D loss: 0.853983, acc.: 25.00%] [G loss: 0.915969]\n",
      "95 [D loss: 0.777505, acc.: 45.00%] [G loss: 0.791664]\n",
      "96 [D loss: 0.703979, acc.: 45.00%] [G loss: 0.787453]\n",
      "97 [D loss: 0.679180, acc.: 60.00%] [G loss: 0.731111]\n",
      "98 [D loss: 0.782153, acc.: 50.00%] [G loss: 0.740448]\n",
      "99 [D loss: 0.656584, acc.: 65.00%] [G loss: 1.014012]\n",
      "100 [D loss: 0.699090, acc.: 55.00%] [G loss: 1.072970]\n",
      "101 [D loss: 0.736362, acc.: 45.00%] [G loss: 0.969222]\n",
      "102 [D loss: 0.707396, acc.: 55.00%] [G loss: 0.811109]\n",
      "103 [D loss: 0.714640, acc.: 60.00%] [G loss: 0.811558]\n",
      "104 [D loss: 0.717324, acc.: 60.00%] [G loss: 0.811846]\n",
      "105 [D loss: 0.744764, acc.: 40.00%] [G loss: 0.945078]\n",
      "106 [D loss: 0.682739, acc.: 55.00%] [G loss: 0.755188]\n",
      "107 [D loss: 0.701852, acc.: 55.00%] [G loss: 0.704704]\n",
      "108 [D loss: 0.704161, acc.: 60.00%] [G loss: 0.764918]\n",
      "109 [D loss: 0.668109, acc.: 55.00%] [G loss: 0.834003]\n",
      "110 [D loss: 0.729174, acc.: 40.00%] [G loss: 0.744918]\n",
      "111 [D loss: 0.681929, acc.: 60.00%] [G loss: 1.013747]\n",
      "112 [D loss: 0.628447, acc.: 60.00%] [G loss: 0.853444]\n",
      "113 [D loss: 0.631879, acc.: 60.00%] [G loss: 0.894560]\n",
      "114 [D loss: 0.683007, acc.: 65.00%] [G loss: 0.808450]\n",
      "115 [D loss: 0.819586, acc.: 45.00%] [G loss: 0.987746]\n",
      "116 [D loss: 0.668887, acc.: 70.00%] [G loss: 0.896657]\n",
      "117 [D loss: 0.722016, acc.: 50.00%] [G loss: 0.701127]\n",
      "118 [D loss: 0.710607, acc.: 40.00%] [G loss: 0.754010]\n",
      "119 [D loss: 0.747219, acc.: 50.00%] [G loss: 0.801181]\n",
      "120 [D loss: 0.685160, acc.: 60.00%] [G loss: 0.831504]\n",
      "121 [D loss: 0.737943, acc.: 35.00%] [G loss: 0.689521]\n",
      "122 [D loss: 0.916613, acc.: 35.00%] [G loss: 0.844050]\n",
      "123 [D loss: 0.688251, acc.: 50.00%] [G loss: 0.767181]\n",
      "124 [D loss: 0.669409, acc.: 55.00%] [G loss: 0.815174]\n",
      "125 [D loss: 0.741940, acc.: 50.00%] [G loss: 0.898487]\n",
      "126 [D loss: 0.751507, acc.: 50.00%] [G loss: 0.694353]\n",
      "127 [D loss: 0.750808, acc.: 35.00%] [G loss: 0.791943]\n",
      "128 [D loss: 0.688313, acc.: 50.00%] [G loss: 0.883437]\n",
      "129 [D loss: 0.857299, acc.: 25.00%] [G loss: 0.607057]\n",
      "130 [D loss: 0.608525, acc.: 70.00%] [G loss: 0.716658]\n",
      "131 [D loss: 0.761427, acc.: 35.00%] [G loss: 0.900043]\n",
      "132 [D loss: 0.823999, acc.: 45.00%] [G loss: 0.957427]\n",
      "133 [D loss: 0.670006, acc.: 50.00%] [G loss: 0.852615]\n",
      "134 [D loss: 0.759338, acc.: 35.00%] [G loss: 1.013602]\n",
      "135 [D loss: 0.669422, acc.: 60.00%] [G loss: 0.897297]\n",
      "136 [D loss: 0.816467, acc.: 45.00%] [G loss: 0.791471]\n",
      "137 [D loss: 0.669754, acc.: 70.00%] [G loss: 0.899504]\n",
      "138 [D loss: 0.668866, acc.: 55.00%] [G loss: 0.840398]\n",
      "139 [D loss: 0.753419, acc.: 45.00%] [G loss: 0.820958]\n",
      "140 [D loss: 0.773673, acc.: 40.00%] [G loss: 0.693712]\n",
      "141 [D loss: 0.756808, acc.: 45.00%] [G loss: 0.991240]\n",
      "142 [D loss: 0.723337, acc.: 60.00%] [G loss: 0.850717]\n",
      "143 [D loss: 0.768744, acc.: 50.00%] [G loss: 0.965980]\n",
      "144 [D loss: 0.708495, acc.: 50.00%] [G loss: 0.969653]\n",
      "145 [D loss: 0.666928, acc.: 60.00%] [G loss: 0.798001]\n",
      "146 [D loss: 0.791781, acc.: 45.00%] [G loss: 0.763604]\n",
      "147 [D loss: 0.737536, acc.: 45.00%] [G loss: 0.774589]\n",
      "148 [D loss: 0.710860, acc.: 50.00%] [G loss: 0.722049]\n",
      "149 [D loss: 0.771473, acc.: 45.00%] [G loss: 0.975308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.728783, acc.: 60.00%] [G loss: 0.707366]\n",
      "151 [D loss: 0.666070, acc.: 70.00%] [G loss: 0.812464]\n",
      "152 [D loss: 0.735646, acc.: 45.00%] [G loss: 0.809903]\n",
      "153 [D loss: 0.773793, acc.: 40.00%] [G loss: 0.886046]\n",
      "154 [D loss: 0.750533, acc.: 50.00%] [G loss: 0.790004]\n",
      "155 [D loss: 0.826161, acc.: 30.00%] [G loss: 0.738916]\n",
      "156 [D loss: 0.744229, acc.: 55.00%] [G loss: 0.937043]\n",
      "157 [D loss: 0.692639, acc.: 50.00%] [G loss: 0.749999]\n",
      "158 [D loss: 0.856111, acc.: 35.00%] [G loss: 0.703155]\n",
      "159 [D loss: 0.780204, acc.: 45.00%] [G loss: 0.726770]\n",
      "160 [D loss: 0.763089, acc.: 40.00%] [G loss: 0.755306]\n",
      "161 [D loss: 0.757342, acc.: 55.00%] [G loss: 0.781765]\n",
      "162 [D loss: 0.656693, acc.: 70.00%] [G loss: 0.754333]\n",
      "163 [D loss: 0.743404, acc.: 55.00%] [G loss: 0.836640]\n",
      "164 [D loss: 0.731319, acc.: 55.00%] [G loss: 0.738024]\n",
      "165 [D loss: 0.766353, acc.: 40.00%] [G loss: 0.796059]\n",
      "166 [D loss: 0.816363, acc.: 45.00%] [G loss: 0.905594]\n",
      "167 [D loss: 0.698586, acc.: 60.00%] [G loss: 0.752312]\n",
      "168 [D loss: 0.763934, acc.: 40.00%] [G loss: 0.689056]\n",
      "169 [D loss: 0.851084, acc.: 45.00%] [G loss: 0.824950]\n",
      "170 [D loss: 0.782531, acc.: 45.00%] [G loss: 1.009530]\n",
      "171 [D loss: 0.650838, acc.: 55.00%] [G loss: 0.901800]\n",
      "172 [D loss: 0.784601, acc.: 45.00%] [G loss: 0.810368]\n",
      "173 [D loss: 0.726685, acc.: 50.00%] [G loss: 0.957150]\n",
      "174 [D loss: 0.892089, acc.: 35.00%] [G loss: 0.796236]\n",
      "175 [D loss: 0.745442, acc.: 50.00%] [G loss: 0.771725]\n",
      "176 [D loss: 0.774858, acc.: 40.00%] [G loss: 0.928431]\n",
      "177 [D loss: 0.718747, acc.: 45.00%] [G loss: 0.821575]\n",
      "178 [D loss: 0.699374, acc.: 40.00%] [G loss: 0.917106]\n",
      "179 [D loss: 0.727596, acc.: 55.00%] [G loss: 0.972780]\n",
      "180 [D loss: 0.777604, acc.: 40.00%] [G loss: 0.794836]\n",
      "181 [D loss: 0.770410, acc.: 45.00%] [G loss: 0.852485]\n",
      "182 [D loss: 0.722760, acc.: 50.00%] [G loss: 0.969320]\n",
      "183 [D loss: 0.762800, acc.: 45.00%] [G loss: 0.764400]\n",
      "184 [D loss: 0.823779, acc.: 25.00%] [G loss: 0.881861]\n",
      "185 [D loss: 0.769386, acc.: 55.00%] [G loss: 0.777298]\n",
      "186 [D loss: 0.821651, acc.: 35.00%] [G loss: 0.904878]\n",
      "187 [D loss: 0.690818, acc.: 60.00%] [G loss: 0.821614]\n",
      "188 [D loss: 0.693548, acc.: 55.00%] [G loss: 0.815567]\n",
      "189 [D loss: 0.744968, acc.: 35.00%] [G loss: 0.868404]\n",
      "190 [D loss: 0.759938, acc.: 50.00%] [G loss: 0.790495]\n",
      "191 [D loss: 0.714059, acc.: 50.00%] [G loss: 0.787612]\n",
      "192 [D loss: 0.814193, acc.: 40.00%] [G loss: 0.828828]\n",
      "193 [D loss: 0.800646, acc.: 35.00%] [G loss: 0.915588]\n",
      "194 [D loss: 0.664915, acc.: 65.00%] [G loss: 0.717087]\n",
      "195 [D loss: 0.738552, acc.: 55.00%] [G loss: 0.727240]\n",
      "196 [D loss: 0.711379, acc.: 60.00%] [G loss: 0.788261]\n",
      "197 [D loss: 0.807868, acc.: 40.00%] [G loss: 0.695038]\n",
      "198 [D loss: 0.603105, acc.: 70.00%] [G loss: 0.778142]\n",
      "199 [D loss: 0.718020, acc.: 45.00%] [G loss: 0.769273]\n",
      "200 [D loss: 0.634348, acc.: 65.00%] [G loss: 0.779951]\n",
      "201 [D loss: 0.761473, acc.: 40.00%] [G loss: 0.748537]\n",
      "202 [D loss: 0.789117, acc.: 25.00%] [G loss: 0.706717]\n",
      "203 [D loss: 0.731970, acc.: 40.00%] [G loss: 0.913614]\n",
      "204 [D loss: 0.742147, acc.: 35.00%] [G loss: 0.807371]\n",
      "205 [D loss: 0.699878, acc.: 45.00%] [G loss: 0.828720]\n",
      "206 [D loss: 0.837498, acc.: 25.00%] [G loss: 0.768825]\n",
      "207 [D loss: 0.715023, acc.: 50.00%] [G loss: 0.738215]\n",
      "208 [D loss: 0.717090, acc.: 55.00%] [G loss: 0.798013]\n",
      "209 [D loss: 0.722639, acc.: 40.00%] [G loss: 0.851774]\n",
      "210 [D loss: 0.771490, acc.: 45.00%] [G loss: 0.876407]\n",
      "211 [D loss: 0.834497, acc.: 35.00%] [G loss: 1.039392]\n",
      "212 [D loss: 0.712628, acc.: 60.00%] [G loss: 0.870671]\n",
      "213 [D loss: 0.711638, acc.: 55.00%] [G loss: 0.773514]\n",
      "214 [D loss: 0.798049, acc.: 50.00%] [G loss: 0.867039]\n",
      "215 [D loss: 0.756493, acc.: 60.00%] [G loss: 0.786980]\n",
      "216 [D loss: 0.783480, acc.: 30.00%] [G loss: 0.819619]\n",
      "217 [D loss: 0.739331, acc.: 45.00%] [G loss: 0.837782]\n",
      "218 [D loss: 0.725109, acc.: 50.00%] [G loss: 0.814547]\n",
      "219 [D loss: 0.773778, acc.: 50.00%] [G loss: 0.821580]\n",
      "220 [D loss: 0.706868, acc.: 45.00%] [G loss: 0.927546]\n",
      "221 [D loss: 0.674796, acc.: 65.00%] [G loss: 0.801458]\n",
      "222 [D loss: 0.674086, acc.: 60.00%] [G loss: 0.744169]\n",
      "223 [D loss: 0.824073, acc.: 30.00%] [G loss: 0.779637]\n",
      "224 [D loss: 0.675617, acc.: 60.00%] [G loss: 0.907717]\n",
      "225 [D loss: 0.631790, acc.: 60.00%] [G loss: 0.803191]\n",
      "226 [D loss: 0.755621, acc.: 50.00%] [G loss: 0.771768]\n",
      "227 [D loss: 0.793613, acc.: 20.00%] [G loss: 0.690936]\n",
      "228 [D loss: 0.765860, acc.: 45.00%] [G loss: 0.843508]\n",
      "229 [D loss: 0.764324, acc.: 45.00%] [G loss: 0.803128]\n",
      "230 [D loss: 0.771823, acc.: 40.00%] [G loss: 0.857897]\n",
      "231 [D loss: 0.727270, acc.: 50.00%] [G loss: 0.842345]\n",
      "232 [D loss: 0.768462, acc.: 45.00%] [G loss: 0.850436]\n",
      "233 [D loss: 0.725461, acc.: 60.00%] [G loss: 0.889175]\n",
      "234 [D loss: 0.798392, acc.: 40.00%] [G loss: 0.759605]\n",
      "235 [D loss: 0.734947, acc.: 40.00%] [G loss: 0.783069]\n",
      "236 [D loss: 0.726013, acc.: 40.00%] [G loss: 0.741926]\n",
      "237 [D loss: 0.746334, acc.: 40.00%] [G loss: 0.857153]\n",
      "238 [D loss: 0.672070, acc.: 50.00%] [G loss: 0.761159]\n",
      "239 [D loss: 0.737959, acc.: 50.00%] [G loss: 0.902046]\n",
      "240 [D loss: 0.658700, acc.: 80.00%] [G loss: 0.613945]\n",
      "241 [D loss: 0.712363, acc.: 50.00%] [G loss: 0.832643]\n",
      "242 [D loss: 0.775626, acc.: 45.00%] [G loss: 0.785784]\n",
      "243 [D loss: 0.824252, acc.: 35.00%] [G loss: 0.881521]\n",
      "244 [D loss: 0.641709, acc.: 65.00%] [G loss: 0.669322]\n",
      "245 [D loss: 0.696994, acc.: 45.00%] [G loss: 0.727828]\n",
      "246 [D loss: 0.738116, acc.: 45.00%] [G loss: 0.607792]\n",
      "247 [D loss: 0.707451, acc.: 65.00%] [G loss: 0.696665]\n",
      "248 [D loss: 0.763159, acc.: 50.00%] [G loss: 0.734910]\n",
      "249 [D loss: 0.763717, acc.: 45.00%] [G loss: 0.714826]\n",
      "250 [D loss: 0.743273, acc.: 35.00%] [G loss: 0.805709]\n",
      "251 [D loss: 0.704074, acc.: 45.00%] [G loss: 0.739050]\n",
      "252 [D loss: 0.764211, acc.: 40.00%] [G loss: 0.818137]\n",
      "253 [D loss: 0.854235, acc.: 45.00%] [G loss: 0.816083]\n",
      "254 [D loss: 0.754137, acc.: 40.00%] [G loss: 0.888811]\n",
      "255 [D loss: 0.719928, acc.: 55.00%] [G loss: 0.831529]\n",
      "256 [D loss: 0.750683, acc.: 35.00%] [G loss: 0.955592]\n",
      "257 [D loss: 0.719024, acc.: 50.00%] [G loss: 0.739385]\n",
      "258 [D loss: 0.794568, acc.: 45.00%] [G loss: 0.700135]\n",
      "259 [D loss: 0.750382, acc.: 45.00%] [G loss: 0.885239]\n",
      "260 [D loss: 0.737954, acc.: 45.00%] [G loss: 0.807509]\n",
      "261 [D loss: 0.737960, acc.: 50.00%] [G loss: 0.882607]\n",
      "262 [D loss: 0.727030, acc.: 45.00%] [G loss: 0.770698]\n",
      "263 [D loss: 0.688657, acc.: 65.00%] [G loss: 0.847511]\n",
      "264 [D loss: 0.672478, acc.: 60.00%] [G loss: 0.785810]\n",
      "265 [D loss: 0.634714, acc.: 60.00%] [G loss: 0.925047]\n",
      "266 [D loss: 0.690654, acc.: 55.00%] [G loss: 0.796047]\n",
      "267 [D loss: 0.697076, acc.: 45.00%] [G loss: 0.774541]\n",
      "268 [D loss: 0.749486, acc.: 40.00%] [G loss: 0.660053]\n",
      "269 [D loss: 0.749238, acc.: 40.00%] [G loss: 0.810629]\n",
      "270 [D loss: 0.798086, acc.: 35.00%] [G loss: 0.889583]\n",
      "271 [D loss: 0.723512, acc.: 40.00%] [G loss: 0.783924]\n",
      "272 [D loss: 0.748197, acc.: 40.00%] [G loss: 0.841654]\n",
      "273 [D loss: 0.632132, acc.: 70.00%] [G loss: 0.797581]\n",
      "274 [D loss: 0.713326, acc.: 40.00%] [G loss: 0.675007]\n",
      "275 [D loss: 0.713792, acc.: 45.00%] [G loss: 0.705974]\n",
      "276 [D loss: 0.813878, acc.: 25.00%] [G loss: 0.809269]\n",
      "277 [D loss: 0.854741, acc.: 35.00%] [G loss: 0.733283]\n",
      "278 [D loss: 0.754167, acc.: 30.00%] [G loss: 0.684986]\n",
      "279 [D loss: 0.714904, acc.: 50.00%] [G loss: 0.758682]\n",
      "280 [D loss: 0.737481, acc.: 45.00%] [G loss: 0.734616]\n",
      "281 [D loss: 0.765722, acc.: 45.00%] [G loss: 0.841956]\n",
      "282 [D loss: 0.760926, acc.: 50.00%] [G loss: 0.926714]\n",
      "283 [D loss: 0.765189, acc.: 30.00%] [G loss: 0.854099]\n",
      "284 [D loss: 0.851021, acc.: 35.00%] [G loss: 0.884244]\n",
      "285 [D loss: 0.665365, acc.: 50.00%] [G loss: 0.818825]\n",
      "286 [D loss: 0.807551, acc.: 25.00%] [G loss: 0.862305]\n",
      "287 [D loss: 0.760715, acc.: 35.00%] [G loss: 0.856095]\n",
      "288 [D loss: 0.671797, acc.: 65.00%] [G loss: 0.757187]\n",
      "289 [D loss: 0.680805, acc.: 60.00%] [G loss: 0.859999]\n",
      "290 [D loss: 0.638265, acc.: 65.00%] [G loss: 0.857513]\n",
      "291 [D loss: 0.745604, acc.: 45.00%] [G loss: 0.734653]\n",
      "292 [D loss: 0.782992, acc.: 35.00%] [G loss: 0.723787]\n",
      "293 [D loss: 0.783047, acc.: 50.00%] [G loss: 0.819100]\n",
      "294 [D loss: 0.746515, acc.: 35.00%] [G loss: 0.897324]\n",
      "295 [D loss: 0.728760, acc.: 40.00%] [G loss: 0.815076]\n",
      "296 [D loss: 0.670850, acc.: 75.00%] [G loss: 0.784648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.620681, acc.: 55.00%] [G loss: 0.744812]\n",
      "298 [D loss: 0.723685, acc.: 50.00%] [G loss: 0.680972]\n",
      "299 [D loss: 0.802286, acc.: 25.00%] [G loss: 0.895308]\n",
      "300 [D loss: 0.746641, acc.: 40.00%] [G loss: 0.798339]\n",
      "301 [D loss: 0.734519, acc.: 45.00%] [G loss: 0.783483]\n",
      "302 [D loss: 0.756972, acc.: 35.00%] [G loss: 0.752837]\n",
      "303 [D loss: 0.838490, acc.: 25.00%] [G loss: 0.886296]\n",
      "304 [D loss: 0.769305, acc.: 35.00%] [G loss: 0.790398]\n",
      "305 [D loss: 0.676940, acc.: 55.00%] [G loss: 0.924132]\n",
      "306 [D loss: 0.694638, acc.: 60.00%] [G loss: 0.715605]\n",
      "307 [D loss: 0.709489, acc.: 40.00%] [G loss: 0.797656]\n",
      "308 [D loss: 0.720628, acc.: 55.00%] [G loss: 0.844555]\n",
      "309 [D loss: 0.649452, acc.: 55.00%] [G loss: 0.686077]\n",
      "310 [D loss: 0.703416, acc.: 40.00%] [G loss: 0.904353]\n",
      "311 [D loss: 0.710205, acc.: 55.00%] [G loss: 0.708382]\n",
      "312 [D loss: 0.678206, acc.: 55.00%] [G loss: 0.863158]\n",
      "313 [D loss: 0.818598, acc.: 35.00%] [G loss: 0.768960]\n",
      "314 [D loss: 0.718520, acc.: 35.00%] [G loss: 0.870202]\n",
      "315 [D loss: 0.724055, acc.: 55.00%] [G loss: 0.647294]\n",
      "316 [D loss: 0.608694, acc.: 60.00%] [G loss: 0.625080]\n",
      "317 [D loss: 0.714685, acc.: 45.00%] [G loss: 0.700028]\n",
      "318 [D loss: 0.690592, acc.: 50.00%] [G loss: 0.769092]\n",
      "319 [D loss: 0.739931, acc.: 35.00%] [G loss: 0.683119]\n",
      "320 [D loss: 0.731509, acc.: 55.00%] [G loss: 0.718622]\n",
      "321 [D loss: 0.732429, acc.: 50.00%] [G loss: 0.822291]\n",
      "322 [D loss: 0.693833, acc.: 40.00%] [G loss: 0.823393]\n",
      "323 [D loss: 0.748463, acc.: 45.00%] [G loss: 0.851288]\n",
      "324 [D loss: 0.693941, acc.: 45.00%] [G loss: 0.920659]\n",
      "325 [D loss: 0.758889, acc.: 35.00%] [G loss: 0.931838]\n",
      "326 [D loss: 0.724997, acc.: 45.00%] [G loss: 0.866747]\n",
      "327 [D loss: 0.757721, acc.: 35.00%] [G loss: 0.799625]\n",
      "328 [D loss: 0.719848, acc.: 60.00%] [G loss: 0.758455]\n",
      "329 [D loss: 0.707707, acc.: 60.00%] [G loss: 0.628908]\n",
      "330 [D loss: 0.695080, acc.: 65.00%] [G loss: 0.786051]\n",
      "331 [D loss: 0.756310, acc.: 40.00%] [G loss: 0.732865]\n",
      "332 [D loss: 0.726726, acc.: 45.00%] [G loss: 0.781070]\n",
      "333 [D loss: 0.714087, acc.: 60.00%] [G loss: 0.755405]\n",
      "334 [D loss: 0.718457, acc.: 45.00%] [G loss: 0.825557]\n",
      "335 [D loss: 0.745819, acc.: 60.00%] [G loss: 0.733702]\n",
      "336 [D loss: 0.809773, acc.: 35.00%] [G loss: 0.899360]\n",
      "337 [D loss: 0.649402, acc.: 75.00%] [G loss: 0.771276]\n",
      "338 [D loss: 0.724198, acc.: 40.00%] [G loss: 0.736456]\n",
      "339 [D loss: 0.713927, acc.: 60.00%] [G loss: 0.875497]\n",
      "340 [D loss: 0.689449, acc.: 55.00%] [G loss: 0.870609]\n",
      "341 [D loss: 0.770171, acc.: 45.00%] [G loss: 0.801148]\n",
      "342 [D loss: 0.783195, acc.: 40.00%] [G loss: 0.803846]\n",
      "343 [D loss: 0.715912, acc.: 55.00%] [G loss: 0.711875]\n",
      "344 [D loss: 0.751596, acc.: 35.00%] [G loss: 0.896105]\n",
      "345 [D loss: 0.745857, acc.: 50.00%] [G loss: 0.765155]\n",
      "346 [D loss: 0.776907, acc.: 30.00%] [G loss: 0.815755]\n",
      "347 [D loss: 0.771952, acc.: 45.00%] [G loss: 0.729155]\n",
      "348 [D loss: 0.720521, acc.: 55.00%] [G loss: 0.738659]\n",
      "349 [D loss: 0.709450, acc.: 45.00%] [G loss: 0.728800]\n",
      "350 [D loss: 0.758967, acc.: 45.00%] [G loss: 0.659975]\n",
      "351 [D loss: 0.750309, acc.: 45.00%] [G loss: 0.922730]\n",
      "352 [D loss: 0.699952, acc.: 35.00%] [G loss: 0.848846]\n",
      "353 [D loss: 0.744827, acc.: 40.00%] [G loss: 0.777692]\n",
      "354 [D loss: 0.655831, acc.: 55.00%] [G loss: 0.878599]\n",
      "355 [D loss: 0.777665, acc.: 30.00%] [G loss: 0.922562]\n",
      "356 [D loss: 0.807709, acc.: 40.00%] [G loss: 0.682824]\n",
      "357 [D loss: 0.806036, acc.: 35.00%] [G loss: 0.842880]\n",
      "358 [D loss: 0.737437, acc.: 45.00%] [G loss: 0.824450]\n",
      "359 [D loss: 0.750088, acc.: 30.00%] [G loss: 0.806933]\n",
      "360 [D loss: 0.700760, acc.: 50.00%] [G loss: 0.769092]\n",
      "361 [D loss: 0.744281, acc.: 40.00%] [G loss: 0.756783]\n",
      "362 [D loss: 0.726019, acc.: 55.00%] [G loss: 0.707989]\n",
      "363 [D loss: 0.709286, acc.: 50.00%] [G loss: 0.802364]\n",
      "364 [D loss: 0.700304, acc.: 55.00%] [G loss: 0.907526]\n",
      "365 [D loss: 0.734067, acc.: 50.00%] [G loss: 0.700877]\n",
      "366 [D loss: 0.709162, acc.: 40.00%] [G loss: 0.921756]\n",
      "367 [D loss: 0.775347, acc.: 35.00%] [G loss: 0.852966]\n",
      "368 [D loss: 0.701124, acc.: 50.00%] [G loss: 0.627983]\n",
      "369 [D loss: 0.816462, acc.: 35.00%] [G loss: 0.849670]\n",
      "370 [D loss: 0.675519, acc.: 50.00%] [G loss: 0.673806]\n",
      "371 [D loss: 0.701522, acc.: 55.00%] [G loss: 0.863189]\n",
      "372 [D loss: 0.756158, acc.: 30.00%] [G loss: 0.728705]\n",
      "373 [D loss: 0.688982, acc.: 50.00%] [G loss: 0.751870]\n",
      "374 [D loss: 0.713075, acc.: 50.00%] [G loss: 0.757503]\n",
      "375 [D loss: 0.833250, acc.: 25.00%] [G loss: 0.803246]\n",
      "376 [D loss: 0.719138, acc.: 60.00%] [G loss: 0.916195]\n",
      "377 [D loss: 0.647936, acc.: 70.00%] [G loss: 0.813029]\n",
      "378 [D loss: 0.688579, acc.: 55.00%] [G loss: 0.681904]\n",
      "379 [D loss: 0.687195, acc.: 60.00%] [G loss: 0.789411]\n",
      "380 [D loss: 0.693308, acc.: 60.00%] [G loss: 0.734994]\n",
      "381 [D loss: 0.774824, acc.: 45.00%] [G loss: 0.731820]\n",
      "382 [D loss: 0.893484, acc.: 30.00%] [G loss: 0.826994]\n",
      "383 [D loss: 0.732327, acc.: 40.00%] [G loss: 0.735008]\n",
      "384 [D loss: 0.610213, acc.: 70.00%] [G loss: 0.817390]\n",
      "385 [D loss: 0.644738, acc.: 50.00%] [G loss: 0.736122]\n",
      "386 [D loss: 0.747439, acc.: 40.00%] [G loss: 0.779515]\n",
      "387 [D loss: 0.715514, acc.: 60.00%] [G loss: 0.752569]\n",
      "388 [D loss: 0.718162, acc.: 60.00%] [G loss: 0.821272]\n",
      "389 [D loss: 0.704373, acc.: 40.00%] [G loss: 0.751500]\n",
      "390 [D loss: 0.763844, acc.: 40.00%] [G loss: 0.848891]\n",
      "391 [D loss: 0.686477, acc.: 60.00%] [G loss: 0.702448]\n",
      "392 [D loss: 0.689640, acc.: 55.00%] [G loss: 0.695081]\n",
      "393 [D loss: 0.705553, acc.: 60.00%] [G loss: 0.729068]\n",
      "394 [D loss: 0.738923, acc.: 50.00%] [G loss: 0.850893]\n",
      "395 [D loss: 0.729036, acc.: 35.00%] [G loss: 0.700009]\n",
      "396 [D loss: 0.784294, acc.: 40.00%] [G loss: 0.735158]\n",
      "397 [D loss: 0.760677, acc.: 45.00%] [G loss: 0.779728]\n",
      "398 [D loss: 0.706962, acc.: 60.00%] [G loss: 0.839304]\n",
      "399 [D loss: 0.730928, acc.: 40.00%] [G loss: 0.717477]\n",
      "400 [D loss: 0.729626, acc.: 45.00%] [G loss: 0.883003]\n",
      "401 [D loss: 0.727953, acc.: 40.00%] [G loss: 0.787134]\n",
      "402 [D loss: 0.725160, acc.: 50.00%] [G loss: 0.792425]\n",
      "403 [D loss: 0.613450, acc.: 70.00%] [G loss: 0.756102]\n",
      "404 [D loss: 0.744487, acc.: 45.00%] [G loss: 0.969215]\n",
      "405 [D loss: 0.644844, acc.: 60.00%] [G loss: 0.757939]\n",
      "406 [D loss: 0.687870, acc.: 50.00%] [G loss: 0.805055]\n",
      "407 [D loss: 0.664514, acc.: 60.00%] [G loss: 0.738396]\n",
      "408 [D loss: 0.860781, acc.: 30.00%] [G loss: 0.850831]\n",
      "409 [D loss: 0.759604, acc.: 45.00%] [G loss: 0.850213]\n",
      "410 [D loss: 0.784885, acc.: 30.00%] [G loss: 0.860005]\n",
      "411 [D loss: 0.663413, acc.: 60.00%] [G loss: 0.890121]\n",
      "412 [D loss: 0.714866, acc.: 45.00%] [G loss: 0.752330]\n",
      "413 [D loss: 0.746462, acc.: 45.00%] [G loss: 0.756069]\n",
      "414 [D loss: 0.758426, acc.: 40.00%] [G loss: 0.888672]\n",
      "415 [D loss: 0.735575, acc.: 55.00%] [G loss: 0.722952]\n",
      "416 [D loss: 0.792225, acc.: 35.00%] [G loss: 0.748290]\n",
      "417 [D loss: 0.768067, acc.: 40.00%] [G loss: 0.788602]\n",
      "418 [D loss: 0.769547, acc.: 35.00%] [G loss: 0.905301]\n",
      "419 [D loss: 0.633810, acc.: 70.00%] [G loss: 0.759058]\n",
      "420 [D loss: 0.720793, acc.: 45.00%] [G loss: 0.738597]\n",
      "421 [D loss: 0.763615, acc.: 30.00%] [G loss: 0.888480]\n",
      "422 [D loss: 0.699762, acc.: 60.00%] [G loss: 0.840893]\n",
      "423 [D loss: 0.784031, acc.: 35.00%] [G loss: 0.902146]\n",
      "424 [D loss: 0.720239, acc.: 45.00%] [G loss: 0.740438]\n",
      "425 [D loss: 0.676416, acc.: 65.00%] [G loss: 0.774886]\n",
      "426 [D loss: 0.718888, acc.: 40.00%] [G loss: 0.653794]\n",
      "427 [D loss: 0.658846, acc.: 60.00%] [G loss: 0.823479]\n",
      "428 [D loss: 0.702951, acc.: 45.00%] [G loss: 0.804560]\n",
      "429 [D loss: 0.744219, acc.: 45.00%] [G loss: 0.730884]\n",
      "430 [D loss: 0.771609, acc.: 40.00%] [G loss: 0.782621]\n",
      "431 [D loss: 0.736470, acc.: 45.00%] [G loss: 0.649939]\n",
      "432 [D loss: 0.746267, acc.: 40.00%] [G loss: 0.778538]\n",
      "433 [D loss: 0.820768, acc.: 25.00%] [G loss: 0.812882]\n",
      "434 [D loss: 0.761431, acc.: 40.00%] [G loss: 0.835784]\n",
      "435 [D loss: 0.687202, acc.: 45.00%] [G loss: 0.733804]\n",
      "436 [D loss: 0.748261, acc.: 45.00%] [G loss: 0.812425]\n",
      "437 [D loss: 0.767968, acc.: 45.00%] [G loss: 0.797849]\n",
      "438 [D loss: 0.767174, acc.: 40.00%] [G loss: 0.839745]\n",
      "439 [D loss: 0.713164, acc.: 55.00%] [G loss: 0.663546]\n",
      "440 [D loss: 0.832754, acc.: 30.00%] [G loss: 0.937206]\n",
      "441 [D loss: 0.781132, acc.: 30.00%] [G loss: 0.781156]\n",
      "442 [D loss: 0.650108, acc.: 60.00%] [G loss: 0.702399]\n",
      "443 [D loss: 0.807311, acc.: 25.00%] [G loss: 0.895287]\n",
      "444 [D loss: 0.635683, acc.: 65.00%] [G loss: 0.817779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 [D loss: 0.753498, acc.: 40.00%] [G loss: 0.899105]\n",
      "446 [D loss: 0.630685, acc.: 60.00%] [G loss: 0.734485]\n",
      "447 [D loss: 0.730197, acc.: 45.00%] [G loss: 0.795581]\n",
      "448 [D loss: 0.814045, acc.: 20.00%] [G loss: 0.755053]\n",
      "449 [D loss: 0.693827, acc.: 75.00%] [G loss: 0.758146]\n",
      "450 [D loss: 0.737154, acc.: 45.00%] [G loss: 0.681597]\n",
      "451 [D loss: 0.730821, acc.: 50.00%] [G loss: 0.860090]\n",
      "452 [D loss: 0.774155, acc.: 50.00%] [G loss: 0.836411]\n",
      "453 [D loss: 0.842172, acc.: 30.00%] [G loss: 0.858687]\n",
      "454 [D loss: 0.740178, acc.: 40.00%] [G loss: 0.842146]\n",
      "455 [D loss: 0.720369, acc.: 45.00%] [G loss: 0.667479]\n",
      "456 [D loss: 0.732263, acc.: 45.00%] [G loss: 0.757342]\n",
      "457 [D loss: 0.788387, acc.: 45.00%] [G loss: 0.827439]\n",
      "458 [D loss: 0.740738, acc.: 40.00%] [G loss: 0.756041]\n",
      "459 [D loss: 0.701515, acc.: 55.00%] [G loss: 0.721777]\n",
      "460 [D loss: 0.705875, acc.: 50.00%] [G loss: 0.942217]\n",
      "461 [D loss: 0.772336, acc.: 30.00%] [G loss: 0.808519]\n",
      "462 [D loss: 0.694974, acc.: 55.00%] [G loss: 0.764270]\n",
      "463 [D loss: 0.736250, acc.: 50.00%] [G loss: 0.732970]\n",
      "464 [D loss: 0.818131, acc.: 35.00%] [G loss: 0.790960]\n",
      "465 [D loss: 0.680608, acc.: 55.00%] [G loss: 0.974401]\n",
      "466 [D loss: 0.668419, acc.: 55.00%] [G loss: 0.840000]\n",
      "467 [D loss: 0.727348, acc.: 30.00%] [G loss: 0.797902]\n",
      "468 [D loss: 0.663605, acc.: 50.00%] [G loss: 0.803735]\n",
      "469 [D loss: 0.786821, acc.: 25.00%] [G loss: 0.799773]\n",
      "470 [D loss: 0.706183, acc.: 50.00%] [G loss: 0.748269]\n",
      "471 [D loss: 0.731957, acc.: 50.00%] [G loss: 0.853495]\n",
      "472 [D loss: 0.732951, acc.: 40.00%] [G loss: 0.698769]\n",
      "473 [D loss: 0.646339, acc.: 55.00%] [G loss: 0.833843]\n",
      "474 [D loss: 0.772423, acc.: 45.00%] [G loss: 0.829782]\n",
      "475 [D loss: 0.703743, acc.: 60.00%] [G loss: 0.644030]\n",
      "476 [D loss: 0.755288, acc.: 50.00%] [G loss: 0.745414]\n",
      "477 [D loss: 0.730442, acc.: 50.00%] [G loss: 0.837722]\n",
      "478 [D loss: 0.716910, acc.: 50.00%] [G loss: 0.852715]\n",
      "479 [D loss: 0.765536, acc.: 45.00%] [G loss: 0.764962]\n",
      "480 [D loss: 0.801525, acc.: 40.00%] [G loss: 0.851595]\n",
      "481 [D loss: 0.742276, acc.: 55.00%] [G loss: 0.809514]\n",
      "482 [D loss: 0.664810, acc.: 75.00%] [G loss: 0.864741]\n",
      "483 [D loss: 0.775247, acc.: 30.00%] [G loss: 0.810897]\n",
      "484 [D loss: 0.714538, acc.: 55.00%] [G loss: 0.867206]\n",
      "485 [D loss: 0.679507, acc.: 50.00%] [G loss: 0.800587]\n",
      "486 [D loss: 0.744502, acc.: 50.00%] [G loss: 0.855499]\n",
      "487 [D loss: 0.755265, acc.: 55.00%] [G loss: 0.901280]\n",
      "488 [D loss: 0.726418, acc.: 50.00%] [G loss: 0.727055]\n",
      "489 [D loss: 0.730560, acc.: 55.00%] [G loss: 0.740430]\n",
      "490 [D loss: 0.695024, acc.: 50.00%] [G loss: 0.911294]\n",
      "491 [D loss: 0.778548, acc.: 30.00%] [G loss: 0.802374]\n",
      "492 [D loss: 0.665161, acc.: 75.00%] [G loss: 0.855311]\n",
      "493 [D loss: 0.764196, acc.: 45.00%] [G loss: 0.863169]\n",
      "494 [D loss: 0.697860, acc.: 50.00%] [G loss: 0.817479]\n",
      "495 [D loss: 0.733285, acc.: 45.00%] [G loss: 0.759484]\n",
      "496 [D loss: 0.694386, acc.: 50.00%] [G loss: 0.774834]\n",
      "497 [D loss: 0.775158, acc.: 50.00%] [G loss: 0.798034]\n",
      "498 [D loss: 0.750301, acc.: 30.00%] [G loss: 0.789330]\n",
      "499 [D loss: 0.789148, acc.: 45.00%] [G loss: 0.706050]\n",
      "500 [D loss: 0.679245, acc.: 55.00%] [G loss: 0.660666]\n",
      "501 [D loss: 0.716098, acc.: 50.00%] [G loss: 0.860141]\n",
      "502 [D loss: 0.724713, acc.: 55.00%] [G loss: 0.782998]\n",
      "503 [D loss: 0.698700, acc.: 60.00%] [G loss: 0.849084]\n",
      "504 [D loss: 0.667276, acc.: 65.00%] [G loss: 0.745002]\n",
      "505 [D loss: 0.681703, acc.: 55.00%] [G loss: 0.709209]\n",
      "506 [D loss: 0.726715, acc.: 45.00%] [G loss: 0.654555]\n",
      "507 [D loss: 0.741506, acc.: 35.00%] [G loss: 0.762624]\n",
      "508 [D loss: 0.672620, acc.: 60.00%] [G loss: 0.714642]\n",
      "509 [D loss: 0.768529, acc.: 50.00%] [G loss: 0.860037]\n",
      "510 [D loss: 0.711042, acc.: 45.00%] [G loss: 0.768250]\n",
      "511 [D loss: 0.711668, acc.: 45.00%] [G loss: 0.756192]\n",
      "512 [D loss: 0.700535, acc.: 50.00%] [G loss: 0.851170]\n",
      "513 [D loss: 0.759043, acc.: 45.00%] [G loss: 0.808462]\n",
      "514 [D loss: 0.723226, acc.: 50.00%] [G loss: 0.773862]\n",
      "515 [D loss: 0.732211, acc.: 50.00%] [G loss: 0.818076]\n",
      "516 [D loss: 0.782154, acc.: 40.00%] [G loss: 0.625446]\n",
      "517 [D loss: 0.693239, acc.: 60.00%] [G loss: 0.812516]\n",
      "518 [D loss: 0.698066, acc.: 55.00%] [G loss: 0.698946]\n",
      "519 [D loss: 0.739877, acc.: 55.00%] [G loss: 0.838274]\n",
      "520 [D loss: 0.696061, acc.: 55.00%] [G loss: 0.837934]\n",
      "521 [D loss: 0.697359, acc.: 55.00%] [G loss: 0.766292]\n",
      "522 [D loss: 0.689064, acc.: 60.00%] [G loss: 0.771825]\n",
      "523 [D loss: 0.720241, acc.: 65.00%] [G loss: 0.722875]\n",
      "524 [D loss: 0.716981, acc.: 50.00%] [G loss: 0.728711]\n",
      "525 [D loss: 0.783604, acc.: 40.00%] [G loss: 0.814891]\n",
      "526 [D loss: 0.709559, acc.: 50.00%] [G loss: 0.703894]\n",
      "527 [D loss: 0.652342, acc.: 70.00%] [G loss: 0.726364]\n",
      "528 [D loss: 0.734023, acc.: 50.00%] [G loss: 0.618620]\n",
      "529 [D loss: 0.724912, acc.: 50.00%] [G loss: 0.788054]\n",
      "530 [D loss: 0.689883, acc.: 55.00%] [G loss: 0.800928]\n",
      "531 [D loss: 0.640212, acc.: 55.00%] [G loss: 0.692621]\n",
      "532 [D loss: 0.722682, acc.: 45.00%] [G loss: 0.812532]\n",
      "533 [D loss: 0.727536, acc.: 55.00%] [G loss: 0.644728]\n",
      "534 [D loss: 0.771221, acc.: 45.00%] [G loss: 0.821019]\n",
      "535 [D loss: 0.663821, acc.: 65.00%] [G loss: 0.788982]\n",
      "536 [D loss: 0.704383, acc.: 40.00%] [G loss: 0.866441]\n",
      "537 [D loss: 0.697948, acc.: 40.00%] [G loss: 0.778953]\n",
      "538 [D loss: 0.759333, acc.: 45.00%] [G loss: 0.737483]\n",
      "539 [D loss: 0.683650, acc.: 55.00%] [G loss: 0.619825]\n",
      "540 [D loss: 0.732354, acc.: 45.00%] [G loss: 0.766362]\n",
      "541 [D loss: 0.703348, acc.: 65.00%] [G loss: 0.701197]\n",
      "542 [D loss: 0.703261, acc.: 65.00%] [G loss: 0.577371]\n",
      "543 [D loss: 0.703432, acc.: 55.00%] [G loss: 0.754618]\n",
      "544 [D loss: 0.793357, acc.: 40.00%] [G loss: 0.789547]\n",
      "545 [D loss: 0.742550, acc.: 35.00%] [G loss: 0.839369]\n",
      "546 [D loss: 0.697872, acc.: 50.00%] [G loss: 0.692574]\n",
      "547 [D loss: 0.715631, acc.: 55.00%] [G loss: 0.702602]\n",
      "548 [D loss: 0.710711, acc.: 45.00%] [G loss: 0.726967]\n",
      "549 [D loss: 0.680312, acc.: 55.00%] [G loss: 0.717984]\n",
      "550 [D loss: 0.810122, acc.: 30.00%] [G loss: 0.969396]\n",
      "551 [D loss: 0.768530, acc.: 40.00%] [G loss: 0.795225]\n",
      "552 [D loss: 0.702737, acc.: 50.00%] [G loss: 0.797587]\n",
      "553 [D loss: 0.731042, acc.: 45.00%] [G loss: 0.780553]\n",
      "554 [D loss: 0.758239, acc.: 45.00%] [G loss: 0.914289]\n",
      "555 [D loss: 0.728381, acc.: 60.00%] [G loss: 0.728868]\n",
      "556 [D loss: 0.770185, acc.: 35.00%] [G loss: 0.727035]\n",
      "557 [D loss: 0.749121, acc.: 45.00%] [G loss: 0.893979]\n",
      "558 [D loss: 0.725183, acc.: 45.00%] [G loss: 0.813791]\n",
      "559 [D loss: 0.662495, acc.: 70.00%] [G loss: 0.795109]\n",
      "560 [D loss: 0.734430, acc.: 60.00%] [G loss: 0.712654]\n",
      "561 [D loss: 0.621838, acc.: 70.00%] [G loss: 0.731328]\n",
      "562 [D loss: 0.716314, acc.: 50.00%] [G loss: 0.826004]\n",
      "563 [D loss: 0.679761, acc.: 55.00%] [G loss: 0.743030]\n",
      "564 [D loss: 0.738212, acc.: 50.00%] [G loss: 0.799111]\n",
      "565 [D loss: 0.731445, acc.: 50.00%] [G loss: 0.820004]\n",
      "566 [D loss: 0.730703, acc.: 50.00%] [G loss: 0.758139]\n",
      "567 [D loss: 0.691196, acc.: 50.00%] [G loss: 0.721205]\n",
      "568 [D loss: 0.711978, acc.: 45.00%] [G loss: 0.760965]\n",
      "569 [D loss: 0.680449, acc.: 55.00%] [G loss: 0.695329]\n",
      "570 [D loss: 0.708523, acc.: 50.00%] [G loss: 0.806901]\n",
      "571 [D loss: 0.811173, acc.: 45.00%] [G loss: 0.770353]\n",
      "572 [D loss: 0.637114, acc.: 70.00%] [G loss: 0.817386]\n",
      "573 [D loss: 0.719487, acc.: 55.00%] [G loss: 0.839578]\n",
      "574 [D loss: 0.779864, acc.: 35.00%] [G loss: 0.759973]\n",
      "575 [D loss: 0.735258, acc.: 45.00%] [G loss: 0.878088]\n",
      "576 [D loss: 0.759725, acc.: 50.00%] [G loss: 0.679652]\n",
      "577 [D loss: 0.657301, acc.: 75.00%] [G loss: 0.849287]\n",
      "578 [D loss: 0.749525, acc.: 40.00%] [G loss: 0.770954]\n",
      "579 [D loss: 0.747535, acc.: 45.00%] [G loss: 0.732978]\n",
      "580 [D loss: 0.758019, acc.: 40.00%] [G loss: 0.867614]\n",
      "581 [D loss: 0.718079, acc.: 45.00%] [G loss: 0.770896]\n",
      "582 [D loss: 0.693809, acc.: 60.00%] [G loss: 0.744699]\n",
      "583 [D loss: 0.765184, acc.: 45.00%] [G loss: 0.745760]\n",
      "584 [D loss: 0.747923, acc.: 35.00%] [G loss: 0.719921]\n",
      "585 [D loss: 0.707685, acc.: 60.00%] [G loss: 0.741499]\n",
      "586 [D loss: 0.756002, acc.: 30.00%] [G loss: 0.739371]\n",
      "587 [D loss: 0.638662, acc.: 70.00%] [G loss: 0.825660]\n",
      "588 [D loss: 0.671494, acc.: 65.00%] [G loss: 0.702952]\n",
      "589 [D loss: 0.723227, acc.: 40.00%] [G loss: 0.733281]\n",
      "590 [D loss: 0.774283, acc.: 35.00%] [G loss: 0.845594]\n",
      "591 [D loss: 0.665309, acc.: 70.00%] [G loss: 0.819979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 [D loss: 0.787607, acc.: 30.00%] [G loss: 0.829971]\n",
      "593 [D loss: 0.746423, acc.: 45.00%] [G loss: 0.713573]\n",
      "594 [D loss: 0.679537, acc.: 60.00%] [G loss: 0.697024]\n",
      "595 [D loss: 0.709468, acc.: 45.00%] [G loss: 0.796578]\n",
      "596 [D loss: 0.652169, acc.: 60.00%] [G loss: 0.717496]\n",
      "597 [D loss: 0.729458, acc.: 50.00%] [G loss: 0.734680]\n",
      "598 [D loss: 0.751603, acc.: 35.00%] [G loss: 0.736377]\n",
      "599 [D loss: 0.726689, acc.: 45.00%] [G loss: 0.735143]\n",
      "600 [D loss: 0.673048, acc.: 55.00%] [G loss: 0.649779]\n",
      "601 [D loss: 0.687266, acc.: 60.00%] [G loss: 0.837164]\n",
      "602 [D loss: 0.732196, acc.: 55.00%] [G loss: 1.005815]\n",
      "603 [D loss: 0.845770, acc.: 35.00%] [G loss: 0.747662]\n",
      "604 [D loss: 0.689804, acc.: 50.00%] [G loss: 0.789453]\n",
      "605 [D loss: 0.716042, acc.: 50.00%] [G loss: 0.862322]\n",
      "606 [D loss: 0.755195, acc.: 45.00%] [G loss: 0.904441]\n",
      "607 [D loss: 0.657265, acc.: 50.00%] [G loss: 0.870365]\n",
      "608 [D loss: 0.683324, acc.: 55.00%] [G loss: 0.836252]\n",
      "609 [D loss: 0.679786, acc.: 60.00%] [G loss: 0.913170]\n",
      "610 [D loss: 0.693931, acc.: 60.00%] [G loss: 0.756317]\n",
      "611 [D loss: 0.716645, acc.: 40.00%] [G loss: 0.733718]\n",
      "612 [D loss: 0.753793, acc.: 40.00%] [G loss: 0.731095]\n",
      "613 [D loss: 0.717027, acc.: 50.00%] [G loss: 0.681551]\n",
      "614 [D loss: 0.651429, acc.: 60.00%] [G loss: 0.810486]\n",
      "615 [D loss: 0.780492, acc.: 35.00%] [G loss: 0.643966]\n",
      "616 [D loss: 0.712810, acc.: 40.00%] [G loss: 0.724300]\n",
      "617 [D loss: 0.686994, acc.: 60.00%] [G loss: 0.765821]\n",
      "618 [D loss: 0.677265, acc.: 55.00%] [G loss: 0.750523]\n",
      "619 [D loss: 0.747865, acc.: 35.00%] [G loss: 0.773686]\n",
      "620 [D loss: 0.794622, acc.: 30.00%] [G loss: 0.703992]\n",
      "621 [D loss: 0.617538, acc.: 75.00%] [G loss: 0.882903]\n",
      "622 [D loss: 0.679712, acc.: 55.00%] [G loss: 0.643274]\n",
      "623 [D loss: 0.730484, acc.: 50.00%] [G loss: 0.712203]\n",
      "624 [D loss: 0.697641, acc.: 55.00%] [G loss: 0.716292]\n",
      "625 [D loss: 0.714167, acc.: 40.00%] [G loss: 0.802027]\n",
      "626 [D loss: 0.747637, acc.: 45.00%] [G loss: 0.582094]\n",
      "627 [D loss: 0.735552, acc.: 50.00%] [G loss: 0.818508]\n",
      "628 [D loss: 0.749793, acc.: 50.00%] [G loss: 0.902803]\n",
      "629 [D loss: 0.671919, acc.: 60.00%] [G loss: 0.838954]\n",
      "630 [D loss: 0.753363, acc.: 50.00%] [G loss: 0.863099]\n",
      "631 [D loss: 0.658997, acc.: 55.00%] [G loss: 0.760367]\n",
      "632 [D loss: 0.682636, acc.: 55.00%] [G loss: 0.818952]\n",
      "633 [D loss: 0.755451, acc.: 40.00%] [G loss: 0.899895]\n",
      "634 [D loss: 0.805097, acc.: 30.00%] [G loss: 0.675133]\n",
      "635 [D loss: 0.728243, acc.: 40.00%] [G loss: 0.831623]\n",
      "636 [D loss: 0.659923, acc.: 60.00%] [G loss: 0.875634]\n",
      "637 [D loss: 0.758442, acc.: 30.00%] [G loss: 0.677331]\n",
      "638 [D loss: 0.740520, acc.: 50.00%] [G loss: 0.804556]\n",
      "639 [D loss: 0.657255, acc.: 55.00%] [G loss: 0.880821]\n",
      "640 [D loss: 0.734716, acc.: 45.00%] [G loss: 0.863067]\n",
      "641 [D loss: 0.649460, acc.: 65.00%] [G loss: 0.702918]\n",
      "642 [D loss: 0.758514, acc.: 45.00%] [G loss: 0.829985]\n",
      "643 [D loss: 0.712700, acc.: 70.00%] [G loss: 0.689294]\n",
      "644 [D loss: 0.706072, acc.: 65.00%] [G loss: 0.757268]\n",
      "645 [D loss: 0.754812, acc.: 45.00%] [G loss: 0.736076]\n",
      "646 [D loss: 0.723227, acc.: 45.00%] [G loss: 0.754067]\n",
      "647 [D loss: 0.778308, acc.: 35.00%] [G loss: 0.800330]\n",
      "648 [D loss: 0.739951, acc.: 45.00%] [G loss: 0.772419]\n",
      "649 [D loss: 0.697344, acc.: 50.00%] [G loss: 0.766093]\n",
      "650 [D loss: 0.726577, acc.: 55.00%] [G loss: 0.857060]\n",
      "651 [D loss: 0.667165, acc.: 70.00%] [G loss: 0.759546]\n",
      "652 [D loss: 0.672354, acc.: 65.00%] [G loss: 0.756086]\n",
      "653 [D loss: 0.653495, acc.: 55.00%] [G loss: 0.532974]\n",
      "654 [D loss: 0.709203, acc.: 40.00%] [G loss: 0.803012]\n",
      "655 [D loss: 0.732300, acc.: 45.00%] [G loss: 0.769372]\n",
      "656 [D loss: 0.817625, acc.: 40.00%] [G loss: 0.734999]\n",
      "657 [D loss: 0.744679, acc.: 30.00%] [G loss: 0.840409]\n",
      "658 [D loss: 0.772050, acc.: 35.00%] [G loss: 0.735143]\n",
      "659 [D loss: 0.694912, acc.: 50.00%] [G loss: 0.809876]\n",
      "660 [D loss: 0.669648, acc.: 60.00%] [G loss: 0.722712]\n",
      "661 [D loss: 0.753012, acc.: 40.00%] [G loss: 0.768558]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN()\n",
    "dcgan.train(epochs=1001, batch_size=10, save_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
